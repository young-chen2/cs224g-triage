{"ast":null,"code":"import { PromptTemplate } from \"@langchain/core/prompts\";\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\nimport { BaseChain } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAChain } from \"./question_answering/load.js\";\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n/**\n * @deprecated This class will be removed in 1.0.0. See below for an example implementation using\n * `createRetrievalChain`.\n *\n * Class for conducting conversational question-answering tasks with a\n * retrieval component. Extends the BaseChain class and implements the\n * ConversationalRetrievalQAChainInput interface.\n * @example\n * ```typescript\n * import { ChatAnthropic } from \"@langchain/anthropic\";\n * import {\n *   ChatPromptTemplate,\n *   MessagesPlaceholder,\n * } from \"@langchain/core/prompts\";\n * import { BaseMessage } from \"@langchain/core/messages\";\n * import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n * import { createHistoryAwareRetriever } from \"langchain/chains/history_aware_retriever\";\n * import { createRetrievalChain } from \"langchain/chains/retrieval\";\n *\n * const retriever = ...your retriever;\n * const llm = new ChatAnthropic();\n *\n * // Contextualize question\n * const contextualizeQSystemPrompt = `\n * Given a chat history and the latest user question\n * which might reference context in the chat history,\n * formulate a standalone question which can be understood\n * without the chat history. Do NOT answer the question, just\n * reformulate it if needed and otherwise return it as is.`;\n * const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", contextualizeQSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n * const historyAwareRetriever = await createHistoryAwareRetriever({\n *   llm,\n *   retriever,\n *   rephrasePrompt: contextualizeQPrompt,\n * });\n *\n * // Answer question\n * const qaSystemPrompt = `\n * You are an assistant for question-answering tasks. Use\n * the following pieces of retrieved context to answer the\n * question. If you don't know the answer, just say that you\n * don't know. Use three sentences maximum and keep the answer\n * concise.\n * \\n\\n\n * {context}`;\n * const qaPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", qaSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n *\n * // Below we use createStuffDocuments_chain to feed all retrieved context\n * // into the LLM. Note that we can also use StuffDocumentsChain and other\n * // instances of BaseCombineDocumentsChain.\n * const questionAnswerChain = await createStuffDocumentsChain({\n *   llm,\n *   prompt: qaPrompt,\n * });\n *\n * const ragChain = await createRetrievalChain({\n *   retriever: historyAwareRetriever,\n *   combineDocsChain: questionAnswerChain,\n * });\n *\n * // Usage:\n * const chat_history: BaseMessage[] = [];\n * const response = await ragChain.invoke({\n *   chat_history,\n *   input: \"...\",\n * });\n * ```\n */\nexport class ConversationalRetrievalQAChain extends BaseChain {\n  static lc_name() {\n    return \"ConversationalRetrievalQAChain\";\n  }\n  get inputKeys() {\n    return [this.inputKey, this.chatHistoryKey];\n  }\n  get outputKeys() {\n    return this.combineDocumentsChain.outputKeys.concat(this.returnSourceDocuments ? [\"sourceDocuments\"] : []);\n  }\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"inputKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"question\"\n    });\n    Object.defineProperty(this, \"chatHistoryKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"chat_history\"\n    });\n    Object.defineProperty(this, \"retriever\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"combineDocumentsChain\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"questionGeneratorChain\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"returnSourceDocuments\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"returnGeneratedQuestion\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    this.retriever = fields.retriever;\n    this.combineDocumentsChain = fields.combineDocumentsChain;\n    this.questionGeneratorChain = fields.questionGeneratorChain;\n    this.inputKey = fields.inputKey ?? this.inputKey;\n    this.returnSourceDocuments = fields.returnSourceDocuments ?? this.returnSourceDocuments;\n    this.returnGeneratedQuestion = fields.returnGeneratedQuestion ?? this.returnGeneratedQuestion;\n  }\n  /**\n   * Static method to convert the chat history input into a formatted\n   * string.\n   * @param chatHistory Chat history input which can be a string, an array of BaseMessage instances, or an array of string arrays.\n   * @returns A formatted string representing the chat history.\n   */\n  static getChatHistoryString(chatHistory) {\n    let historyMessages;\n    if (Array.isArray(chatHistory)) {\n      // TODO: Deprecate on a breaking release\n      if (Array.isArray(chatHistory[0]) && typeof chatHistory[0][0] === \"string\") {\n        console.warn(\"Passing chat history as an array of strings is deprecated.\\nPlease see https://js.langchain.com/docs/modules/chains/popular/chat_vector_db#externally-managed-memory for more information.\");\n        historyMessages = chatHistory.flat().map((stringMessage, i) => {\n          if (i % 2 === 0) {\n            return new HumanMessage(stringMessage);\n          } else {\n            return new AIMessage(stringMessage);\n          }\n        });\n      } else {\n        historyMessages = chatHistory;\n      }\n      return historyMessages.map(chatMessage => {\n        if (chatMessage._getType() === \"human\") {\n          return `Human: ${chatMessage.content}`;\n        } else if (chatMessage._getType() === \"ai\") {\n          return `Assistant: ${chatMessage.content}`;\n        } else {\n          return `${chatMessage.content}`;\n        }\n      }).join(\"\\n\");\n    }\n    return chatHistory;\n  }\n  /** @ignore */\n  async _call(values, runManager) {\n    if (!(this.inputKey in values)) {\n      throw new Error(`Question key ${this.inputKey} not found.`);\n    }\n    if (!(this.chatHistoryKey in values)) {\n      throw new Error(`Chat history key ${this.chatHistoryKey} not found.`);\n    }\n    const question = values[this.inputKey];\n    const chatHistory = ConversationalRetrievalQAChain.getChatHistoryString(values[this.chatHistoryKey]);\n    let newQuestion = question;\n    if (chatHistory.length > 0) {\n      const result = await this.questionGeneratorChain.call({\n        question,\n        chat_history: chatHistory\n      }, runManager?.getChild(\"question_generator\"));\n      const keys = Object.keys(result);\n      if (keys.length === 1) {\n        newQuestion = result[keys[0]];\n      } else {\n        throw new Error(\"Return from llm chain has multiple values, only single values supported.\");\n      }\n    }\n    const docs = await this.retriever.getRelevantDocuments(newQuestion, runManager?.getChild(\"retriever\"));\n    const inputs = {\n      question: newQuestion,\n      input_documents: docs,\n      chat_history: chatHistory\n    };\n    let result = await this.combineDocumentsChain.call(inputs, runManager?.getChild(\"combine_documents\"));\n    if (this.returnSourceDocuments) {\n      result = {\n        ...result,\n        sourceDocuments: docs\n      };\n    }\n    if (this.returnGeneratedQuestion) {\n      result = {\n        ...result,\n        generatedQuestion: newQuestion\n      };\n    }\n    return result;\n  }\n  _chainType() {\n    return \"conversational_retrieval_chain\";\n  }\n  static async deserialize(_data, _values) {\n    throw new Error(\"Not implemented.\");\n  }\n  serialize() {\n    throw new Error(\"Not implemented.\");\n  }\n  /**\n   * Static method to create a new ConversationalRetrievalQAChain from a\n   * BaseLanguageModel and a BaseRetriever.\n   * @param llm {@link BaseLanguageModelInterface} instance used to generate a new question.\n   * @param retriever {@link BaseRetrieverInterface} instance used to retrieve relevant documents.\n   * @param options.returnSourceDocuments Whether to return source documents in the final output\n   * @param options.questionGeneratorChainOptions Options to initialize the standalone question generation chain used as the first internal step\n   * @param options.qaChainOptions {@link QAChainParams} used to initialize the QA chain used as the second internal step\n   * @returns A new instance of ConversationalRetrievalQAChain.\n   */\n  static fromLLM(llm, retriever, options = {}) {\n    const {\n      questionGeneratorTemplate,\n      qaTemplate,\n      qaChainOptions = {\n        type: \"stuff\",\n        prompt: qaTemplate ? PromptTemplate.fromTemplate(qaTemplate) : undefined\n      },\n      questionGeneratorChainOptions,\n      verbose,\n      ...rest\n    } = options;\n    const qaChain = loadQAChain(llm, qaChainOptions);\n    const questionGeneratorChainPrompt = PromptTemplate.fromTemplate(questionGeneratorChainOptions?.template ?? questionGeneratorTemplate ?? question_generator_template);\n    const questionGeneratorChain = new LLMChain({\n      prompt: questionGeneratorChainPrompt,\n      llm: questionGeneratorChainOptions?.llm ?? llm,\n      verbose\n    });\n    const instance = new this({\n      retriever,\n      combineDocumentsChain: qaChain,\n      questionGeneratorChain,\n      verbose,\n      ...rest\n    });\n    return instance;\n  }\n}","map":{"version":3,"names":["PromptTemplate","HumanMessage","AIMessage","BaseChain","LLMChain","loadQAChain","question_generator_template","ConversationalRetrievalQAChain","lc_name","inputKeys","inputKey","chatHistoryKey","outputKeys","combineDocumentsChain","concat","returnSourceDocuments","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","retriever","questionGeneratorChain","returnGeneratedQuestion","getChatHistoryString","chatHistory","historyMessages","Array","isArray","console","warn","flat","map","stringMessage","i","chatMessage","_getType","content","join","_call","values","runManager","Error","question","newQuestion","length","result","call","chat_history","getChild","keys","docs","getRelevantDocuments","inputs","input_documents","sourceDocuments","generatedQuestion","_chainType","deserialize","_data","_values","serialize","fromLLM","llm","options","questionGeneratorTemplate","qaTemplate","qaChainOptions","type","prompt","fromTemplate","undefined","questionGeneratorChainOptions","verbose","rest","qaChain","questionGeneratorChainPrompt","template","instance"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/langchain/dist/chains/conversational_retrieval_chain.js"],"sourcesContent":["import { PromptTemplate } from \"@langchain/core/prompts\";\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\nimport { BaseChain } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAChain } from \"./question_answering/load.js\";\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n/**\n * @deprecated This class will be removed in 1.0.0. See below for an example implementation using\n * `createRetrievalChain`.\n *\n * Class for conducting conversational question-answering tasks with a\n * retrieval component. Extends the BaseChain class and implements the\n * ConversationalRetrievalQAChainInput interface.\n * @example\n * ```typescript\n * import { ChatAnthropic } from \"@langchain/anthropic\";\n * import {\n *   ChatPromptTemplate,\n *   MessagesPlaceholder,\n * } from \"@langchain/core/prompts\";\n * import { BaseMessage } from \"@langchain/core/messages\";\n * import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n * import { createHistoryAwareRetriever } from \"langchain/chains/history_aware_retriever\";\n * import { createRetrievalChain } from \"langchain/chains/retrieval\";\n *\n * const retriever = ...your retriever;\n * const llm = new ChatAnthropic();\n *\n * // Contextualize question\n * const contextualizeQSystemPrompt = `\n * Given a chat history and the latest user question\n * which might reference context in the chat history,\n * formulate a standalone question which can be understood\n * without the chat history. Do NOT answer the question, just\n * reformulate it if needed and otherwise return it as is.`;\n * const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", contextualizeQSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n * const historyAwareRetriever = await createHistoryAwareRetriever({\n *   llm,\n *   retriever,\n *   rephrasePrompt: contextualizeQPrompt,\n * });\n *\n * // Answer question\n * const qaSystemPrompt = `\n * You are an assistant for question-answering tasks. Use\n * the following pieces of retrieved context to answer the\n * question. If you don't know the answer, just say that you\n * don't know. Use three sentences maximum and keep the answer\n * concise.\n * \\n\\n\n * {context}`;\n * const qaPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", qaSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n *\n * // Below we use createStuffDocuments_chain to feed all retrieved context\n * // into the LLM. Note that we can also use StuffDocumentsChain and other\n * // instances of BaseCombineDocumentsChain.\n * const questionAnswerChain = await createStuffDocumentsChain({\n *   llm,\n *   prompt: qaPrompt,\n * });\n *\n * const ragChain = await createRetrievalChain({\n *   retriever: historyAwareRetriever,\n *   combineDocsChain: questionAnswerChain,\n * });\n *\n * // Usage:\n * const chat_history: BaseMessage[] = [];\n * const response = await ragChain.invoke({\n *   chat_history,\n *   input: \"...\",\n * });\n * ```\n */\nexport class ConversationalRetrievalQAChain extends BaseChain {\n    static lc_name() {\n        return \"ConversationalRetrievalQAChain\";\n    }\n    get inputKeys() {\n        return [this.inputKey, this.chatHistoryKey];\n    }\n    get outputKeys() {\n        return this.combineDocumentsChain.outputKeys.concat(this.returnSourceDocuments ? [\"sourceDocuments\"] : []);\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"inputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"question\"\n        });\n        Object.defineProperty(this, \"chatHistoryKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"chat_history\"\n        });\n        Object.defineProperty(this, \"retriever\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"combineDocumentsChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"questionGeneratorChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnSourceDocuments\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"returnGeneratedQuestion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        this.retriever = fields.retriever;\n        this.combineDocumentsChain = fields.combineDocumentsChain;\n        this.questionGeneratorChain = fields.questionGeneratorChain;\n        this.inputKey = fields.inputKey ?? this.inputKey;\n        this.returnSourceDocuments =\n            fields.returnSourceDocuments ?? this.returnSourceDocuments;\n        this.returnGeneratedQuestion =\n            fields.returnGeneratedQuestion ?? this.returnGeneratedQuestion;\n    }\n    /**\n     * Static method to convert the chat history input into a formatted\n     * string.\n     * @param chatHistory Chat history input which can be a string, an array of BaseMessage instances, or an array of string arrays.\n     * @returns A formatted string representing the chat history.\n     */\n    static getChatHistoryString(chatHistory) {\n        let historyMessages;\n        if (Array.isArray(chatHistory)) {\n            // TODO: Deprecate on a breaking release\n            if (Array.isArray(chatHistory[0]) &&\n                typeof chatHistory[0][0] === \"string\") {\n                console.warn(\"Passing chat history as an array of strings is deprecated.\\nPlease see https://js.langchain.com/docs/modules/chains/popular/chat_vector_db#externally-managed-memory for more information.\");\n                historyMessages = chatHistory.flat().map((stringMessage, i) => {\n                    if (i % 2 === 0) {\n                        return new HumanMessage(stringMessage);\n                    }\n                    else {\n                        return new AIMessage(stringMessage);\n                    }\n                });\n            }\n            else {\n                historyMessages = chatHistory;\n            }\n            return historyMessages\n                .map((chatMessage) => {\n                if (chatMessage._getType() === \"human\") {\n                    return `Human: ${chatMessage.content}`;\n                }\n                else if (chatMessage._getType() === \"ai\") {\n                    return `Assistant: ${chatMessage.content}`;\n                }\n                else {\n                    return `${chatMessage.content}`;\n                }\n            })\n                .join(\"\\n\");\n        }\n        return chatHistory;\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        if (!(this.inputKey in values)) {\n            throw new Error(`Question key ${this.inputKey} not found.`);\n        }\n        if (!(this.chatHistoryKey in values)) {\n            throw new Error(`Chat history key ${this.chatHistoryKey} not found.`);\n        }\n        const question = values[this.inputKey];\n        const chatHistory = ConversationalRetrievalQAChain.getChatHistoryString(values[this.chatHistoryKey]);\n        let newQuestion = question;\n        if (chatHistory.length > 0) {\n            const result = await this.questionGeneratorChain.call({\n                question,\n                chat_history: chatHistory,\n            }, runManager?.getChild(\"question_generator\"));\n            const keys = Object.keys(result);\n            if (keys.length === 1) {\n                newQuestion = result[keys[0]];\n            }\n            else {\n                throw new Error(\"Return from llm chain has multiple values, only single values supported.\");\n            }\n        }\n        const docs = await this.retriever.getRelevantDocuments(newQuestion, runManager?.getChild(\"retriever\"));\n        const inputs = {\n            question: newQuestion,\n            input_documents: docs,\n            chat_history: chatHistory,\n        };\n        let result = await this.combineDocumentsChain.call(inputs, runManager?.getChild(\"combine_documents\"));\n        if (this.returnSourceDocuments) {\n            result = {\n                ...result,\n                sourceDocuments: docs,\n            };\n        }\n        if (this.returnGeneratedQuestion) {\n            result = {\n                ...result,\n                generatedQuestion: newQuestion,\n            };\n        }\n        return result;\n    }\n    _chainType() {\n        return \"conversational_retrieval_chain\";\n    }\n    static async deserialize(_data, _values) {\n        throw new Error(\"Not implemented.\");\n    }\n    serialize() {\n        throw new Error(\"Not implemented.\");\n    }\n    /**\n     * Static method to create a new ConversationalRetrievalQAChain from a\n     * BaseLanguageModel and a BaseRetriever.\n     * @param llm {@link BaseLanguageModelInterface} instance used to generate a new question.\n     * @param retriever {@link BaseRetrieverInterface} instance used to retrieve relevant documents.\n     * @param options.returnSourceDocuments Whether to return source documents in the final output\n     * @param options.questionGeneratorChainOptions Options to initialize the standalone question generation chain used as the first internal step\n     * @param options.qaChainOptions {@link QAChainParams} used to initialize the QA chain used as the second internal step\n     * @returns A new instance of ConversationalRetrievalQAChain.\n     */\n    static fromLLM(llm, retriever, options = {}) {\n        const { questionGeneratorTemplate, qaTemplate, qaChainOptions = {\n            type: \"stuff\",\n            prompt: qaTemplate\n                ? PromptTemplate.fromTemplate(qaTemplate)\n                : undefined,\n        }, questionGeneratorChainOptions, verbose, ...rest } = options;\n        const qaChain = loadQAChain(llm, qaChainOptions);\n        const questionGeneratorChainPrompt = PromptTemplate.fromTemplate(questionGeneratorChainOptions?.template ??\n            questionGeneratorTemplate ??\n            question_generator_template);\n        const questionGeneratorChain = new LLMChain({\n            prompt: questionGeneratorChainPrompt,\n            llm: questionGeneratorChainOptions?.llm ?? llm,\n            verbose,\n        });\n        const instance = new this({\n            retriever,\n            combineDocumentsChain: qaChain,\n            questionGeneratorChain,\n            verbose,\n            ...rest,\n        });\n        return instance;\n    }\n}\n"],"mappings":"AAAA,SAASA,cAAc,QAAQ,yBAAyB;AACxD,SAASC,YAAY,EAAEC,SAAS,QAAQ,0BAA0B;AAClE,SAASC,SAAS,QAAQ,WAAW;AACrC,SAASC,QAAQ,QAAQ,gBAAgB;AACzC,SAASC,WAAW,QAAQ,8BAA8B;AAC1D,MAAMC,2BAA2B,GAAG;AACpC;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,8BAA8B,SAASJ,SAAS,CAAC;EAC1D,OAAOK,OAAOA,CAAA,EAAG;IACb,OAAO,gCAAgC;EAC3C;EACA,IAAIC,SAASA,CAAA,EAAG;IACZ,OAAO,CAAC,IAAI,CAACC,QAAQ,EAAE,IAAI,CAACC,cAAc,CAAC;EAC/C;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO,IAAI,CAACC,qBAAqB,CAACD,UAAU,CAACE,MAAM,CAAC,IAAI,CAACC,qBAAqB,GAAG,CAAC,iBAAiB,CAAC,GAAG,EAAE,CAAC;EAC9G;EACAC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,UAAU,EAAE;MACpCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,gBAAgB,EAAE;MAC1CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,uBAAuB,EAAE;MACjDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,wBAAwB,EAAE;MAClDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,uBAAuB,EAAE;MACjDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,yBAAyB,EAAE;MACnDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACF,IAAI,CAACC,SAAS,GAAGP,MAAM,CAACO,SAAS;IACjC,IAAI,CAACX,qBAAqB,GAAGI,MAAM,CAACJ,qBAAqB;IACzD,IAAI,CAACY,sBAAsB,GAAGR,MAAM,CAACQ,sBAAsB;IAC3D,IAAI,CAACf,QAAQ,GAAGO,MAAM,CAACP,QAAQ,IAAI,IAAI,CAACA,QAAQ;IAChD,IAAI,CAACK,qBAAqB,GACtBE,MAAM,CAACF,qBAAqB,IAAI,IAAI,CAACA,qBAAqB;IAC9D,IAAI,CAACW,uBAAuB,GACxBT,MAAM,CAACS,uBAAuB,IAAI,IAAI,CAACA,uBAAuB;EACtE;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,OAAOC,oBAAoBA,CAACC,WAAW,EAAE;IACrC,IAAIC,eAAe;IACnB,IAAIC,KAAK,CAACC,OAAO,CAACH,WAAW,CAAC,EAAE;MAC5B;MACA,IAAIE,KAAK,CAACC,OAAO,CAACH,WAAW,CAAC,CAAC,CAAC,CAAC,IAC7B,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;QACvCI,OAAO,CAACC,IAAI,CAAC,4LAA4L,CAAC;QAC1MJ,eAAe,GAAGD,WAAW,CAACM,IAAI,CAAC,CAAC,CAACC,GAAG,CAAC,CAACC,aAAa,EAAEC,CAAC,KAAK;UAC3D,IAAIA,CAAC,GAAG,CAAC,KAAK,CAAC,EAAE;YACb,OAAO,IAAIpC,YAAY,CAACmC,aAAa,CAAC;UAC1C,CAAC,MACI;YACD,OAAO,IAAIlC,SAAS,CAACkC,aAAa,CAAC;UACvC;QACJ,CAAC,CAAC;MACN,CAAC,MACI;QACDP,eAAe,GAAGD,WAAW;MACjC;MACA,OAAOC,eAAe,CACjBM,GAAG,CAAEG,WAAW,IAAK;QACtB,IAAIA,WAAW,CAACC,QAAQ,CAAC,CAAC,KAAK,OAAO,EAAE;UACpC,OAAO,UAAUD,WAAW,CAACE,OAAO,EAAE;QAC1C,CAAC,MACI,IAAIF,WAAW,CAACC,QAAQ,CAAC,CAAC,KAAK,IAAI,EAAE;UACtC,OAAO,cAAcD,WAAW,CAACE,OAAO,EAAE;QAC9C,CAAC,MACI;UACD,OAAO,GAAGF,WAAW,CAACE,OAAO,EAAE;QACnC;MACJ,CAAC,CAAC,CACGC,IAAI,CAAC,IAAI,CAAC;IACnB;IACA,OAAOb,WAAW;EACtB;EACA;EACA,MAAMc,KAAKA,CAACC,MAAM,EAAEC,UAAU,EAAE;IAC5B,IAAI,EAAE,IAAI,CAAClC,QAAQ,IAAIiC,MAAM,CAAC,EAAE;MAC5B,MAAM,IAAIE,KAAK,CAAC,gBAAgB,IAAI,CAACnC,QAAQ,aAAa,CAAC;IAC/D;IACA,IAAI,EAAE,IAAI,CAACC,cAAc,IAAIgC,MAAM,CAAC,EAAE;MAClC,MAAM,IAAIE,KAAK,CAAC,oBAAoB,IAAI,CAAClC,cAAc,aAAa,CAAC;IACzE;IACA,MAAMmC,QAAQ,GAAGH,MAAM,CAAC,IAAI,CAACjC,QAAQ,CAAC;IACtC,MAAMkB,WAAW,GAAGrB,8BAA8B,CAACoB,oBAAoB,CAACgB,MAAM,CAAC,IAAI,CAAChC,cAAc,CAAC,CAAC;IACpG,IAAIoC,WAAW,GAAGD,QAAQ;IAC1B,IAAIlB,WAAW,CAACoB,MAAM,GAAG,CAAC,EAAE;MACxB,MAAMC,MAAM,GAAG,MAAM,IAAI,CAACxB,sBAAsB,CAACyB,IAAI,CAAC;QAClDJ,QAAQ;QACRK,YAAY,EAAEvB;MAClB,CAAC,EAAEgB,UAAU,EAAEQ,QAAQ,CAAC,oBAAoB,CAAC,CAAC;MAC9C,MAAMC,IAAI,GAAGnC,MAAM,CAACmC,IAAI,CAACJ,MAAM,CAAC;MAChC,IAAII,IAAI,CAACL,MAAM,KAAK,CAAC,EAAE;QACnBD,WAAW,GAAGE,MAAM,CAACI,IAAI,CAAC,CAAC,CAAC,CAAC;MACjC,CAAC,MACI;QACD,MAAM,IAAIR,KAAK,CAAC,0EAA0E,CAAC;MAC/F;IACJ;IACA,MAAMS,IAAI,GAAG,MAAM,IAAI,CAAC9B,SAAS,CAAC+B,oBAAoB,CAACR,WAAW,EAAEH,UAAU,EAAEQ,QAAQ,CAAC,WAAW,CAAC,CAAC;IACtG,MAAMI,MAAM,GAAG;MACXV,QAAQ,EAAEC,WAAW;MACrBU,eAAe,EAAEH,IAAI;MACrBH,YAAY,EAAEvB;IAClB,CAAC;IACD,IAAIqB,MAAM,GAAG,MAAM,IAAI,CAACpC,qBAAqB,CAACqC,IAAI,CAACM,MAAM,EAAEZ,UAAU,EAAEQ,QAAQ,CAAC,mBAAmB,CAAC,CAAC;IACrG,IAAI,IAAI,CAACrC,qBAAqB,EAAE;MAC5BkC,MAAM,GAAG;QACL,GAAGA,MAAM;QACTS,eAAe,EAAEJ;MACrB,CAAC;IACL;IACA,IAAI,IAAI,CAAC5B,uBAAuB,EAAE;MAC9BuB,MAAM,GAAG;QACL,GAAGA,MAAM;QACTU,iBAAiB,EAAEZ;MACvB,CAAC;IACL;IACA,OAAOE,MAAM;EACjB;EACAW,UAAUA,CAAA,EAAG;IACT,OAAO,gCAAgC;EAC3C;EACA,aAAaC,WAAWA,CAACC,KAAK,EAAEC,OAAO,EAAE;IACrC,MAAM,IAAIlB,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACAmB,SAASA,CAAA,EAAG;IACR,MAAM,IAAInB,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,OAAOoB,OAAOA,CAACC,GAAG,EAAE1C,SAAS,EAAE2C,OAAO,GAAG,CAAC,CAAC,EAAE;IACzC,MAAM;MAAEC,yBAAyB;MAAEC,UAAU;MAAEC,cAAc,GAAG;QAC5DC,IAAI,EAAE,OAAO;QACbC,MAAM,EAAEH,UAAU,GACZrE,cAAc,CAACyE,YAAY,CAACJ,UAAU,CAAC,GACvCK;MACV,CAAC;MAAEC,6BAA6B;MAAEC,OAAO;MAAE,GAAGC;IAAK,CAAC,GAAGV,OAAO;IAC9D,MAAMW,OAAO,GAAGzE,WAAW,CAAC6D,GAAG,EAAEI,cAAc,CAAC;IAChD,MAAMS,4BAA4B,GAAG/E,cAAc,CAACyE,YAAY,CAACE,6BAA6B,EAAEK,QAAQ,IACpGZ,yBAAyB,IACzB9D,2BAA2B,CAAC;IAChC,MAAMmB,sBAAsB,GAAG,IAAIrB,QAAQ,CAAC;MACxCoE,MAAM,EAAEO,4BAA4B;MACpCb,GAAG,EAAES,6BAA6B,EAAET,GAAG,IAAIA,GAAG;MAC9CU;IACJ,CAAC,CAAC;IACF,MAAMK,QAAQ,GAAG,IAAI,IAAI,CAAC;MACtBzD,SAAS;MACTX,qBAAqB,EAAEiE,OAAO;MAC9BrD,sBAAsB;MACtBmD,OAAO;MACP,GAAGC;IACP,CAAC,CAAC;IACF,OAAOI,QAAQ;EACnB;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}