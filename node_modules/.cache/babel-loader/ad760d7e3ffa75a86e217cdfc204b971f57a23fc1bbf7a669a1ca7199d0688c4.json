{"ast":null,"code":"import { OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessage, AIMessageChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessageChunk, SystemMessageChunk, ToolMessageChunk, isAIMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport { isOpenAITool } from \"@langchain/core/language_models/base\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { JsonOutputParser, StructuredOutputParser } from \"@langchain/core/output_parsers\";\nimport { JsonOutputKeyToolsParser, convertLangChainToolCallToOpenAI, makeInvalidToolCall, parseToolCall } from \"@langchain/core/output_parsers/openai_tools\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\nimport { getEndpoint } from \"./utils/azure.js\";\nimport { formatToOpenAIToolChoice, wrapOpenAIClientError } from \"./utils/openai.js\";\nimport { formatFunctionDefinitions } from \"./utils/openai-format-fndef.js\";\nimport { _convertToOpenAITool } from \"./utils/tools.js\";\nfunction extractGenericMessageCustomRole(message) {\n  if (message.role !== \"system\" && message.role !== \"developer\" && message.role !== \"assistant\" && message.role !== \"user\" && message.role !== \"function\" && message.role !== \"tool\") {\n    console.warn(`Unknown message role: ${message.role}`);\n  }\n  return message.role;\n}\nexport function messageToOpenAIRole(message) {\n  const type = message._getType();\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"function\":\n      return \"function\";\n    case \"tool\":\n      return \"tool\";\n    case \"generic\":\n      {\n        if (!ChatMessage.isInstance(message)) throw new Error(\"Invalid generic chat message\");\n        return extractGenericMessageCustomRole(message);\n      }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n// Used in LangSmith, export is important here\nexport function _convertMessagesToOpenAIParams(messages, model) {\n  // TODO: Function messages do not support array content, fix cast\n  return messages.flatMap(message => {\n    let role = messageToOpenAIRole(message);\n    if (role === \"system\" && model?.startsWith(\"o1\")) {\n      role = \"developer\";\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const completionParam = {\n      role,\n      content: message.content\n    };\n    if (message.name != null) {\n      completionParam.name = message.name;\n    }\n    if (message.additional_kwargs.function_call != null) {\n      completionParam.function_call = message.additional_kwargs.function_call;\n      completionParam.content = null;\n    }\n    if (isAIMessage(message) && !!message.tool_calls?.length) {\n      completionParam.tool_calls = message.tool_calls.map(convertLangChainToolCallToOpenAI);\n      completionParam.content = null;\n    } else {\n      if (message.additional_kwargs.tool_calls != null) {\n        completionParam.tool_calls = message.additional_kwargs.tool_calls;\n      }\n      if (message.tool_call_id != null) {\n        completionParam.tool_call_id = message.tool_call_id;\n      }\n    }\n    if (message.additional_kwargs.audio && typeof message.additional_kwargs.audio === \"object\" && \"id\" in message.additional_kwargs.audio) {\n      const audioMessage = {\n        role: \"assistant\",\n        audio: {\n          id: message.additional_kwargs.audio.id\n        }\n      };\n      return [completionParam, audioMessage];\n    }\n    return completionParam;\n  });\n}\nfunction _convertChatOpenAIToolTypeToOpenAITool(tool, fields) {\n  if (isOpenAITool(tool)) {\n    if (fields?.strict !== undefined) {\n      return {\n        ...tool,\n        function: {\n          ...tool.function,\n          strict: fields.strict\n        }\n      };\n    }\n    return tool;\n  }\n  return _convertToOpenAITool(tool, fields);\n}\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.bind`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.bind`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.bind({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // baseUrl: \"...\",\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.bind({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.bind` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.bind` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOpenAI extends BaseChatModel {\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n  get callKeys() {\n    return [...super.callKeys, \"options\", \"function_call\", \"functions\", \"tools\", \"tool_choice\", \"promptIndex\", \"response_format\", \"seed\", \"reasoning_effort\"];\n  }\n  get lc_secrets() {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\"\n    };\n  }\n  get lc_aliases() {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      apiKey: \"openai_api_key\"\n    };\n  }\n  constructor(fields) {\n    super(fields ?? {});\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    /** @deprecated Use \"model\" instead */\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"model\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"gpt-3.5-turbo\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stopSequences\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"user\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"streamUsage\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"logprobs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"topLogprobs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"openAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"apiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"organization\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"__includeRawResponse\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    Object.defineProperty(this, \"supportsStrictToolCalling\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"audio\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modalities\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"reasoningEffort\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.openAIApiKey = fields?.apiKey ?? fields?.openAIApiKey ?? fields?.configuration?.apiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.apiKey = this.openAIApiKey;\n    this.organization = fields?.configuration?.organization ?? getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.modelName = this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.maxTokens = fields?.maxTokens;\n    this.logprobs = fields?.logprobs;\n    this.topLogprobs = fields?.topLogprobs;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this?.stop;\n    this.user = fields?.user;\n    this.__includeRawResponse = fields?.__includeRawResponse;\n    this.audio = fields?.audio;\n    this.modalities = fields?.modalities;\n    this.reasoningEffort = fields?.reasoningEffort;\n    if (this.model === \"o1\") {\n      this.disableStreaming = true;\n    }\n    this.streaming = fields?.streaming ?? false;\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration\n    };\n    // If `supportsStrictToolCalling` is explicitly set, use that value.\n    // Else leave undefined so it's not passed to OpenAI.\n    if (fields?.supportsStrictToolCalling !== undefined) {\n      this.supportsStrictToolCalling = fields.supportsStrictToolCalling;\n    }\n  }\n  getLsParams(options) {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"openai\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.temperature ?? undefined,\n      ls_max_tokens: params.max_tokens ?? undefined,\n      ls_stop: options.stop\n    };\n  }\n  bindTools(tools, kwargs) {\n    let strict;\n    if (kwargs?.strict !== undefined) {\n      strict = kwargs.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n    return this.bind({\n      tools: tools.map(tool => _convertChatOpenAIToolTypeToOpenAITool(tool, {\n        strict\n      })),\n      ...kwargs\n    });\n  }\n  createResponseFormat(resFormat) {\n    if (resFormat && resFormat.type === \"json_schema\" && resFormat.json_schema.schema && isZodSchema(resFormat.json_schema.schema)) {\n      return zodResponseFormat(resFormat.json_schema.schema, resFormat.json_schema.name, {\n        description: resFormat.json_schema.description\n      });\n    }\n    return resFormat;\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(options, extra) {\n    let strict;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n    let streamOptionsConfig = {};\n    if (options?.stream_options !== undefined) {\n      streamOptionsConfig = {\n        stream_options: options.stream_options\n      };\n    } else if (this.streamUsage && (this.streaming || extra?.streaming)) {\n      streamOptionsConfig = {\n        stream_options: {\n          include_usage: true\n        }\n      };\n    }\n    const params = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      logprobs: this.logprobs,\n      top_logprobs: this.topLogprobs,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      functions: options?.functions,\n      function_call: options?.function_call,\n      tools: options?.tools?.length ? options.tools.map(tool => _convertChatOpenAIToolTypeToOpenAITool(tool, {\n        strict\n      })) : undefined,\n      tool_choice: formatToOpenAIToolChoice(options?.tool_choice),\n      response_format: this.createResponseFormat(options?.response_format),\n      seed: options?.seed,\n      ...streamOptionsConfig,\n      parallel_tool_calls: options?.parallel_tool_calls,\n      ...(this.audio || options?.audio ? {\n        audio: this.audio || options?.audio\n      } : {}),\n      ...(this.modalities || options?.modalities ? {\n        modalities: this.modalities || options?.modalities\n      } : {}),\n      ...this.modelKwargs\n    };\n    if (options?.prediction !== undefined) {\n      params.prediction = options.prediction;\n    }\n    const reasoningEffort = options?.reasoning_effort ?? this.reasoningEffort;\n    if (reasoningEffort !== undefined) {\n      params.reasoning_effort = reasoningEffort;\n    }\n    return params;\n  }\n  _convertOpenAIChatCompletionMessageToBaseMessage(message, rawResponse) {\n    const rawToolCalls = message.tool_calls;\n    switch (message.role) {\n      case \"assistant\":\n        {\n          const toolCalls = [];\n          const invalidToolCalls = [];\n          for (const rawToolCall of rawToolCalls ?? []) {\n            try {\n              toolCalls.push(parseToolCall(rawToolCall, {\n                returnId: true\n              }));\n              // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            } catch (e) {\n              invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n            }\n          }\n          const additional_kwargs = {\n            function_call: message.function_call,\n            tool_calls: rawToolCalls\n          };\n          if (this.__includeRawResponse !== undefined) {\n            additional_kwargs.__raw_response = rawResponse;\n          }\n          const response_metadata = {\n            model_name: rawResponse.model,\n            ...(rawResponse.system_fingerprint ? {\n              usage: {\n                ...rawResponse.usage\n              },\n              system_fingerprint: rawResponse.system_fingerprint\n            } : {})\n          };\n          if (message.audio) {\n            additional_kwargs.audio = message.audio;\n          }\n          return new AIMessage({\n            content: message.content || \"\",\n            tool_calls: toolCalls,\n            invalid_tool_calls: invalidToolCalls,\n            additional_kwargs,\n            response_metadata,\n            id: rawResponse.id\n          });\n        }\n      default:\n        return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n    }\n  }\n  _convertOpenAIDeltaToBaseMessageChunk(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  delta, rawResponse, defaultRole) {\n    const role = delta.role ?? defaultRole;\n    const content = delta.content ?? \"\";\n    let additional_kwargs;\n    if (delta.function_call) {\n      additional_kwargs = {\n        function_call: delta.function_call\n      };\n    } else if (delta.tool_calls) {\n      additional_kwargs = {\n        tool_calls: delta.tool_calls\n      };\n    } else {\n      additional_kwargs = {};\n    }\n    if (this.__includeRawResponse) {\n      additional_kwargs.__raw_response = rawResponse;\n    }\n    if (delta.audio) {\n      additional_kwargs.audio = {\n        ...delta.audio,\n        index: rawResponse.choices[0].index\n      };\n    }\n    const response_metadata = {\n      usage: {\n        ...rawResponse.usage\n      }\n    };\n    if (role === \"user\") {\n      return new HumanMessageChunk({\n        content,\n        response_metadata\n      });\n    } else if (role === \"assistant\") {\n      const toolCallChunks = [];\n      if (Array.isArray(delta.tool_calls)) {\n        for (const rawToolCall of delta.tool_calls) {\n          toolCallChunks.push({\n            name: rawToolCall.function?.name,\n            args: rawToolCall.function?.arguments,\n            id: rawToolCall.id,\n            index: rawToolCall.index,\n            type: \"tool_call_chunk\"\n          });\n        }\n      }\n      return new AIMessageChunk({\n        content,\n        tool_call_chunks: toolCallChunks,\n        additional_kwargs,\n        id: rawResponse.id,\n        response_metadata\n      });\n    } else if (role === \"system\") {\n      return new SystemMessageChunk({\n        content,\n        response_metadata\n      });\n    } else if (role === \"developer\") {\n      return new SystemMessageChunk({\n        content,\n        response_metadata,\n        additional_kwargs: {\n          __openai_role__: \"developer\"\n        }\n      });\n    } else if (role === \"function\") {\n      return new FunctionMessageChunk({\n        content,\n        additional_kwargs,\n        name: delta.name,\n        response_metadata\n      });\n    } else if (role === \"tool\") {\n      return new ToolMessageChunk({\n        content,\n        additional_kwargs,\n        tool_call_id: delta.tool_call_id,\n        response_metadata\n      });\n    } else {\n      return new ChatMessageChunk({\n        content,\n        role,\n        response_metadata\n      });\n    }\n  }\n  /** @ignore */\n  _identifyingParams() {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  async *_streamResponseChunks(messages, options, runManager) {\n    const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);\n    const params = {\n      ...this.invocationParams(options, {\n        streaming: true\n      }),\n      messages: messagesMapped,\n      stream: true\n    };\n    let defaultRole;\n    const streamIterable = await this.completionWithRetry(params, options);\n    let usage;\n    for await (const data of streamIterable) {\n      const choice = data?.choices?.[0];\n      if (data.usage) {\n        usage = data.usage;\n      }\n      if (!choice) {\n        continue;\n      }\n      const {\n        delta\n      } = choice;\n      if (!delta) {\n        continue;\n      }\n      const chunk = this._convertOpenAIDeltaToBaseMessageChunk(delta, data, defaultRole);\n      defaultRole = delta.role ?? defaultRole;\n      const newTokenIndices = {\n        prompt: options.promptIndex ?? 0,\n        completion: choice.index ?? 0\n      };\n      if (typeof chunk.content !== \"string\") {\n        console.log(\"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\");\n        continue;\n      }\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      const generationInfo = {\n        ...newTokenIndices\n      };\n      if (choice.finish_reason != null) {\n        generationInfo.finish_reason = choice.finish_reason;\n        // Only include system fingerprint in the last chunk for now\n        // to avoid concatenation issues\n        generationInfo.system_fingerprint = data.system_fingerprint;\n        generationInfo.model_name = data.model;\n      }\n      if (this.logprobs) {\n        generationInfo.logprobs = choice.logprobs;\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo\n      });\n      yield generationChunk;\n      await runManager?.handleLLMNewToken(generationChunk.text ?? \"\", newTokenIndices, undefined, undefined, undefined, {\n        chunk: generationChunk\n      });\n    }\n    if (usage) {\n      const inputTokenDetails = {\n        ...(usage.prompt_tokens_details?.audio_tokens !== null && {\n          audio: usage.prompt_tokens_details?.audio_tokens\n        }),\n        ...(usage.prompt_tokens_details?.cached_tokens !== null && {\n          cache_read: usage.prompt_tokens_details?.cached_tokens\n        })\n      };\n      const outputTokenDetails = {\n        ...(usage.completion_tokens_details?.audio_tokens !== null && {\n          audio: usage.completion_tokens_details?.audio_tokens\n        }),\n        ...(usage.completion_tokens_details?.reasoning_tokens !== null && {\n          reasoning: usage.completion_tokens_details?.reasoning_tokens\n        })\n      };\n      const generationChunk = new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: \"\",\n          response_metadata: {\n            usage: {\n              ...usage\n            }\n          },\n          usage_metadata: {\n            input_tokens: usage.prompt_tokens,\n            output_tokens: usage.completion_tokens,\n            total_tokens: usage.total_tokens,\n            ...(Object.keys(inputTokenDetails).length > 0 && {\n              input_token_details: inputTokenDetails\n            }),\n            ...(Object.keys(outputTokenDetails).length > 0 && {\n              output_token_details: outputTokenDetails\n            })\n          }\n        }),\n        text: \"\"\n      });\n      yield generationChunk;\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n  /**\n   * Get the identifying parameters for the model\n   *\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /** @ignore */\n  async _generate(messages, options, runManager) {\n    const usageMetadata = {};\n    const params = this.invocationParams(options);\n    const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);\n    if (params.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks = {};\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata\n        };\n        const index = chunk.generationInfo?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks).sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10)).map(([_, value]) => value);\n      const {\n        functions,\n        function_call\n      } = this.invocationParams(options);\n      // OpenAI does not support token usage report under stream mode,\n      // fallback to estimation.\n      const promptTokenUsage = await this.getEstimatedTokenCountFromPrompt(messages, functions, function_call);\n      const completionTokenUsage = await this.getNumTokensFromGenerations(generations);\n      usageMetadata.input_tokens = promptTokenUsage;\n      usageMetadata.output_tokens = completionTokenUsage;\n      usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens\n          }\n        }\n      };\n    } else {\n      let data;\n      if (options.response_format && options.response_format.type === \"json_schema\") {\n        data = await this.betaParsedCompletionWithRetry({\n          ...params,\n          stream: false,\n          messages: messagesMapped\n        }, {\n          signal: options?.signal,\n          ...options?.options\n        });\n      } else {\n        data = await this.completionWithRetry({\n          ...params,\n          stream: false,\n          messages: messagesMapped\n        }, {\n          signal: options?.signal,\n          ...options?.options\n        });\n      }\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n        prompt_tokens_details: promptTokensDetails,\n        completion_tokens_details: completionTokensDetails\n      } = data?.usage ?? {};\n      if (completionTokens) {\n        usageMetadata.output_tokens = (usageMetadata.output_tokens ?? 0) + completionTokens;\n      }\n      if (promptTokens) {\n        usageMetadata.input_tokens = (usageMetadata.input_tokens ?? 0) + promptTokens;\n      }\n      if (totalTokens) {\n        usageMetadata.total_tokens = (usageMetadata.total_tokens ?? 0) + totalTokens;\n      }\n      if (promptTokensDetails?.audio_tokens !== null || promptTokensDetails?.cached_tokens !== null) {\n        usageMetadata.input_token_details = {\n          ...(promptTokensDetails?.audio_tokens !== null && {\n            audio: promptTokensDetails?.audio_tokens\n          }),\n          ...(promptTokensDetails?.cached_tokens !== null && {\n            cache_read: promptTokensDetails?.cached_tokens\n          })\n        };\n      }\n      if (completionTokensDetails?.audio_tokens !== null || completionTokensDetails?.reasoning_tokens !== null) {\n        usageMetadata.output_token_details = {\n          ...(completionTokensDetails?.audio_tokens !== null && {\n            audio: completionTokensDetails?.audio_tokens\n          }),\n          ...(completionTokensDetails?.reasoning_tokens !== null && {\n            reasoning: completionTokensDetails?.reasoning_tokens\n          })\n        };\n      }\n      const generations = [];\n      for (const part of data?.choices ?? []) {\n        const text = part.message?.content ?? \"\";\n        const generation = {\n          text,\n          message: this._convertOpenAIChatCompletionMessageToBaseMessage(part.message ?? {\n            role: \"assistant\"\n          }, data)\n        };\n        generation.generationInfo = {\n          ...(part.finish_reason ? {\n            finish_reason: part.finish_reason\n          } : {}),\n          ...(part.logprobs ? {\n            logprobs: part.logprobs\n          } : {})\n        };\n        if (isAIMessage(generation.message)) {\n          generation.message.usage_metadata = usageMetadata;\n        }\n        // Fields are not serialized unless passed to the constructor\n        // Doing this ensures all fields on the message are serialized\n        generation.message = new AIMessage(Object.fromEntries(Object.entries(generation.message).filter(([key]) => !key.startsWith(\"lc_\"))));\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens\n          }\n        }\n      };\n    }\n  }\n  /**\n   * Estimate the number of tokens a prompt will use.\n   * Modified from: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts\n   */\n  async getEstimatedTokenCountFromPrompt(messages, functions, function_call) {\n    // It appears that if functions are present, the first system message is padded with a trailing newline. This\n    // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n    let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n    // If there are functions, add the function definitions as they count towards token usage\n    if (functions && function_call !== \"auto\") {\n      const promptDefinitions = formatFunctionDefinitions(functions);\n      tokens += await this.getNumTokens(promptDefinitions);\n      tokens += 9; // Add nine per completion\n    }\n    // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n    // functions typically add a system message, but reuse the first one if it's already there. This offsets\n    // the extra 9 tokens added by the function definitions.\n    if (functions && messages.find(m => m._getType() === \"system\")) {\n      tokens -= 4;\n    }\n    // If function_call is 'none', add one token.\n    // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n    // If it's undefined or 'auto', don't add anything.\n    if (function_call === \"none\") {\n      tokens += 1;\n    } else if (typeof function_call === \"object\") {\n      tokens += (await this.getNumTokens(function_call.name)) + 4;\n    }\n    return tokens;\n  }\n  /**\n   * Estimate the number of tokens an array of generations have used.\n   */\n  async getNumTokensFromGenerations(generations) {\n    const generationUsages = await Promise.all(generations.map(async generation => {\n      if (generation.message.additional_kwargs?.function_call) {\n        return (await this.getNumTokensFromMessages([generation.message])).countPerMessage[0];\n      } else {\n        return await this.getNumTokens(generation.message.content);\n      }\n    }));\n    return generationUsages.reduce((a, b) => a + b, 0);\n  }\n  async getNumTokensFromMessages(messages) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (this.model === \"gpt-3.5-turbo-0301\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n    const countPerMessage = await Promise.all(messages.map(async message => {\n      const textCount = await this.getNumTokens(message.content);\n      const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n      const nameCount = message.name !== undefined ? tokensPerName + (await this.getNumTokens(message.name)) : 0;\n      let count = textCount + tokensPerMessage + roleCount + nameCount;\n      // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n      const openAIMessage = message;\n      if (openAIMessage._getType() === \"function\") {\n        count -= 2;\n      }\n      if (openAIMessage.additional_kwargs?.function_call) {\n        count += 3;\n      }\n      if (openAIMessage?.additional_kwargs.function_call?.name) {\n        count += await this.getNumTokens(openAIMessage.additional_kwargs.function_call?.name);\n      }\n      if (openAIMessage.additional_kwargs.function_call?.arguments) {\n        try {\n          count += await this.getNumTokens(\n          // Remove newlines and spaces\n          JSON.stringify(JSON.parse(openAIMessage.additional_kwargs.function_call?.arguments)));\n        } catch (error) {\n          console.error(\"Error parsing function arguments\", error, JSON.stringify(openAIMessage.additional_kwargs.function_call));\n          count += await this.getNumTokens(openAIMessage.additional_kwargs.function_call?.arguments);\n        }\n      }\n      totalCount += count;\n      return count;\n    }));\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n    return {\n      totalCount,\n      countPerMessage\n    };\n  }\n  async completionWithRetry(request, options) {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.chat.completions.create(request, requestOptions);\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n  /**\n   * Call the beta chat completions parse endpoint. This should only be called if\n   * response_format is set to \"json_object\".\n   * @param {OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming} request\n   * @param {OpenAICoreRequestOptions | undefined} options\n   */\n  async betaParsedCompletionWithRetry(request, options\n  // Avoid relying importing a beta type with no official entrypoint\n  ) {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.beta.chat.completions.parse(request, requestOptions);\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n  _getClientOptions(options) {\n    if (!this.client) {\n      const openAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL\n      };\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options\n    };\n    return requestOptions;\n  }\n  _llmType() {\n    return \"openai\";\n  }\n  /** @ignore */\n  _combineLLMOutput(...llmOutputs) {\n    return llmOutputs.reduce((acc, llmOutput) => {\n      if (llmOutput && llmOutput.tokenUsage) {\n        acc.tokenUsage.completionTokens += llmOutput.tokenUsage.completionTokens ?? 0;\n        acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n        acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n      }\n      return acc;\n    }, {\n      tokenUsage: {\n        completionTokens: 0,\n        promptTokens: 0,\n        totalTokens: 0\n      }\n    });\n  }\n  withStructuredOutput(outputSchema, config) {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    let schema;\n    let name;\n    let method;\n    let includeRaw;\n    if (isStructuredOutputMethodParams(outputSchema)) {\n      schema = outputSchema.schema;\n      name = outputSchema.name;\n      method = outputSchema.method;\n      includeRaw = outputSchema.includeRaw;\n    } else {\n      schema = outputSchema;\n      name = config?.name;\n      method = config?.method;\n      includeRaw = config?.includeRaw;\n    }\n    let llm;\n    let outputParser;\n    if (config?.strict !== undefined && method === \"jsonMode\") {\n      throw new Error(\"Argument `strict` is only supported for `method` = 'function_calling'\");\n    }\n    if (!this.model.startsWith(\"gpt-3\") && !this.model.startsWith(\"gpt-4-\") && this.model !== \"gpt-4\") {\n      if (method === undefined) {\n        method = \"jsonSchema\";\n      }\n    } else if (method === \"jsonSchema\") {\n      console.warn(`[WARNING]: JSON Schema is not supported for model \"${this.model}\". Falling back to tool calling.`);\n    }\n    if (method === \"jsonMode\") {\n      llm = this.bind({\n        response_format: {\n          type: \"json_object\"\n        }\n      });\n      if (isZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser();\n      }\n    } else if (method === \"jsonSchema\") {\n      llm = this.bind({\n        response_format: {\n          type: \"json_schema\",\n          json_schema: {\n            name: name ?? \"extract\",\n            description: schema.description,\n            schema,\n            strict: config?.strict\n          }\n        }\n      });\n      if (isZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser();\n      }\n    } else {\n      let functionName = name ?? \"extract\";\n      // Is function calling\n      if (isZodSchema(schema)) {\n        const asJsonSchema = zodToJsonSchema(schema);\n        llm = this.bind({\n          tools: [{\n            type: \"function\",\n            function: {\n              name: functionName,\n              description: asJsonSchema.description,\n              parameters: asJsonSchema\n            }\n          }],\n          tool_choice: {\n            type: \"function\",\n            function: {\n              name: functionName\n            }\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? {\n            strict: config.strict\n          } : {})\n        });\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema\n        });\n      } else {\n        let openAIFunctionDefinition;\n        if (typeof schema.name === \"string\" && typeof schema.parameters === \"object\" && schema.parameters != null) {\n          openAIFunctionDefinition = schema;\n          functionName = schema.name;\n        } else {\n          functionName = schema.title ?? functionName;\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: schema.description ?? \"\",\n            parameters: schema\n          };\n        }\n        llm = this.bind({\n          tools: [{\n            type: \"function\",\n            function: openAIFunctionDefinition\n          }],\n          tool_choice: {\n            type: \"function\",\n            function: {\n              name: functionName\n            }\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? {\n            strict: config.strict\n          } : {})\n        });\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName\n        });\n      }\n    }\n    if (!includeRaw) {\n      return llm.pipe(outputParser);\n    }\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input, config) => outputParser.invoke(input.raw, config)\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone]\n    });\n    return RunnableSequence.from([{\n      raw: llm\n    }, parsedWithFallback]);\n  }\n}\nfunction isZodSchema(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ninput) {\n  // Check for a characteristic method of Zod schemas\n  return typeof input?.parse === \"function\";\n}\nfunction isStructuredOutputMethodParams(x\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n) {\n  return x !== undefined &&\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  typeof x.schema === \"object\";\n}","map":{"version":3,"names":["OpenAI","OpenAIClient","AIMessage","AIMessageChunk","ChatMessage","ChatMessageChunk","FunctionMessageChunk","HumanMessageChunk","SystemMessageChunk","ToolMessageChunk","isAIMessage","ChatGenerationChunk","getEnvironmentVariable","BaseChatModel","isOpenAITool","RunnablePassthrough","RunnableSequence","JsonOutputParser","StructuredOutputParser","JsonOutputKeyToolsParser","convertLangChainToolCallToOpenAI","makeInvalidToolCall","parseToolCall","zodToJsonSchema","zodResponseFormat","getEndpoint","formatToOpenAIToolChoice","wrapOpenAIClientError","formatFunctionDefinitions","_convertToOpenAITool","extractGenericMessageCustomRole","message","role","console","warn","messageToOpenAIRole","type","_getType","isInstance","Error","_convertMessagesToOpenAIParams","messages","model","flatMap","startsWith","completionParam","content","name","additional_kwargs","function_call","tool_calls","length","map","tool_call_id","audio","audioMessage","id","_convertChatOpenAIToolTypeToOpenAITool","tool","fields","strict","undefined","function","ChatOpenAI","lc_name","callKeys","lc_secrets","openAIApiKey","apiKey","organization","lc_aliases","modelName","constructor","Object","defineProperty","enumerable","configurable","writable","value","configuration","modelKwargs","timeout","temperature","topP","frequencyPenalty","presencePenalty","maxTokens","logprobs","topLogprobs","n","logitBias","stop","stopSequences","user","__includeRawResponse","modalities","reasoningEffort","disableStreaming","streaming","streamUsage","clientConfig","dangerouslyAllowBrowser","supportsStrictToolCalling","getLsParams","options","params","invocationParams","ls_provider","ls_model_name","ls_model_type","ls_temperature","ls_max_tokens","max_tokens","ls_stop","bindTools","tools","kwargs","bind","createResponseFormat","resFormat","json_schema","schema","isZodSchema","description","extra","streamOptionsConfig","stream_options","include_usage","top_p","frequency_penalty","presence_penalty","top_logprobs","logit_bias","stream","functions","tool_choice","response_format","seed","parallel_tool_calls","prediction","reasoning_effort","_convertOpenAIChatCompletionMessageToBaseMessage","rawResponse","rawToolCalls","toolCalls","invalidToolCalls","rawToolCall","push","returnId","e","__raw_response","response_metadata","model_name","system_fingerprint","usage","invalid_tool_calls","_convertOpenAIDeltaToBaseMessageChunk","delta","defaultRole","index","choices","toolCallChunks","Array","isArray","args","arguments","tool_call_chunks","__openai_role__","_identifyingParams","_streamResponseChunks","runManager","messagesMapped","streamIterable","completionWithRetry","data","choice","chunk","newTokenIndices","prompt","promptIndex","completion","log","generationInfo","finish_reason","generationChunk","text","handleLLMNewToken","inputTokenDetails","prompt_tokens_details","audio_tokens","cached_tokens","cache_read","outputTokenDetails","completion_tokens_details","reasoning_tokens","reasoning","usage_metadata","input_tokens","prompt_tokens","output_tokens","completion_tokens","total_tokens","keys","input_token_details","output_token_details","signal","aborted","identifyingParams","_generate","usageMetadata","finalChunks","concat","generations","entries","sort","aKey","bKey","parseInt","_","promptTokenUsage","getEstimatedTokenCountFromPrompt","completionTokenUsage","getNumTokensFromGenerations","llmOutput","estimatedTokenUsage","promptTokens","completionTokens","totalTokens","betaParsedCompletionWithRetry","promptTokensDetails","completionTokensDetails","part","generation","fromEntries","filter","key","tokenUsage","tokens","getNumTokensFromMessages","totalCount","promptDefinitions","getNumTokens","find","m","generationUsages","Promise","all","countPerMessage","reduce","a","b","tokensPerMessage","tokensPerName","textCount","roleCount","nameCount","count","openAIMessage","JSON","stringify","parse","error","request","requestOptions","_getClientOptions","caller","call","res","client","chat","completions","create","beta","openAIEndpointConfig","baseURL","endpoint","maxRetries","_llmType","_combineLLMOutput","llmOutputs","acc","withStructuredOutput","outputSchema","config","method","includeRaw","isStructuredOutputMethodParams","llm","outputParser","fromZodSchema","functionName","asJsonSchema","parameters","returnSingle","keyName","zodSchema","openAIFunctionDefinition","title","pipe","parserAssign","assign","parsed","input","invoke","raw","parserNone","parsedWithFallback","withFallbacks","fallbacks","from","x"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/@langchain/openai/dist/chat_models.js"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessage, AIMessageChunk, ChatMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessageChunk, SystemMessageChunk, ToolMessageChunk, isAIMessage, } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { BaseChatModel, } from \"@langchain/core/language_models/chat_models\";\nimport { isOpenAITool, } from \"@langchain/core/language_models/base\";\nimport { RunnablePassthrough, RunnableSequence, } from \"@langchain/core/runnables\";\nimport { JsonOutputParser, StructuredOutputParser, } from \"@langchain/core/output_parsers\";\nimport { JsonOutputKeyToolsParser, convertLangChainToolCallToOpenAI, makeInvalidToolCall, parseToolCall, } from \"@langchain/core/output_parsers/openai_tools\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\nimport { getEndpoint } from \"./utils/azure.js\";\nimport { formatToOpenAIToolChoice, wrapOpenAIClientError, } from \"./utils/openai.js\";\nimport { formatFunctionDefinitions, } from \"./utils/openai-format-fndef.js\";\nimport { _convertToOpenAITool } from \"./utils/tools.js\";\nfunction extractGenericMessageCustomRole(message) {\n    if (message.role !== \"system\" &&\n        message.role !== \"developer\" &&\n        message.role !== \"assistant\" &&\n        message.role !== \"user\" &&\n        message.role !== \"function\" &&\n        message.role !== \"tool\") {\n        console.warn(`Unknown message role: ${message.role}`);\n    }\n    return message.role;\n}\nexport function messageToOpenAIRole(message) {\n    const type = message._getType();\n    switch (type) {\n        case \"system\":\n            return \"system\";\n        case \"ai\":\n            return \"assistant\";\n        case \"human\":\n            return \"user\";\n        case \"function\":\n            return \"function\";\n        case \"tool\":\n            return \"tool\";\n        case \"generic\": {\n            if (!ChatMessage.isInstance(message))\n                throw new Error(\"Invalid generic chat message\");\n            return extractGenericMessageCustomRole(message);\n        }\n        default:\n            throw new Error(`Unknown message type: ${type}`);\n    }\n}\n// Used in LangSmith, export is important here\nexport function _convertMessagesToOpenAIParams(messages, model) {\n    // TODO: Function messages do not support array content, fix cast\n    return messages.flatMap((message) => {\n        let role = messageToOpenAIRole(message);\n        if (role === \"system\" && model?.startsWith(\"o1\")) {\n            role = \"developer\";\n        }\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        const completionParam = {\n            role,\n            content: message.content,\n        };\n        if (message.name != null) {\n            completionParam.name = message.name;\n        }\n        if (message.additional_kwargs.function_call != null) {\n            completionParam.function_call = message.additional_kwargs.function_call;\n            completionParam.content = null;\n        }\n        if (isAIMessage(message) && !!message.tool_calls?.length) {\n            completionParam.tool_calls = message.tool_calls.map(convertLangChainToolCallToOpenAI);\n            completionParam.content = null;\n        }\n        else {\n            if (message.additional_kwargs.tool_calls != null) {\n                completionParam.tool_calls = message.additional_kwargs.tool_calls;\n            }\n            if (message.tool_call_id != null) {\n                completionParam.tool_call_id = message.tool_call_id;\n            }\n        }\n        if (message.additional_kwargs.audio &&\n            typeof message.additional_kwargs.audio === \"object\" &&\n            \"id\" in message.additional_kwargs.audio) {\n            const audioMessage = {\n                role: \"assistant\",\n                audio: {\n                    id: message.additional_kwargs.audio.id,\n                },\n            };\n            return [completionParam, audioMessage];\n        }\n        return completionParam;\n    });\n}\nfunction _convertChatOpenAIToolTypeToOpenAITool(tool, fields) {\n    if (isOpenAITool(tool)) {\n        if (fields?.strict !== undefined) {\n            return {\n                ...tool,\n                function: {\n                    ...tool.function,\n                    strict: fields.strict,\n                },\n            };\n        }\n        return tool;\n    }\n    return _convertToOpenAITool(tool, fields);\n}\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.bind`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.bind`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.bind({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // baseUrl: \"...\",\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.bind({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.bind` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.bind` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOpenAI extends BaseChatModel {\n    static lc_name() {\n        return \"ChatOpenAI\";\n    }\n    get callKeys() {\n        return [\n            ...super.callKeys,\n            \"options\",\n            \"function_call\",\n            \"functions\",\n            \"tools\",\n            \"tool_choice\",\n            \"promptIndex\",\n            \"response_format\",\n            \"seed\",\n            \"reasoning_effort\",\n        ];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            apiKey: \"OPENAI_API_KEY\",\n            organization: \"OPENAI_ORGANIZATION\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            apiKey: \"openai_api_key\",\n        };\n    }\n    constructor(fields) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        /** @deprecated Use \"model\" instead */\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"model\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stopSequences\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"user\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"streamUsage\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"logprobs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"topLogprobs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"apiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"organization\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"__includeRawResponse\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        /**\n         * Whether the model supports the `strict` argument when passing in tools.\n         * If `undefined` the `strict` argument will not be passed to OpenAI.\n         */\n        Object.defineProperty(this, \"supportsStrictToolCalling\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"audio\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modalities\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"reasoningEffort\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.apiKey ??\n                fields?.openAIApiKey ??\n                fields?.configuration?.apiKey ??\n                getEnvironmentVariable(\"OPENAI_API_KEY\");\n        this.apiKey = this.openAIApiKey;\n        this.organization =\n            fields?.configuration?.organization ??\n                getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n        this.model = fields?.model ?? fields?.modelName ?? this.model;\n        this.modelName = this.model;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.maxTokens = fields?.maxTokens;\n        this.logprobs = fields?.logprobs;\n        this.topLogprobs = fields?.topLogprobs;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stopSequences ?? fields?.stop;\n        this.stopSequences = this?.stop;\n        this.user = fields?.user;\n        this.__includeRawResponse = fields?.__includeRawResponse;\n        this.audio = fields?.audio;\n        this.modalities = fields?.modalities;\n        this.reasoningEffort = fields?.reasoningEffort;\n        if (this.model === \"o1\") {\n            this.disableStreaming = true;\n        }\n        this.streaming = fields?.streaming ?? false;\n        this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n        this.clientConfig = {\n            apiKey: this.apiKey,\n            organization: this.organization,\n            dangerouslyAllowBrowser: true,\n            ...fields?.configuration,\n        };\n        // If `supportsStrictToolCalling` is explicitly set, use that value.\n        // Else leave undefined so it's not passed to OpenAI.\n        if (fields?.supportsStrictToolCalling !== undefined) {\n            this.supportsStrictToolCalling = fields.supportsStrictToolCalling;\n        }\n    }\n    getLsParams(options) {\n        const params = this.invocationParams(options);\n        return {\n            ls_provider: \"openai\",\n            ls_model_name: this.model,\n            ls_model_type: \"chat\",\n            ls_temperature: params.temperature ?? undefined,\n            ls_max_tokens: params.max_tokens ?? undefined,\n            ls_stop: options.stop,\n        };\n    }\n    bindTools(tools, kwargs) {\n        let strict;\n        if (kwargs?.strict !== undefined) {\n            strict = kwargs.strict;\n        }\n        else if (this.supportsStrictToolCalling !== undefined) {\n            strict = this.supportsStrictToolCalling;\n        }\n        return this.bind({\n            tools: tools.map((tool) => _convertChatOpenAIToolTypeToOpenAITool(tool, { strict })),\n            ...kwargs,\n        });\n    }\n    createResponseFormat(resFormat) {\n        if (resFormat &&\n            resFormat.type === \"json_schema\" &&\n            resFormat.json_schema.schema &&\n            isZodSchema(resFormat.json_schema.schema)) {\n            return zodResponseFormat(resFormat.json_schema.schema, resFormat.json_schema.name, {\n                description: resFormat.json_schema.description,\n            });\n        }\n        return resFormat;\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options, extra) {\n        let strict;\n        if (options?.strict !== undefined) {\n            strict = options.strict;\n        }\n        else if (this.supportsStrictToolCalling !== undefined) {\n            strict = this.supportsStrictToolCalling;\n        }\n        let streamOptionsConfig = {};\n        if (options?.stream_options !== undefined) {\n            streamOptionsConfig = { stream_options: options.stream_options };\n        }\n        else if (this.streamUsage && (this.streaming || extra?.streaming)) {\n            streamOptionsConfig = { stream_options: { include_usage: true } };\n        }\n        const params = {\n            model: this.model,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            logprobs: this.logprobs,\n            top_logprobs: this.topLogprobs,\n            n: this.n,\n            logit_bias: this.logitBias,\n            stop: options?.stop ?? this.stopSequences,\n            user: this.user,\n            // if include_usage is set or streamUsage then stream must be set to true.\n            stream: this.streaming,\n            functions: options?.functions,\n            function_call: options?.function_call,\n            tools: options?.tools?.length\n                ? options.tools.map((tool) => _convertChatOpenAIToolTypeToOpenAITool(tool, { strict }))\n                : undefined,\n            tool_choice: formatToOpenAIToolChoice(options?.tool_choice),\n            response_format: this.createResponseFormat(options?.response_format),\n            seed: options?.seed,\n            ...streamOptionsConfig,\n            parallel_tool_calls: options?.parallel_tool_calls,\n            ...(this.audio || options?.audio\n                ? { audio: this.audio || options?.audio }\n                : {}),\n            ...(this.modalities || options?.modalities\n                ? { modalities: this.modalities || options?.modalities }\n                : {}),\n            ...this.modelKwargs,\n        };\n        if (options?.prediction !== undefined) {\n            params.prediction = options.prediction;\n        }\n        const reasoningEffort = options?.reasoning_effort ?? this.reasoningEffort;\n        if (reasoningEffort !== undefined) {\n            params.reasoning_effort = reasoningEffort;\n        }\n        return params;\n    }\n    _convertOpenAIChatCompletionMessageToBaseMessage(message, rawResponse) {\n        const rawToolCalls = message.tool_calls;\n        switch (message.role) {\n            case \"assistant\": {\n                const toolCalls = [];\n                const invalidToolCalls = [];\n                for (const rawToolCall of rawToolCalls ?? []) {\n                    try {\n                        toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n                        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n                    }\n                    catch (e) {\n                        invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n                    }\n                }\n                const additional_kwargs = {\n                    function_call: message.function_call,\n                    tool_calls: rawToolCalls,\n                };\n                if (this.__includeRawResponse !== undefined) {\n                    additional_kwargs.__raw_response = rawResponse;\n                }\n                const response_metadata = {\n                    model_name: rawResponse.model,\n                    ...(rawResponse.system_fingerprint\n                        ? {\n                            usage: { ...rawResponse.usage },\n                            system_fingerprint: rawResponse.system_fingerprint,\n                        }\n                        : {}),\n                };\n                if (message.audio) {\n                    additional_kwargs.audio = message.audio;\n                }\n                return new AIMessage({\n                    content: message.content || \"\",\n                    tool_calls: toolCalls,\n                    invalid_tool_calls: invalidToolCalls,\n                    additional_kwargs,\n                    response_metadata,\n                    id: rawResponse.id,\n                });\n            }\n            default:\n                return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n        }\n    }\n    _convertOpenAIDeltaToBaseMessageChunk(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta, rawResponse, defaultRole) {\n        const role = delta.role ?? defaultRole;\n        const content = delta.content ?? \"\";\n        let additional_kwargs;\n        if (delta.function_call) {\n            additional_kwargs = {\n                function_call: delta.function_call,\n            };\n        }\n        else if (delta.tool_calls) {\n            additional_kwargs = {\n                tool_calls: delta.tool_calls,\n            };\n        }\n        else {\n            additional_kwargs = {};\n        }\n        if (this.__includeRawResponse) {\n            additional_kwargs.__raw_response = rawResponse;\n        }\n        if (delta.audio) {\n            additional_kwargs.audio = {\n                ...delta.audio,\n                index: rawResponse.choices[0].index,\n            };\n        }\n        const response_metadata = { usage: { ...rawResponse.usage } };\n        if (role === \"user\") {\n            return new HumanMessageChunk({ content, response_metadata });\n        }\n        else if (role === \"assistant\") {\n            const toolCallChunks = [];\n            if (Array.isArray(delta.tool_calls)) {\n                for (const rawToolCall of delta.tool_calls) {\n                    toolCallChunks.push({\n                        name: rawToolCall.function?.name,\n                        args: rawToolCall.function?.arguments,\n                        id: rawToolCall.id,\n                        index: rawToolCall.index,\n                        type: \"tool_call_chunk\",\n                    });\n                }\n            }\n            return new AIMessageChunk({\n                content,\n                tool_call_chunks: toolCallChunks,\n                additional_kwargs,\n                id: rawResponse.id,\n                response_metadata,\n            });\n        }\n        else if (role === \"system\") {\n            return new SystemMessageChunk({ content, response_metadata });\n        }\n        else if (role === \"developer\") {\n            return new SystemMessageChunk({\n                content,\n                response_metadata,\n                additional_kwargs: {\n                    __openai_role__: \"developer\",\n                },\n            });\n        }\n        else if (role === \"function\") {\n            return new FunctionMessageChunk({\n                content,\n                additional_kwargs,\n                name: delta.name,\n                response_metadata,\n            });\n        }\n        else if (role === \"tool\") {\n            return new ToolMessageChunk({\n                content,\n                additional_kwargs,\n                tool_call_id: delta.tool_call_id,\n                response_metadata,\n            });\n        }\n        else {\n            return new ChatMessageChunk({ content, role, response_metadata });\n        }\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.model,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    async *_streamResponseChunks(messages, options, runManager) {\n        const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);\n        const params = {\n            ...this.invocationParams(options, {\n                streaming: true,\n            }),\n            messages: messagesMapped,\n            stream: true,\n        };\n        let defaultRole;\n        const streamIterable = await this.completionWithRetry(params, options);\n        let usage;\n        for await (const data of streamIterable) {\n            const choice = data?.choices?.[0];\n            if (data.usage) {\n                usage = data.usage;\n            }\n            if (!choice) {\n                continue;\n            }\n            const { delta } = choice;\n            if (!delta) {\n                continue;\n            }\n            const chunk = this._convertOpenAIDeltaToBaseMessageChunk(delta, data, defaultRole);\n            defaultRole = delta.role ?? defaultRole;\n            const newTokenIndices = {\n                prompt: options.promptIndex ?? 0,\n                completion: choice.index ?? 0,\n            };\n            if (typeof chunk.content !== \"string\") {\n                console.log(\"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\");\n                continue;\n            }\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            const generationInfo = { ...newTokenIndices };\n            if (choice.finish_reason != null) {\n                generationInfo.finish_reason = choice.finish_reason;\n                // Only include system fingerprint in the last chunk for now\n                // to avoid concatenation issues\n                generationInfo.system_fingerprint = data.system_fingerprint;\n                generationInfo.model_name = data.model;\n            }\n            if (this.logprobs) {\n                generationInfo.logprobs = choice.logprobs;\n            }\n            const generationChunk = new ChatGenerationChunk({\n                message: chunk,\n                text: chunk.content,\n                generationInfo,\n            });\n            yield generationChunk;\n            await runManager?.handleLLMNewToken(generationChunk.text ?? \"\", newTokenIndices, undefined, undefined, undefined, { chunk: generationChunk });\n        }\n        if (usage) {\n            const inputTokenDetails = {\n                ...(usage.prompt_tokens_details?.audio_tokens !== null && {\n                    audio: usage.prompt_tokens_details?.audio_tokens,\n                }),\n                ...(usage.prompt_tokens_details?.cached_tokens !== null && {\n                    cache_read: usage.prompt_tokens_details?.cached_tokens,\n                }),\n            };\n            const outputTokenDetails = {\n                ...(usage.completion_tokens_details?.audio_tokens !== null && {\n                    audio: usage.completion_tokens_details?.audio_tokens,\n                }),\n                ...(usage.completion_tokens_details?.reasoning_tokens !== null && {\n                    reasoning: usage.completion_tokens_details?.reasoning_tokens,\n                }),\n            };\n            const generationChunk = new ChatGenerationChunk({\n                message: new AIMessageChunk({\n                    content: \"\",\n                    response_metadata: {\n                        usage: { ...usage },\n                    },\n                    usage_metadata: {\n                        input_tokens: usage.prompt_tokens,\n                        output_tokens: usage.completion_tokens,\n                        total_tokens: usage.total_tokens,\n                        ...(Object.keys(inputTokenDetails).length > 0 && {\n                            input_token_details: inputTokenDetails,\n                        }),\n                        ...(Object.keys(outputTokenDetails).length > 0 && {\n                            output_token_details: outputTokenDetails,\n                        }),\n                    },\n                }),\n                text: \"\",\n            });\n            yield generationChunk;\n        }\n        if (options.signal?.aborted) {\n            throw new Error(\"AbortError\");\n        }\n    }\n    /**\n     * Get the identifying parameters for the model\n     *\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /** @ignore */\n    async _generate(messages, options, runManager) {\n        const usageMetadata = {};\n        const params = this.invocationParams(options);\n        const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);\n        if (params.stream) {\n            const stream = this._streamResponseChunks(messages, options, runManager);\n            const finalChunks = {};\n            for await (const chunk of stream) {\n                chunk.message.response_metadata = {\n                    ...chunk.generationInfo,\n                    ...chunk.message.response_metadata,\n                };\n                const index = chunk.generationInfo?.completion ?? 0;\n                if (finalChunks[index] === undefined) {\n                    finalChunks[index] = chunk;\n                }\n                else {\n                    finalChunks[index] = finalChunks[index].concat(chunk);\n                }\n            }\n            const generations = Object.entries(finalChunks)\n                .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n                .map(([_, value]) => value);\n            const { functions, function_call } = this.invocationParams(options);\n            // OpenAI does not support token usage report under stream mode,\n            // fallback to estimation.\n            const promptTokenUsage = await this.getEstimatedTokenCountFromPrompt(messages, functions, function_call);\n            const completionTokenUsage = await this.getNumTokensFromGenerations(generations);\n            usageMetadata.input_tokens = promptTokenUsage;\n            usageMetadata.output_tokens = completionTokenUsage;\n            usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;\n            return {\n                generations,\n                llmOutput: {\n                    estimatedTokenUsage: {\n                        promptTokens: usageMetadata.input_tokens,\n                        completionTokens: usageMetadata.output_tokens,\n                        totalTokens: usageMetadata.total_tokens,\n                    },\n                },\n            };\n        }\n        else {\n            let data;\n            if (options.response_format &&\n                options.response_format.type === \"json_schema\") {\n                data = await this.betaParsedCompletionWithRetry({\n                    ...params,\n                    stream: false,\n                    messages: messagesMapped,\n                }, {\n                    signal: options?.signal,\n                    ...options?.options,\n                });\n            }\n            else {\n                data = await this.completionWithRetry({\n                    ...params,\n                    stream: false,\n                    messages: messagesMapped,\n                }, {\n                    signal: options?.signal,\n                    ...options?.options,\n                });\n            }\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, prompt_tokens_details: promptTokensDetails, completion_tokens_details: completionTokensDetails, } = data?.usage ?? {};\n            if (completionTokens) {\n                usageMetadata.output_tokens =\n                    (usageMetadata.output_tokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                usageMetadata.input_tokens =\n                    (usageMetadata.input_tokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                usageMetadata.total_tokens =\n                    (usageMetadata.total_tokens ?? 0) + totalTokens;\n            }\n            if (promptTokensDetails?.audio_tokens !== null ||\n                promptTokensDetails?.cached_tokens !== null) {\n                usageMetadata.input_token_details = {\n                    ...(promptTokensDetails?.audio_tokens !== null && {\n                        audio: promptTokensDetails?.audio_tokens,\n                    }),\n                    ...(promptTokensDetails?.cached_tokens !== null && {\n                        cache_read: promptTokensDetails?.cached_tokens,\n                    }),\n                };\n            }\n            if (completionTokensDetails?.audio_tokens !== null ||\n                completionTokensDetails?.reasoning_tokens !== null) {\n                usageMetadata.output_token_details = {\n                    ...(completionTokensDetails?.audio_tokens !== null && {\n                        audio: completionTokensDetails?.audio_tokens,\n                    }),\n                    ...(completionTokensDetails?.reasoning_tokens !== null && {\n                        reasoning: completionTokensDetails?.reasoning_tokens,\n                    }),\n                };\n            }\n            const generations = [];\n            for (const part of data?.choices ?? []) {\n                const text = part.message?.content ?? \"\";\n                const generation = {\n                    text,\n                    message: this._convertOpenAIChatCompletionMessageToBaseMessage(part.message ?? { role: \"assistant\" }, data),\n                };\n                generation.generationInfo = {\n                    ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),\n                    ...(part.logprobs ? { logprobs: part.logprobs } : {}),\n                };\n                if (isAIMessage(generation.message)) {\n                    generation.message.usage_metadata = usageMetadata;\n                }\n                // Fields are not serialized unless passed to the constructor\n                // Doing this ensures all fields on the message are serialized\n                generation.message = new AIMessage(Object.fromEntries(Object.entries(generation.message).filter(([key]) => !key.startsWith(\"lc_\"))));\n                generations.push(generation);\n            }\n            return {\n                generations,\n                llmOutput: {\n                    tokenUsage: {\n                        promptTokens: usageMetadata.input_tokens,\n                        completionTokens: usageMetadata.output_tokens,\n                        totalTokens: usageMetadata.total_tokens,\n                    },\n                },\n            };\n        }\n    }\n    /**\n     * Estimate the number of tokens a prompt will use.\n     * Modified from: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts\n     */\n    async getEstimatedTokenCountFromPrompt(messages, functions, function_call) {\n        // It appears that if functions are present, the first system message is padded with a trailing newline. This\n        // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n        let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n        // If there are functions, add the function definitions as they count towards token usage\n        if (functions && function_call !== \"auto\") {\n            const promptDefinitions = formatFunctionDefinitions(functions);\n            tokens += await this.getNumTokens(promptDefinitions);\n            tokens += 9; // Add nine per completion\n        }\n        // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n        // functions typically add a system message, but reuse the first one if it's already there. This offsets\n        // the extra 9 tokens added by the function definitions.\n        if (functions && messages.find((m) => m._getType() === \"system\")) {\n            tokens -= 4;\n        }\n        // If function_call is 'none', add one token.\n        // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n        // If it's undefined or 'auto', don't add anything.\n        if (function_call === \"none\") {\n            tokens += 1;\n        }\n        else if (typeof function_call === \"object\") {\n            tokens += (await this.getNumTokens(function_call.name)) + 4;\n        }\n        return tokens;\n    }\n    /**\n     * Estimate the number of tokens an array of generations have used.\n     */\n    async getNumTokensFromGenerations(generations) {\n        const generationUsages = await Promise.all(generations.map(async (generation) => {\n            if (generation.message.additional_kwargs?.function_call) {\n                return (await this.getNumTokensFromMessages([generation.message]))\n                    .countPerMessage[0];\n            }\n            else {\n                return await this.getNumTokens(generation.message.content);\n            }\n        }));\n        return generationUsages.reduce((a, b) => a + b, 0);\n    }\n    async getNumTokensFromMessages(messages) {\n        let totalCount = 0;\n        let tokensPerMessage = 0;\n        let tokensPerName = 0;\n        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        if (this.model === \"gpt-3.5-turbo-0301\") {\n            tokensPerMessage = 4;\n            tokensPerName = -1;\n        }\n        else {\n            tokensPerMessage = 3;\n            tokensPerName = 1;\n        }\n        const countPerMessage = await Promise.all(messages.map(async (message) => {\n            const textCount = await this.getNumTokens(message.content);\n            const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n            const nameCount = message.name !== undefined\n                ? tokensPerName + (await this.getNumTokens(message.name))\n                : 0;\n            let count = textCount + tokensPerMessage + roleCount + nameCount;\n            // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n            const openAIMessage = message;\n            if (openAIMessage._getType() === \"function\") {\n                count -= 2;\n            }\n            if (openAIMessage.additional_kwargs?.function_call) {\n                count += 3;\n            }\n            if (openAIMessage?.additional_kwargs.function_call?.name) {\n                count += await this.getNumTokens(openAIMessage.additional_kwargs.function_call?.name);\n            }\n            if (openAIMessage.additional_kwargs.function_call?.arguments) {\n                try {\n                    count += await this.getNumTokens(\n                    // Remove newlines and spaces\n                    JSON.stringify(JSON.parse(openAIMessage.additional_kwargs.function_call?.arguments)));\n                }\n                catch (error) {\n                    console.error(\"Error parsing function arguments\", error, JSON.stringify(openAIMessage.additional_kwargs.function_call));\n                    count += await this.getNumTokens(openAIMessage.additional_kwargs.function_call?.arguments);\n                }\n            }\n            totalCount += count;\n            return count;\n        }));\n        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n        return { totalCount, countPerMessage };\n    }\n    async completionWithRetry(request, options) {\n        const requestOptions = this._getClientOptions(options);\n        return this.caller.call(async () => {\n            try {\n                const res = await this.client.chat.completions.create(request, requestOptions);\n                return res;\n            }\n            catch (e) {\n                const error = wrapOpenAIClientError(e);\n                throw error;\n            }\n        });\n    }\n    /**\n     * Call the beta chat completions parse endpoint. This should only be called if\n     * response_format is set to \"json_object\".\n     * @param {OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming} request\n     * @param {OpenAICoreRequestOptions | undefined} options\n     */\n    async betaParsedCompletionWithRetry(request, options\n    // Avoid relying importing a beta type with no official entrypoint\n    ) {\n        const requestOptions = this._getClientOptions(options);\n        return this.caller.call(async () => {\n            try {\n                const res = await this.client.beta.chat.completions.parse(request, requestOptions);\n                return res;\n            }\n            catch (e) {\n                const error = wrapOpenAIClientError(e);\n                throw error;\n            }\n        });\n    }\n    _getClientOptions(options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                baseURL: this.clientConfig.baseURL,\n            };\n            const endpoint = getEndpoint(openAIEndpointConfig);\n            const params = {\n                ...this.clientConfig,\n                baseURL: endpoint,\n                timeout: this.timeout,\n                maxRetries: 0,\n            };\n            if (!params.baseURL) {\n                delete params.baseURL;\n            }\n            this.client = new OpenAIClient(params);\n        }\n        const requestOptions = {\n            ...this.clientConfig,\n            ...options,\n        };\n        return requestOptions;\n    }\n    _llmType() {\n        return \"openai\";\n    }\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs) {\n        return llmOutputs.reduce((acc, llmOutput) => {\n            if (llmOutput && llmOutput.tokenUsage) {\n                acc.tokenUsage.completionTokens +=\n                    llmOutput.tokenUsage.completionTokens ?? 0;\n                acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n                acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n            }\n            return acc;\n        }, {\n            tokenUsage: {\n                completionTokens: 0,\n                promptTokens: 0,\n                totalTokens: 0,\n            },\n        });\n    }\n    withStructuredOutput(outputSchema, config) {\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let schema;\n        let name;\n        let method;\n        let includeRaw;\n        if (isStructuredOutputMethodParams(outputSchema)) {\n            schema = outputSchema.schema;\n            name = outputSchema.name;\n            method = outputSchema.method;\n            includeRaw = outputSchema.includeRaw;\n        }\n        else {\n            schema = outputSchema;\n            name = config?.name;\n            method = config?.method;\n            includeRaw = config?.includeRaw;\n        }\n        let llm;\n        let outputParser;\n        if (config?.strict !== undefined && method === \"jsonMode\") {\n            throw new Error(\"Argument `strict` is only supported for `method` = 'function_calling'\");\n        }\n        if (!this.model.startsWith(\"gpt-3\") &&\n            !this.model.startsWith(\"gpt-4-\") &&\n            this.model !== \"gpt-4\") {\n            if (method === undefined) {\n                method = \"jsonSchema\";\n            }\n        }\n        else if (method === \"jsonSchema\") {\n            console.warn(`[WARNING]: JSON Schema is not supported for model \"${this.model}\". Falling back to tool calling.`);\n        }\n        if (method === \"jsonMode\") {\n            llm = this.bind({\n                response_format: { type: \"json_object\" },\n            });\n            if (isZodSchema(schema)) {\n                outputParser = StructuredOutputParser.fromZodSchema(schema);\n            }\n            else {\n                outputParser = new JsonOutputParser();\n            }\n        }\n        else if (method === \"jsonSchema\") {\n            llm = this.bind({\n                response_format: {\n                    type: \"json_schema\",\n                    json_schema: {\n                        name: name ?? \"extract\",\n                        description: schema.description,\n                        schema,\n                        strict: config?.strict,\n                    },\n                },\n            });\n            if (isZodSchema(schema)) {\n                outputParser = StructuredOutputParser.fromZodSchema(schema);\n            }\n            else {\n                outputParser = new JsonOutputParser();\n            }\n        }\n        else {\n            let functionName = name ?? \"extract\";\n            // Is function calling\n            if (isZodSchema(schema)) {\n                const asJsonSchema = zodToJsonSchema(schema);\n                llm = this.bind({\n                    tools: [\n                        {\n                            type: \"function\",\n                            function: {\n                                name: functionName,\n                                description: asJsonSchema.description,\n                                parameters: asJsonSchema,\n                            },\n                        },\n                    ],\n                    tool_choice: {\n                        type: \"function\",\n                        function: {\n                            name: functionName,\n                        },\n                    },\n                    // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n                    ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n                });\n                outputParser = new JsonOutputKeyToolsParser({\n                    returnSingle: true,\n                    keyName: functionName,\n                    zodSchema: schema,\n                });\n            }\n            else {\n                let openAIFunctionDefinition;\n                if (typeof schema.name === \"string\" &&\n                    typeof schema.parameters === \"object\" &&\n                    schema.parameters != null) {\n                    openAIFunctionDefinition = schema;\n                    functionName = schema.name;\n                }\n                else {\n                    functionName = schema.title ?? functionName;\n                    openAIFunctionDefinition = {\n                        name: functionName,\n                        description: schema.description ?? \"\",\n                        parameters: schema,\n                    };\n                }\n                llm = this.bind({\n                    tools: [\n                        {\n                            type: \"function\",\n                            function: openAIFunctionDefinition,\n                        },\n                    ],\n                    tool_choice: {\n                        type: \"function\",\n                        function: {\n                            name: functionName,\n                        },\n                    },\n                    // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n                    ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n                });\n                outputParser = new JsonOutputKeyToolsParser({\n                    returnSingle: true,\n                    keyName: functionName,\n                });\n            }\n        }\n        if (!includeRaw) {\n            return llm.pipe(outputParser);\n        }\n        const parserAssign = RunnablePassthrough.assign({\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            parsed: (input, config) => outputParser.invoke(input.raw, config),\n        });\n        const parserNone = RunnablePassthrough.assign({\n            parsed: () => null,\n        });\n        const parsedWithFallback = parserAssign.withFallbacks({\n            fallbacks: [parserNone],\n        });\n        return RunnableSequence.from([\n            {\n                raw: llm,\n            },\n            parsedWithFallback,\n        ]);\n    }\n}\nfunction isZodSchema(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ninput) {\n    // Check for a characteristic method of Zod schemas\n    return typeof input?.parse === \"function\";\n}\nfunction isStructuredOutputMethodParams(x\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n) {\n    return (x !== undefined &&\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        typeof x.schema ===\n            \"object\");\n}\n"],"mappings":"AAAA,SAASA,MAAM,IAAIC,YAAY,QAAQ,QAAQ;AAC/C,SAASC,SAAS,EAAEC,cAAc,EAAEC,WAAW,EAAEC,gBAAgB,EAAEC,oBAAoB,EAAEC,iBAAiB,EAAEC,kBAAkB,EAAEC,gBAAgB,EAAEC,WAAW,QAAS,0BAA0B;AAChM,SAASC,mBAAmB,QAAS,yBAAyB;AAC9D,SAASC,sBAAsB,QAAQ,2BAA2B;AAClE,SAASC,aAAa,QAAS,6CAA6C;AAC5E,SAASC,YAAY,QAAS,sCAAsC;AACpE,SAASC,mBAAmB,EAAEC,gBAAgB,QAAS,2BAA2B;AAClF,SAASC,gBAAgB,EAAEC,sBAAsB,QAAS,gCAAgC;AAC1F,SAASC,wBAAwB,EAAEC,gCAAgC,EAAEC,mBAAmB,EAAEC,aAAa,QAAS,6CAA6C;AAC7J,SAASC,eAAe,QAAQ,oBAAoB;AACpD,SAASC,iBAAiB,QAAQ,oBAAoB;AACtD,SAASC,WAAW,QAAQ,kBAAkB;AAC9C,SAASC,wBAAwB,EAAEC,qBAAqB,QAAS,mBAAmB;AACpF,SAASC,yBAAyB,QAAS,gCAAgC;AAC3E,SAASC,oBAAoB,QAAQ,kBAAkB;AACvD,SAASC,+BAA+BA,CAACC,OAAO,EAAE;EAC9C,IAAIA,OAAO,CAACC,IAAI,KAAK,QAAQ,IACzBD,OAAO,CAACC,IAAI,KAAK,WAAW,IAC5BD,OAAO,CAACC,IAAI,KAAK,WAAW,IAC5BD,OAAO,CAACC,IAAI,KAAK,MAAM,IACvBD,OAAO,CAACC,IAAI,KAAK,UAAU,IAC3BD,OAAO,CAACC,IAAI,KAAK,MAAM,EAAE;IACzBC,OAAO,CAACC,IAAI,CAAC,yBAAyBH,OAAO,CAACC,IAAI,EAAE,CAAC;EACzD;EACA,OAAOD,OAAO,CAACC,IAAI;AACvB;AACA,OAAO,SAASG,mBAAmBA,CAACJ,OAAO,EAAE;EACzC,MAAMK,IAAI,GAAGL,OAAO,CAACM,QAAQ,CAAC,CAAC;EAC/B,QAAQD,IAAI;IACR,KAAK,QAAQ;MACT,OAAO,QAAQ;IACnB,KAAK,IAAI;MACL,OAAO,WAAW;IACtB,KAAK,OAAO;MACR,OAAO,MAAM;IACjB,KAAK,UAAU;MACX,OAAO,UAAU;IACrB,KAAK,MAAM;MACP,OAAO,MAAM;IACjB,KAAK,SAAS;MAAE;QACZ,IAAI,CAAChC,WAAW,CAACkC,UAAU,CAACP,OAAO,CAAC,EAChC,MAAM,IAAIQ,KAAK,CAAC,8BAA8B,CAAC;QACnD,OAAOT,+BAA+B,CAACC,OAAO,CAAC;MACnD;IACA;MACI,MAAM,IAAIQ,KAAK,CAAC,yBAAyBH,IAAI,EAAE,CAAC;EACxD;AACJ;AACA;AACA,OAAO,SAASI,8BAA8BA,CAACC,QAAQ,EAAEC,KAAK,EAAE;EAC5D;EACA,OAAOD,QAAQ,CAACE,OAAO,CAAEZ,OAAO,IAAK;IACjC,IAAIC,IAAI,GAAGG,mBAAmB,CAACJ,OAAO,CAAC;IACvC,IAAIC,IAAI,KAAK,QAAQ,IAAIU,KAAK,EAAEE,UAAU,CAAC,IAAI,CAAC,EAAE;MAC9CZ,IAAI,GAAG,WAAW;IACtB;IACA;IACA,MAAMa,eAAe,GAAG;MACpBb,IAAI;MACJc,OAAO,EAAEf,OAAO,CAACe;IACrB,CAAC;IACD,IAAIf,OAAO,CAACgB,IAAI,IAAI,IAAI,EAAE;MACtBF,eAAe,CAACE,IAAI,GAAGhB,OAAO,CAACgB,IAAI;IACvC;IACA,IAAIhB,OAAO,CAACiB,iBAAiB,CAACC,aAAa,IAAI,IAAI,EAAE;MACjDJ,eAAe,CAACI,aAAa,GAAGlB,OAAO,CAACiB,iBAAiB,CAACC,aAAa;MACvEJ,eAAe,CAACC,OAAO,GAAG,IAAI;IAClC;IACA,IAAIpC,WAAW,CAACqB,OAAO,CAAC,IAAI,CAAC,CAACA,OAAO,CAACmB,UAAU,EAAEC,MAAM,EAAE;MACtDN,eAAe,CAACK,UAAU,GAAGnB,OAAO,CAACmB,UAAU,CAACE,GAAG,CAAChC,gCAAgC,CAAC;MACrFyB,eAAe,CAACC,OAAO,GAAG,IAAI;IAClC,CAAC,MACI;MACD,IAAIf,OAAO,CAACiB,iBAAiB,CAACE,UAAU,IAAI,IAAI,EAAE;QAC9CL,eAAe,CAACK,UAAU,GAAGnB,OAAO,CAACiB,iBAAiB,CAACE,UAAU;MACrE;MACA,IAAInB,OAAO,CAACsB,YAAY,IAAI,IAAI,EAAE;QAC9BR,eAAe,CAACQ,YAAY,GAAGtB,OAAO,CAACsB,YAAY;MACvD;IACJ;IACA,IAAItB,OAAO,CAACiB,iBAAiB,CAACM,KAAK,IAC/B,OAAOvB,OAAO,CAACiB,iBAAiB,CAACM,KAAK,KAAK,QAAQ,IACnD,IAAI,IAAIvB,OAAO,CAACiB,iBAAiB,CAACM,KAAK,EAAE;MACzC,MAAMC,YAAY,GAAG;QACjBvB,IAAI,EAAE,WAAW;QACjBsB,KAAK,EAAE;UACHE,EAAE,EAAEzB,OAAO,CAACiB,iBAAiB,CAACM,KAAK,CAACE;QACxC;MACJ,CAAC;MACD,OAAO,CAACX,eAAe,EAAEU,YAAY,CAAC;IAC1C;IACA,OAAOV,eAAe;EAC1B,CAAC,CAAC;AACN;AACA,SAASY,sCAAsCA,CAACC,IAAI,EAAEC,MAAM,EAAE;EAC1D,IAAI7C,YAAY,CAAC4C,IAAI,CAAC,EAAE;IACpB,IAAIC,MAAM,EAAEC,MAAM,KAAKC,SAAS,EAAE;MAC9B,OAAO;QACH,GAAGH,IAAI;QACPI,QAAQ,EAAE;UACN,GAAGJ,IAAI,CAACI,QAAQ;UAChBF,MAAM,EAAED,MAAM,CAACC;QACnB;MACJ,CAAC;IACL;IACA,OAAOF,IAAI;EACf;EACA,OAAO7B,oBAAo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lD,aAAa,CAAC;EAC1C,OAAOmD,OAAOA,CAAA,EAAG;IACb,OAAO,YAAY;EACvB;EACA,IAAIC,QAAQA,CAAA,EAAG;IACX,OAAO,CACH,GAAG,KAAK,CAACA,QAAQ,EACjB,SAAS,EACT,eAAe,EACf,WAAW,EACX,OAAO,EACP,aAAa,EACb,aAAa,EACb,iBAAiB,EACjB,MAAM,EACN,kBAAkB,CACrB;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,YAAY,EAAE,gBAAgB;MAC9BC,MAAM,EAAE,gBAAgB;MACxBC,YAAY,EAAE;IAClB,CAAC;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,SAAS,EAAE,OAAO;MAClBJ,YAAY,EAAE,gBAAgB;MAC9BC,MAAM,EAAE;IACZ,CAAC;EACL;EACAI,WAAWA,CAACb,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBc,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF;IACAL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,eAAe,EAAE;MACzCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,UAAU,EAAE;MACpCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,sBAAsB,EAAE;MAChDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF;AACR;AACA;AACA;IACQL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,2BAA2B,EAAE;MACrDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,YAAY,EAAE;MACtCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACX,YAAY,GACbR,MAAM,EAAES,MAAM,IACVT,MAAM,EAAEQ,YAAY,IACpBR,MAAM,EAAEoB,aAAa,EAAEX,MAAM,IAC7BxD,sBAAsB,CAAC,gBAAgB,CAAC;IAChD,IAAI,CAACwD,MAAM,GAAG,IAAI,CAACD,YAAY;IAC/B,IAAI,CAACE,YAAY,GACbV,MAAM,EAAEoB,aAAa,EAAEV,YAAY,IAC/BzD,sBAAsB,CAAC,qBAAqB,CAAC;IACrD,IAAI,CAAC8B,KAAK,GAAGiB,MAAM,EAAEjB,KAAK,IAAIiB,MAAM,EAAEY,SAAS,IAAI,IAAI,CAAC7B,KAAK;IAC7D,IAAI,CAAC6B,SAAS,GAAG,IAAI,CAAC7B,KAAK;IAC3B,IAAI,CAACsC,WAAW,GAAGrB,MAAM,EAAEqB,WAAW,IAAI,CAAC,CAAC;IAC5C,IAAI,CAACC,OAAO,GAAGtB,MAAM,EAAEsB,OAAO;IAC9B,IAAI,CAACC,WAAW,GAAGvB,MAAM,EAAEuB,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,IAAI,GAAGxB,MAAM,EAAEwB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,gBAAgB,GAAGzB,MAAM,EAAEyB,gBAAgB,IAAI,IAAI,CAACA,gBAAgB;IACzE,IAAI,CAACC,eAAe,GAAG1B,MAAM,EAAE0B,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,CAACC,SAAS,GAAG3B,MAAM,EAAE2B,SAAS;IAClC,IAAI,CAACC,QAAQ,GAAG5B,MAAM,EAAE4B,QAAQ;IAChC,IAAI,CAACC,WAAW,GAAG7B,MAAM,EAAE6B,WAAW;IACtC,IAAI,CAACC,CAAC,GAAG9B,MAAM,EAAE8B,CAAC,IAAI,IAAI,CAACA,CAAC;IAC5B,IAAI,CAACC,SAAS,GAAG/B,MAAM,EAAE+B,SAAS;IAClC,IAAI,CAACC,IAAI,GAAGhC,MAAM,EAAEiC,aAAa,IAAIjC,MAAM,EAAEgC,IAAI;IACjD,IAAI,CAACC,aAAa,GAAG,IAAI,EAAED,IAAI;IAC/B,IAAI,CAACE,IAAI,GAAGlC,MAAM,EAAEkC,IAAI;IACxB,IAAI,CAACC,oBAAoB,GAAGnC,MAAM,EAAEmC,oBAAoB;IACxD,IAAI,CAACxC,KAAK,GAAGK,MAAM,EAAEL,KAAK;IAC1B,IAAI,CAACyC,UAAU,GAAGpC,MAAM,EAAEoC,UAAU;IACpC,IAAI,CAACC,eAAe,GAAGrC,MAAM,EAAEqC,eAAe;IAC9C,IAAI,IAAI,CAACtD,KAAK,KAAK,IAAI,EAAE;MACrB,IAAI,CAACuD,gBAAgB,GAAG,IAAI;IAChC;IACA,IAAI,CAACC,SAAS,GAAGvC,MAAM,EAAEuC,SAAS,IAAI,KAAK;IAC3C,IAAI,CAACC,WAAW,GAAGxC,MAAM,EAAEwC,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,YAAY,GAAG;MAChBhC,MAAM,EAAE,IAAI,CAACA,MAAM;MACnBC,YAAY,EAAE,IAAI,CAACA,YAAY;MAC/BgC,uBAAuB,EAAE,IAAI;MAC7B,GAAG1C,MAAM,EAAEoB;IACf,CAAC;IACD;IACA;IACA,IAAIpB,MAAM,EAAE2C,yBAAyB,KAAKzC,SAAS,EAAE;MACjD,IAAI,CAACyC,yBAAyB,GAAG3C,MAAM,CAAC2C,yBAAyB;IACrE;EACJ;EACAC,WAAWA,CAACC,OAAO,EAAE;IACjB,MAAMC,MAAM,GAAG,IAAI,CAACC,gBAAgB,CAACF,OAAO,CAAC;IAC7C,OAAO;MACHG,WAAW,EAAE,QAAQ;MACrBC,aAAa,EAAE,IAAI,CAAClE,KAAK;MACzBmE,aAAa,EAAE,MAAM;MACrBC,cAAc,EAAEL,MAAM,CAACvB,WAAW,IAAIrB,SAAS;MAC/CkD,aAAa,EAAEN,MAAM,CAACO,UAAU,IAAInD,SAAS;MAC7CoD,OAAO,EAAET,OAAO,CAACb;IACrB,CAAC;EACL;EACAuB,SAASA,CAACC,KAAK,EAAEC,MAAM,EAAE;IACrB,IAAIxD,MAAM;IACV,IAAIwD,MAAM,EAAExD,MAAM,KAAKC,SAAS,EAAE;MAC9BD,MAAM,GAAGwD,MAAM,CAACxD,MAAM;IAC1B,CAAC,MACI,IAAI,IAAI,CAAC0C,yBAAyB,KAAKzC,SAAS,EAAE;MACnDD,MAAM,GAAG,IAAI,CAAC0C,yBAAyB;IAC3C;IACA,OAAO,IAAI,CAACe,IAAI,CAAC;MACbF,KAAK,EAAEA,KAAK,CAAC/D,GAAG,CAAEM,IAAI,IAAKD,sCAAsC,CAACC,IAAI,EAAE;QAAEE;MAAO,CAAC,CAAC,CAAC;MACpF,GAAGwD;IACP,CAAC,CAAC;EACN;EACAE,oBAAoBA,CAACC,SAAS,EAAE;IAC5B,IAAIA,SAAS,IACTA,SAAS,CAACnF,IAAI,KAAK,aAAa,IAChCmF,SAAS,CAACC,WAAW,CAACC,MAAM,IAC5BC,WAAW,CAACH,SAAS,CAACC,WAAW,CAACC,MAAM,CAAC,EAAE;MAC3C,OAAOjG,iBAAiB,CAAC+F,SAAS,CAACC,WAAW,CAACC,MAAM,EAAEF,SAAS,CAACC,WAAW,CAACzE,IAAI,EAAE;QAC/E4E,WAAW,EAAEJ,SAAS,CAACC,WAAW,CAACG;MACvC,CAAC,CAAC;IACN;IACA,OAAOJ,SAAS;EACpB;EACA;AACJ;AACA;EACIb,gBAAgBA,CAACF,OAAO,EAAEoB,KAAK,EAAE;IAC7B,IAAIhE,MAAM;IACV,IAAI4C,OAAO,EAAE5C,MAAM,KAAKC,SAAS,EAAE;MAC/BD,MAAM,GAAG4C,OAAO,CAAC5C,MAAM;IAC3B,CAAC,MACI,IAAI,IAAI,CAAC0C,yBAAyB,KAAKzC,SAAS,EAAE;MACnDD,MAAM,GAAG,IAAI,CAAC0C,yBAAyB;IAC3C;IACA,IAAIuB,mBAAmB,GAAG,CAAC,CAAC;IAC5B,IAAIrB,OAAO,EAAEsB,cAAc,KAAKjE,SAAS,EAAE;MACvCgE,mBAAmB,GAAG;QAAEC,cAAc,EAAEtB,OAAO,CAACsB;MAAe,CAAC;IACpE,CAAC,MACI,IAAI,IAAI,CAAC3B,WAAW,KAAK,IAAI,CAACD,SAAS,IAAI0B,KAAK,EAAE1B,SAAS,CAAC,EAAE;MAC/D2B,mBAAmB,GAAG;QAAEC,cAAc,EAAE;UAAEC,aAAa,EAAE;QAAK;MAAE,CAAC;IACrE;IACA,MAAMtB,MAAM,GAAG;MACX/D,KAAK,EAAE,IAAI,CAACA,KAAK;MACjBwC,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7B8C,KAAK,EAAE,IAAI,CAAC7C,IAAI;MAChB8C,iBAAiB,EAAE,IAAI,CAAC7C,gBAAgB;MACxC8C,gBAAgB,EAAE,IAAI,CAAC7C,eAAe;MACtC2B,UAAU,EAAE,IAAI,CAAC1B,SAAS,KAAK,CAAC,CAAC,GAAGzB,SAAS,GAAG,IAAI,CAACyB,SAAS;MAC9DC,QAAQ,EAAE,IAAI,CAACA,QAAQ;MACvB4C,YAAY,EAAE,IAAI,CAAC3C,WAAW;MAC9BC,CAAC,EAAE,IAAI,CAACA,CAAC;MACT2C,UAAU,EAAE,IAAI,CAAC1C,SAAS;MAC1BC,IAAI,EAAEa,OAAO,EAAEb,IAAI,IAAI,IAAI,CAACC,aAAa;MACzCC,IAAI,EAAE,IAAI,CAACA,IAAI;MACf;MACAwC,MAAM,EAAE,IAAI,CAACnC,SAAS;MACtBoC,SAAS,EAAE9B,OAAO,EAAE8B,SAAS;MAC7BrF,aAAa,EAAEuD,OAAO,EAAEvD,aAAa;MACrCkE,KAAK,EAAEX,OAAO,EAAEW,KAAK,EAAEhE,MAAM,GACvBqD,OAAO,CAACW,KAAK,CAAC/D,GAAG,CAAEM,IAAI,IAAKD,sCAAsC,CAACC,IAAI,EAAE;QAAEE;MAAO,CAAC,CAAC,CAAC,GACrFC,SAAS;MACf0E,WAAW,EAAE7G,wBAAwB,CAAC8E,OAAO,EAAE+B,WAAW,CAAC;MAC3DC,eAAe,EAAE,IAAI,CAAClB,oBAAoB,CAACd,OAAO,EAAEgC,eAAe,CAAC;MACpEC,IAAI,EAAEjC,OAAO,EAAEiC,IAAI;MACnB,GAAGZ,mBAAmB;MACtBa,mBAAmB,EAAElC,OAAO,EAAEkC,mBAAmB;MACjD,IAAI,IAAI,CAACpF,KAAK,IAAIkD,OAAO,EAAElD,KAAK,GAC1B;QAAEA,KAAK,EAAE,IAAI,CAACA,KAAK,IAAIkD,OAAO,EAAElD;MAAM,CAAC,GACvC,CAAC,CAAC,CAAC;MACT,IAAI,IAAI,CAACyC,UAAU,IAAIS,OAAO,EAAET,UAAU,GACpC;QAAEA,UAAU,EAAE,IAAI,CAACA,UAAU,IAAIS,OAAO,EAAET;MAAW,CAAC,GACtD,CAAC,CAAC,CAAC;MACT,GAAG,IAAI,CAACf;IACZ,CAAC;IACD,IAAIwB,OAAO,EAAEmC,UAAU,KAAK9E,SAAS,EAAE;MACnC4C,MAAM,CAACkC,UAAU,GAAGnC,OAAO,CAACmC,UAAU;IAC1C;IACA,MAAM3C,eAAe,GAAGQ,OAAO,EAAEoC,gBAAgB,IAAI,IAAI,CAAC5C,eAAe;IACzE,IAAIA,eAAe,KAAKnC,SAAS,EAAE;MAC/B4C,MAAM,CAACmC,gBAAgB,GAAG5C,eAAe;IAC7C;IACA,OAAOS,MAAM;EACjB;EACAoC,gDAAgDA,CAAC9G,OAAO,EAAE+G,WAAW,EAAE;IACnE,MAAMC,YAAY,GAAGhH,OAAO,CAACmB,UAAU;IACvC,QAAQnB,OAAO,CAACC,IAAI;MAChB,KAAK,WAAW;QAAE;UACd,MAAMgH,SAAS,GAAG,EAAE;UACpB,MAAMC,gBAAgB,GAAG,EAAE;UAC3B,KAAK,MAAMC,WAAW,IAAIH,YAAY,IAAI,EAAE,EAAE;YAC1C,IAAI;cACAC,SAAS,CAACG,IAAI,CAAC7H,aAAa,CAAC4H,WAAW,EAAE;gBAAEE,QAAQ,EAAE;cAAK,CAAC,CAAC,CAAC;cAC9D;YACJ,CAAC,CACD,OAAOC,CAAC,EAAE;cACNJ,gBAAgB,CAACE,IAAI,CAAC9H,mBAAmB,CAAC6H,WAAW,EAAEG,CAAC,CAACtH,OAAO,CAAC,CAAC;YACtE;UACJ;UACA,MAAMiB,iBAAiB,GAAG;YACtBC,aAAa,EAAElB,OAAO,CAACkB,aAAa;YACpCC,UAAU,EAAE6F;UAChB,CAAC;UACD,IAAI,IAAI,CAACjD,oBAAoB,KAAKjC,SAAS,EAAE;YACzCb,iBAAiB,CAACsG,cAAc,GAAGR,WAAW;UAClD;UACA,MAAMS,iBAAiB,GAAG;YACtBC,UAAU,EAAEV,WAAW,CAACpG,KAAK;YAC7B,IAAIoG,WAAW,CAACW,kBAAkB,GAC5B;cACEC,KAAK,EAAE;gBAAE,GAAGZ,WAAW,CAACY;cAAM,CAAC;cAC/BD,kBAAkB,EAAEX,WAAW,CAACW;YACpC,CAAC,GACC,CAAC,CAAC;UACZ,CAAC;UACD,IAAI1H,OAAO,CAACuB,KAAK,EAAE;YACfN,iBAAiB,CAACM,KAAK,GAAGvB,OAAO,CAACuB,KAAK;UAC3C;UACA,OAAO,IAAIpD,SAAS,CAAC;YACjB4C,OAAO,EAAEf,OAAO,CAACe,OAAO,IAAI,EAAE;YAC9BI,UAAU,EAAE8F,SAAS;YACrBW,kBAAkB,EAAEV,gBAAgB;YACpCjG,iBAAiB;YACjBuG,iBAAiB;YACjB/F,EAAE,EAAEsF,WAAW,CAACtF;UACpB,CAAC,CAAC;QACN;MACA;QACI,OAAO,IAAIpD,WAAW,CAAC2B,OAAO,CAACe,OAAO,IAAI,EAAE,EAAEf,OAAO,CAACC,IAAI,IAAI,SAAS,CAAC;IAChF;EACJ;EACA4H,qCAAqCA;EACrC;EACAC,KAAK,EAAEf,WAAW,EAAEgB,WAAW,EAAE;IAC7B,MAAM9H,IAAI,GAAG6H,KAAK,CAAC7H,IAAI,IAAI8H,WAAW;IACtC,MAAMhH,OAAO,GAAG+G,KAAK,CAAC/G,OAAO,IAAI,EAAE;IACnC,IAAIE,iBAAiB;IACrB,IAAI6G,KAAK,CAAC5G,aAAa,EAAE;MACrBD,iBAAiB,GAAG;QAChBC,aAAa,EAAE4G,KAAK,CAAC5G;MACzB,CAAC;IACL,CAAC,MACI,IAAI4G,KAAK,CAAC3G,UAAU,EAAE;MACvBF,iBAAiB,GAAG;QAChBE,UAAU,EAAE2G,KAAK,CAAC3G;MACtB,CAAC;IACL,CAAC,MACI;MACDF,iBAAiB,GAAG,CAAC,CAAC;IAC1B;IACA,IAAI,IAAI,CAAC8C,oBAAoB,EAAE;MAC3B9C,iBAAiB,CAACsG,cAAc,GAAGR,WAAW;IAClD;IACA,IAAIe,KAAK,CAACvG,KAAK,EAAE;MACbN,iBAAiB,CAACM,KAAK,GAAG;QACtB,GAAGuG,KAAK,CAACvG,KAAK;QACdyG,KAAK,EAAEjB,WAAW,CAACkB,OAAO,CAAC,CAAC,CAAC,CAACD;MAClC,CAAC;IACL;IACA,MAAMR,iBAAiB,GAAG;MAAEG,KAAK,EAAE;QAAE,GAAGZ,WAAW,CAACY;MAAM;IAAE,CAAC;IAC7D,IAAI1H,IAAI,KAAK,MAAM,EAAE;MACjB,OAAO,IAAIzB,iBAAiB,CAAC;QAAEuC,OAAO;QAAEyG;MAAkB,CAAC,CAAC;IAChE,CAAC,MACI,IAAIvH,IAAI,KAAK,WAAW,EAAE;MAC3B,MAAMiI,cAAc,GAAG,EAAE;MACzB,IAAIC,KAAK,CAACC,OAAO,CAACN,KAAK,CAAC3G,UAAU,CAAC,EAAE;QACjC,KAAK,MAAMgG,WAAW,IAAIW,KAAK,CAAC3G,UAAU,EAAE;UACxC+G,cAAc,CAACd,IAAI,CAAC;YAChBpG,IAAI,EAAEmG,WAAW,CAACpF,QAAQ,EAAEf,IAAI;YAChCqH,IAAI,EAAElB,WAAW,CAACpF,QAAQ,EAAEuG,SAAS;YACrC7G,EAAE,EAAE0F,WAAW,CAAC1F,EAAE;YAClBuG,KAAK,EAAEb,WAAW,CAACa,KAAK;YACxB3H,IAAI,EAAE;UACV,CAAC,CAAC;QACN;MACJ;MACA,OAAO,IAAIjC,cAAc,CAAC;QACtB2C,OAAO;QACPwH,gBAAgB,EAAEL,cAAc;QAChCjH,iBAAiB;QACjBQ,EAAE,EAAEsF,WAAW,CAACtF,EAAE;QAClB+F;MACJ,CAAC,CAAC;IACN,CAAC,MACI,IAAIvH,IAAI,KAAK,QAAQ,EAAE;MACxB,OAAO,IAAIxB,kBAAkB,CAAC;QAAEsC,OAAO;QAAEyG;MAAkB,CAAC,CAAC;IACjE,CAAC,MACI,IAAIvH,IAAI,KAAK,WAAW,EAAE;MAC3B,OAAO,IAAIxB,kBAAkB,CAAC;QAC1BsC,OAAO;QACPyG,iBAAiB;QACjBvG,iBAAiB,EAAE;UACfuH,eAAe,EAAE;QACrB;MACJ,CAAC,CAAC;IACN,CAAC,MACI,IAAIvI,IAAI,KAAK,UAAU,EAAE;MAC1B,OAAO,IAAI1B,oBAAoB,CAAC;QAC5BwC,OAAO;QACPE,iBAAiB;QACjBD,IAAI,EAAE8G,KAAK,CAAC9G,IAAI;QAChBwG;MACJ,CAAC,CAAC;IACN,CAAC,MACI,IAAIvH,IAAI,KAAK,MAAM,EAAE;MACtB,OAAO,IAAIvB,gBAAgB,CAAC;QACxBqC,OAAO;QACPE,iBAAiB;QACjBK,YAAY,EAAEwG,KAAK,CAACxG,YAAY;QAChCkG;MACJ,CAAC,CAAC;IACN,CAAC,MACI;MACD,OAAO,IAAIlJ,gBAAgB,CAAC;QAAEyC,OAAO;QAAEd,IAAI;QAAEuH;MAAkB,CAAC,CAAC;IACrE;EACJ;EACA;EACAiB,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHhB,UAAU,EAAE,IAAI,CAAC9G,KAAK;MACtB,GAAG,IAAI,CAACgE,gBAAgB,CAAC,CAAC;MAC1B,GAAG,IAAI,CAACN;IACZ,CAAC;EACL;EACA,OAAOqE,qBAAqBA,CAAChI,QAAQ,EAAE+D,OAAO,EAAEkE,UAAU,EAAE;IACxD,MAAMC,cAAc,GAAGnI,8BAA8B,CAACC,QAAQ,EAAE,IAAI,CAACC,KAAK,CAAC;IAC3E,MAAM+D,MAAM,GAAG;MACX,GAAG,IAAI,CAACC,gBAAgB,CAACF,OAAO,EAAE;QAC9BN,SAAS,EAAE;MACf,CAAC,CAAC;MACFzD,QAAQ,EAAEkI,cAAc;MACxBtC,MAAM,EAAE;IACZ,CAAC;IACD,IAAIyB,WAAW;IACf,MAAMc,cAAc,GAAG,MAAM,IAAI,CAACC,mBAAmB,CAACpE,MAAM,EAAED,OAAO,CAAC;IACtE,IAAIkD,KAAK;IACT,WAAW,MAAMoB,IAAI,IAAIF,cAAc,EAAE;MACrC,MAAMG,MAAM,GAAGD,IAAI,EAAEd,OAAO,GAAG,CAAC,CAAC;MACjC,IAAIc,IAAI,CAACpB,KAAK,EAAE;QACZA,KAAK,GAAGoB,IAAI,CAACpB,KAAK;MACtB;MACA,IAAI,CAACqB,MAAM,EAAE;QACT;MACJ;MACA,MAAM;QAAElB;MAAM,CAAC,GAAGkB,MAAM;MACxB,IAAI,CAAClB,KAAK,EAAE;QACR;MACJ;MACA,MAAMmB,KAAK,GAAG,IAAI,CAACpB,qCAAqC,CAACC,KAAK,EAAEiB,IAAI,EAAEhB,WAAW,CAAC;MAClFA,WAAW,GAAGD,KAAK,CAAC7H,IAAI,IAAI8H,WAAW;MACvC,MAAMmB,eAAe,GAAG;QACpBC,MAAM,EAAE1E,OAAO,CAAC2E,WAAW,IAAI,CAAC;QAChCC,UAAU,EAAEL,MAAM,CAAChB,KAAK,IAAI;MAChC,CAAC;MACD,IAAI,OAAOiB,KAAK,CAAClI,OAAO,KAAK,QAAQ,EAAE;QACnCb,OAAO,CAACoJ,GAAG,CAAC,sFAAsF,CAAC;QACnG;MACJ;MACA;MACA,MAAMC,cAAc,GAAG;QAAE,GAAGL;MAAgB,CAAC;MAC7C,IAAIF,MAAM,CAACQ,aAAa,IAAI,IAAI,EAAE;QAC9BD,cAAc,CAACC,aAAa,GAAGR,MAAM,CAACQ,aAAa;QACnD;QACA;QACAD,cAAc,CAAC7B,kBAAkB,GAAGqB,IAAI,CAACrB,kBAAkB;QAC3D6B,cAAc,CAAC9B,UAAU,GAAGsB,IAAI,CAACpI,KAAK;MAC1C;MACA,IAAI,IAAI,CAAC6C,QAAQ,EAAE;QACf+F,cAAc,CAAC/F,QAAQ,GAAGwF,MAAM,CAACxF,QAAQ;MAC7C;MACA,MAAMiG,eAAe,GAAG,IAAI7K,mBAAmB,CAAC;QAC5CoB,OAAO,EAAEiJ,KAAK;QACdS,IAAI,EAAET,KAAK,CAAClI,OAAO;QACnBwI;MACJ,CAAC,CAAC;MACF,MAAME,eAAe;MACrB,MAAMd,UAAU,EAAEgB,iBAAiB,CAACF,eAAe,CAACC,IAAI,IAAI,EAAE,EAAER,eAAe,EAAEpH,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;QAAEmH,KAAK,EAAEQ;MAAgB,CAAC,CAAC;IACjJ;IACA,IAAI9B,KAAK,EAAE;MACP,MAAMiC,iBAAiB,GAAG;QACtB,IAAIjC,KAAK,CAACkC,qBAAqB,EAAEC,YAAY,KAAK,IAAI,IAAI;UACtDvI,KAAK,EAAEoG,KAAK,CAACkC,qBAAqB,EAAEC;QACxC,CAAC,CAAC;QACF,IAAInC,KAAK,CAACkC,qBAAqB,EAAEE,aAAa,KAAK,IAAI,IAAI;UACvDC,UAAU,EAAErC,KAAK,CAACkC,qBAAqB,EAAEE;QAC7C,CAAC;MACL,CAAC;MACD,MAAME,kBAAkB,GAAG;QACvB,IAAItC,KAAK,CAACuC,yBAAyB,EAAEJ,YAAY,KAAK,IAAI,IAAI;UAC1DvI,KAAK,EAAEoG,KAAK,CAACuC,yBAAyB,EAAEJ;QAC5C,CAAC,CAAC;QACF,IAAInC,KAAK,CAACuC,yBAAyB,EAAEC,gBAAgB,KAAK,IAAI,IAAI;UAC9DC,SAAS,EAAEzC,KAAK,CAACuC,yBAAyB,EAAEC;QAChD,CAAC;MACL,CAAC;MACD,MAAMV,eAAe,GAAG,IAAI7K,mBAAmB,CAAC;QAC5CoB,OAAO,EAAE,IAAI5B,cAAc,CAAC;UACxB2C,OAAO,EAAE,EAAE;UACXyG,iBAAiB,EAAE;YACfG,KAAK,EAAE;cAAE,GAAGA;YAAM;UACtB,CAAC;UACD0C,cAAc,EAAE;YACZC,YAAY,EAAE3C,KAAK,CAAC4C,aAAa;YACjCC,aAAa,EAAE7C,KAAK,CAAC8C,iBAAiB;YACtCC,YAAY,EAAE/C,KAAK,CAAC+C,YAAY;YAChC,IAAIhI,MAAM,CAACiI,IAAI,CAACf,iBAAiB,CAAC,CAACxI,MAAM,GAAG,CAAC,IAAI;cAC7CwJ,mBAAmB,EAAEhB;YACzB,CAAC,CAAC;YACF,IAAIlH,MAAM,CAACiI,IAAI,CAACV,kBAAkB,CAAC,CAAC7I,MAAM,GAAG,CAAC,IAAI;cAC9CyJ,oBAAoB,EAAEZ;YAC1B,CAAC;UACL;QACJ,CAAC,CAAC;QACFP,IAAI,EAAE;MACV,CAAC,CAAC;MACF,MAAMD,eAAe;IACzB;IACA,IAAIhF,OAAO,CAACqG,MAAM,EAAEC,OAAO,EAAE;MACzB,MAAM,IAAIvK,KAAK,CAAC,YAAY,CAAC;IACjC;EACJ;EACA;AACJ;AACA;AACA;EACIwK,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAACvC,kBAAkB,CAAC,CAAC;EACpC;EACA;EACA,MAAMwC,SAASA,CAACvK,QAAQ,EAAE+D,OAAO,EAAEkE,UAAU,EAAE;IAC3C,MAAMuC,aAAa,GAAG,CAAC,CAAC;IACxB,MAAMxG,MAAM,GAAG,IAAI,CAACC,gBAAgB,CAACF,OAAO,CAAC;IAC7C,MAAMmE,cAAc,GAAGnI,8BAA8B,CAACC,QAAQ,EAAE,IAAI,CAACC,KAAK,CAAC;IAC3E,IAAI+D,MAAM,CAAC4B,MAAM,EAAE;MACf,MAAMA,MAAM,GAAG,IAAI,CAACoC,qBAAqB,CAAChI,QAAQ,EAAE+D,OAAO,EAAEkE,UAAU,CAAC;MACxE,MAAMwC,WAAW,GAAG,CAAC,CAAC;MACtB,WAAW,MAAMlC,KAAK,IAAI3C,MAAM,EAAE;QAC9B2C,KAAK,CAACjJ,OAAO,CAACwH,iBAAiB,GAAG;UAC9B,GAAGyB,KAAK,CAACM,cAAc;UACvB,GAAGN,KAAK,CAACjJ,OAAO,CAACwH;QACrB,CAAC;QACD,MAAMQ,KAAK,GAAGiB,KAAK,CAACM,cAAc,EAAEF,UAAU,IAAI,CAAC;QACnD,IAAI8B,WAAW,CAACnD,KAAK,CAAC,KAAKlG,SAAS,EAAE;UAClCqJ,WAAW,CAACnD,KAAK,CAAC,GAAGiB,KAAK;QAC9B,CAAC,MACI;UACDkC,WAAW,CAACnD,KAAK,CAAC,GAAGmD,WAAW,CAACnD,KAAK,CAAC,CAACoD,MAAM,CAACnC,KAAK,CAAC;QACzD;MACJ;MACA,MAAMoC,WAAW,GAAG3I,MAAM,CAAC4I,OAAO,CAACH,WAAW,CAAC,CAC1CI,IAAI,CAAC,CAAC,CAACC,IAAI,CAAC,EAAE,CAACC,IAAI,CAAC,KAAKC,QAAQ,CAACF,IAAI,EAAE,EAAE,CAAC,GAAGE,QAAQ,CAACD,IAAI,EAAE,EAAE,CAAC,CAAC,CACjEpK,GAAG,CAAC,CAAC,CAACsK,CAAC,EAAE5I,KAAK,CAAC,KAAKA,KAAK,CAAC;MAC/B,MAAM;QAAEwD,SAAS;QAAErF;MAAc,CAAC,GAAG,IAAI,CAACyD,gBAAgB,CAACF,OAAO,CAAC;MACnE;MACA;MACA,MAAMmH,gBAAgB,GAAG,MAAM,IAAI,CAACC,gCAAgC,CAACnL,QAAQ,EAAE6F,SAAS,EAAErF,aAAa,CAAC;MACxG,MAAM4K,oBAAoB,GAAG,MAAM,IAAI,CAACC,2BAA2B,CAACV,WAAW,CAAC;MAChFH,aAAa,CAACZ,YAAY,GAAGsB,gBAAgB;MAC7CV,aAAa,CAACV,aAAa,GAAGsB,oBAAoB;MAClDZ,aAAa,CAACR,YAAY,GAAGkB,gBAAgB,GAAGE,oBAAoB;MACpE,OAAO;QACHT,WAAW;QACXW,SAAS,EAAE;UACPC,mBAAmB,EAAE;YACjBC,YAAY,EAAEhB,aAAa,CAACZ,YAAY;YACxC6B,gBAAgB,EAAEjB,aAAa,CAACV,aAAa;YAC7C4B,WAAW,EAAElB,aAAa,CAACR;UAC/B;QACJ;MACJ,CAAC;IACL,CAAC,MACI;MACD,IAAI3B,IAAI;MACR,IAAItE,OAAO,CAACgC,eAAe,IACvBhC,OAAO,CAACgC,eAAe,CAACpG,IAAI,KAAK,aAAa,EAAE;QAChD0I,IAAI,GAAG,MAAM,IAAI,CAACsD,6BAA6B,CAAC;UAC5C,GAAG3H,MAAM;UACT4B,MAAM,EAAE,KAAK;UACb5F,QAAQ,EAAEkI;QACd,CAAC,EAAE;UACCkC,MAAM,EAAErG,OAAO,EAAEqG,MAAM;UACvB,GAAGrG,OAAO,EAAEA;QAChB,CAAC,CAAC;MACN,CAAC,MACI;QACDsE,IAAI,GAAG,MAAM,IAAI,CAACD,mBAAmB,CAAC;UAClC,GAAGpE,MAAM;UACT4B,MAAM,EAAE,KAAK;UACb5F,QAAQ,EAAEkI;QACd,CAAC,EAAE;UACCkC,MAAM,EAAErG,OAAO,EAAEqG,MAAM;UACvB,GAAGrG,OAAO,EAAEA;QAChB,CAAC,CAAC;MACN;MACA,MAAM;QAAEgG,iBAAiB,EAAE0B,gBAAgB;QAAE5B,aAAa,EAAE2B,YAAY;QAAExB,YAAY,EAAE0B,WAAW;QAAEvC,qBAAqB,EAAEyC,mBAAmB;QAAEpC,yBAAyB,EAAEqC;MAAyB,CAAC,GAAGxD,IAAI,EAAEpB,KAAK,IAAI,CAAC,CAAC;MAC1N,IAAIwE,gBAAgB,EAAE;QAClBjB,aAAa,CAACV,aAAa,GACvB,CAACU,aAAa,CAACV,aAAa,IAAI,CAAC,IAAI2B,gBAAgB;MAC7D;MACA,IAAID,YAAY,EAAE;QACdhB,aAAa,CAACZ,YAAY,GACtB,CAACY,aAAa,CAACZ,YAAY,IAAI,CAAC,IAAI4B,YAAY;MACxD;MACA,IAAIE,WAAW,EAAE;QACblB,aAAa,CAACR,YAAY,GACtB,CAACQ,aAAa,CAACR,YAAY,IAAI,CAAC,IAAI0B,WAAW;MACvD;MACA,IAAIE,mBAAmB,EAAExC,YAAY,KAAK,IAAI,IAC1CwC,mBAAmB,EAAEvC,aAAa,KAAK,IAAI,EAAE;QAC7CmB,aAAa,CAACN,mBAAmB,GAAG;UAChC,IAAI0B,mBAAmB,EAAExC,YAAY,KAAK,IAAI,IAAI;YAC9CvI,KAAK,EAAE+K,mBAAmB,EAAExC;UAChC,CAAC,CAAC;UACF,IAAIwC,mBAAmB,EAAEvC,aAAa,KAAK,IAAI,IAAI;YAC/CC,UAAU,EAAEsC,mBAAmB,EAAEvC;UACrC,CAAC;QACL,CAAC;MACL;MACA,IAAIwC,uBAAuB,EAAEzC,YAAY,KAAK,IAAI,IAC9CyC,uBAAuB,EAAEpC,gBAAgB,KAAK,IAAI,EAAE;QACpDe,aAAa,CAACL,oBAAoB,GAAG;UACjC,IAAI0B,uBAAuB,EAAEzC,YAAY,KAAK,IAAI,IAAI;YAClDvI,KAAK,EAAEgL,uBAAuB,EAAEzC;UACpC,CAAC,CAAC;UACF,IAAIyC,uBAAuB,EAAEpC,gBAAgB,KAAK,IAAI,IAAI;YACtDC,SAAS,EAAEmC,uBAAuB,EAAEpC;UACxC,CAAC;QACL,CAAC;MACL;MACA,MAAMkB,WAAW,GAAG,EAAE;MACtB,KAAK,MAAMmB,IAAI,IAAIzD,IAAI,EAAEd,OAAO,IAAI,EAAE,EAAE;QACpC,MAAMyB,IAAI,GAAG8C,IAAI,CAACxM,OAAO,EAAEe,OAAO,IAAI,EAAE;QACxC,MAAM0L,UAAU,GAAG;UACf/C,IAAI;UACJ1J,OAAO,EAAE,IAAI,CAAC8G,gDAAgD,CAAC0F,IAAI,CAACxM,OAAO,IAAI;YAAEC,IAAI,EAAE;UAAY,CAAC,EAAE8I,IAAI;QAC9G,CAAC;QACD0D,UAAU,CAAClD,cAAc,GAAG;UACxB,IAAIiD,IAAI,CAAChD,aAAa,GAAG;YAAEA,aAAa,EAAEgD,IAAI,CAAChD;UAAc,CAAC,GAAG,CAAC,CAAC,CAAC;UACpE,IAAIgD,IAAI,CAAChJ,QAAQ,GAAG;YAAEA,QAAQ,EAAEgJ,IAAI,CAAChJ;UAAS,CAAC,GAAG,CAAC,CAAC;QACxD,CAAC;QACD,IAAI7E,WAAW,CAAC8N,UAAU,CAACzM,OAAO,CAAC,EAAE;UACjCyM,UAAU,CAACzM,OAAO,CAACqK,cAAc,GAAGa,aAAa;QACrD;QACA;QACA;QACAuB,UAAU,CAACzM,OAAO,GAAG,IAAI7B,SAAS,CAACuE,MAAM,CAACgK,WAAW,CAAChK,MAAM,CAAC4I,OAAO,CAACmB,UAAU,CAACzM,OAAO,CAAC,CAAC2M,MAAM,CAAC,CAAC,CAACC,GAAG,CAAC,KAAK,CAACA,GAAG,CAAC/L,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QACpIwK,WAAW,CAACjE,IAAI,CAACqF,UAAU,CAAC;MAChC;MACA,OAAO;QACHpB,WAAW;QACXW,SAAS,EAAE;UACPa,UAAU,EAAE;YACRX,YAAY,EAAEhB,aAAa,CAACZ,YAAY;YACxC6B,gBAAgB,EAAEjB,aAAa,CAACV,aAAa;YAC7C4B,WAAW,EAAElB,aAAa,CAACR;UAC/B;QACJ;MACJ,CAAC;IACL;EACJ;EACA;AACJ;AACA;AACA;EACI,MAAMmB,gCAAgCA,CAACnL,QAAQ,EAAE6F,SAAS,EAAErF,aAAa,EAAE;IACvE;IACA;IACA,IAAI4L,MAAM,GAAG,CAAC,MAAM,IAAI,CAACC,wBAAwB,CAACrM,QAAQ,CAAC,EAAEsM,UAAU;IACvE;IACA,IAAIzG,SAAS,IAAIrF,aAAa,KAAK,MAAM,EAAE;MACvC,MAAM+L,iBAAiB,GAAGpN,yBAAyB,CAAC0G,SAAS,CAAC;MAC9DuG,MAAM,IAAI,MAAM,IAAI,CAACI,YAAY,CAACD,iBAAiB,CAAC;MACpDH,MAAM,IAAI,CAAC,CAAC,CAAC;IACjB;IACA;IACA;IACA;IACA,IAAIvG,SAAS,IAAI7F,QAAQ,CAACyM,IAAI,CAAEC,CAAC,IAAKA,CAAC,CAAC9M,QAAQ,CAAC,CAAC,KAAK,QAAQ,CAAC,EAAE;MAC9DwM,MAAM,IAAI,CAAC;IACf;IACA;IACA;IACA;IACA,IAAI5L,aAAa,KAAK,MAAM,EAAE;MAC1B4L,MAAM,IAAI,CAAC;IACf,CAAC,MACI,IAAI,OAAO5L,aAAa,KAAK,QAAQ,EAAE;MACxC4L,MAAM,IAAI,CAAC,MAAM,IAAI,CAACI,YAAY,CAAChM,aAAa,CAACF,IAAI,CAAC,IAAI,CAAC;IAC/D;IACA,OAAO8L,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMf,2BAA2BA,CAACV,WAAW,EAAE;IAC3C,MAAMgC,gBAAgB,GAAG,MAAMC,OAAO,CAACC,GAAG,CAAClC,WAAW,CAAChK,GAAG,CAAC,MAAOoL,UAAU,IAAK;MAC7E,IAAIA,UAAU,CAACzM,OAAO,CAACiB,iBAAiB,EAAEC,aAAa,EAAE;QACrD,OAAO,CAAC,MAAM,IAAI,CAAC6L,wBAAwB,CAAC,CAACN,UAAU,CAACzM,OAAO,CAAC,CAAC,EAC5DwN,eAAe,CAAC,CAAC,CAAC;MAC3B,CAAC,MACI;QACD,OAAO,MAAM,IAAI,CAACN,YAAY,CAACT,UAAU,CAACzM,OAAO,CAACe,OAAO,CAAC;MAC9D;IACJ,CAAC,CAAC,CAAC;IACH,OAAOsM,gBAAgB,CAACI,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;EACtD;EACA,MAAMZ,wBAAwBA,CAACrM,QAAQ,EAAE;IACrC,IAAIsM,UAAU,GAAG,CAAC;IAClB,IAAIY,gBAAgB,GAAG,CAAC;IACxB,IAAIC,aAAa,GAAG,CAAC;IACrB;IACA,IAAI,IAAI,CAAClN,KAAK,KAAK,oBAAoB,EAAE;MACrCiN,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC,CAAC;IACtB,CAAC,MACI;MACDD,gBAAgB,GAAG,CAAC;MACpBC,aAAa,GAAG,CAAC;IACrB;IACA,MAAML,eAAe,GAAG,MAAMF,OAAO,CAACC,GAAG,CAAC7M,QAAQ,CAACW,GAAG,CAAC,MAAOrB,OAAO,IAAK;MACtE,MAAM8N,SAAS,GAAG,MAAM,IAAI,CAACZ,YAAY,CAAClN,OAAO,CAACe,OAAO,CAAC;MAC1D,MAAMgN,SAAS,GAAG,MAAM,IAAI,CAACb,YAAY,CAAC9M,mBAAmB,CAACJ,OAAO,CAAC,CAAC;MACvE,MAAMgO,SAAS,GAAGhO,OAAO,CAACgB,IAAI,KAAKc,SAAS,GACtC+L,aAAa,IAAI,MAAM,IAAI,CAACX,YAAY,CAAClN,OAAO,CAACgB,IAAI,CAAC,CAAC,GACvD,CAAC;MACP,IAAIiN,KAAK,GAAGH,SAAS,GAAGF,gBAAgB,GAAGG,SAAS,GAAGC,SAAS;MAChE;MACA,MAAME,aAAa,GAAGlO,OAAO;MAC7B,IAAIkO,aAAa,CAAC5N,QAAQ,CAAC,CAAC,KAAK,UAAU,EAAE;QACzC2N,KAAK,IAAI,CAAC;MACd;MACA,IAAIC,aAAa,CAACjN,iBAAiB,EAAEC,aAAa,EAAE;QAChD+M,KAAK,IAAI,CAAC;MACd;MACA,IAAIC,aAAa,EAAEjN,iBAAiB,CAACC,aAAa,EAAEF,IAAI,EAAE;QACtDiN,KAAK,IAAI,MAAM,IAAI,CAACf,YAAY,CAACgB,aAAa,CAACjN,iBAAiB,CAACC,aAAa,EAAEF,IAAI,CAAC;MACzF;MACA,IAAIkN,aAAa,CAACjN,iBAAiB,CAACC,aAAa,EAAEoH,SAAS,EAAE;QAC1D,IAAI;UACA2F,KAAK,IAAI,MAAM,IAAI,CAACf,YAAY;UAChC;UACAiB,IAAI,CAACC,SAAS,CAACD,IAAI,CAACE,KAAK,CAACH,aAAa,CAACjN,iBAAiB,CAACC,aAAa,EAAEoH,SAAS,CAAC,CAAC,CAAC;QACzF,CAAC,CACD,OAAOgG,KAAK,EAAE;UACVpO,OAAO,CAACoO,KAAK,CAAC,kCAAkC,EAAEA,KAAK,EAAEH,IAAI,CAACC,SAAS,CAACF,aAAa,CAACjN,iBAAiB,CAACC,aAAa,CAAC,CAAC;UACvH+M,KAAK,IAAI,MAAM,IAAI,CAACf,YAAY,CAACgB,aAAa,CAACjN,iBAAiB,CAACC,aAAa,EAAEoH,SAAS,CAAC;QAC9F;MACJ;MACA0E,UAAU,IAAIiB,KAAK;MACnB,OAAOA,KAAK;IAChB,CAAC,CAAC,CAAC;IACHjB,UAAU,IAAI,CAAC,CAAC,CAAC;IACjB,OAAO;MAAEA,UAAU;MAAEQ;IAAgB,CAAC;EAC1C;EACA,MAAM1E,mBAAmBA,CAACyF,OAAO,EAAE9J,OAAO,EAAE;IACxC,MAAM+J,cAAc,GAAG,IAAI,CAACC,iBAAiB,CAAChK,OAAO,CAAC;IACtD,OAAO,IAAI,CAACiK,MAAM,CAACC,IAAI,CAAC,YAAY;MAChC,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAACC,MAAM,CAACC,IAAI,CAACC,WAAW,CAACC,MAAM,CAACT,OAAO,EAAEC,cAAc,CAAC;QAC9E,OAAOI,GAAG;MACd,CAAC,CACD,OAAOtH,CAAC,EAAE;QACN,MAAMgH,KAAK,GAAG1O,qBAAqB,CAAC0H,CAAC,CAAC;QACtC,MAAMgH,KAAK;MACf;IACJ,CAAC,CAAC;EACN;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMjC,6BAA6BA,CAACkC,OAAO,EAAE9J;EAC7C;EAAA,EACE;IACE,MAAM+J,cAAc,GAAG,IAAI,CAACC,iBAAiB,CAAChK,OAAO,CAAC;IACtD,OAAO,IAAI,CAACiK,MAAM,CAACC,IAAI,CAAC,YAAY;MAChC,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAACC,MAAM,CAACI,IAAI,CAACH,IAAI,CAACC,WAAW,CAACV,KAAK,CAACE,OAAO,EAAEC,cAAc,CAAC;QAClF,OAAOI,GAAG;MACd,CAAC,CACD,OAAOtH,CAAC,EAAE;QACN,MAAMgH,KAAK,GAAG1O,qBAAqB,CAAC0H,CAAC,CAAC;QACtC,MAAMgH,KAAK;MACf;IACJ,CAAC,CAAC;EACN;EACAG,iBAAiBA,CAAChK,OAAO,EAAE;IACvB,IAAI,CAAC,IAAI,CAACoK,MAAM,EAAE;MACd,MAAMK,oBAAoB,GAAG;QACzBC,OAAO,EAAE,IAAI,CAAC9K,YAAY,CAAC8K;MAC/B,CAAC;MACD,MAAMC,QAAQ,GAAG1P,WAAW,CAACwP,oBAAoB,CAAC;MAClD,MAAMxK,MAAM,GAAG;QACX,GAAG,IAAI,CAACL,YAAY;QACpB8K,OAAO,EAAEC,QAAQ;QACjBlM,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBmM,UAAU,EAAE;MAChB,CAAC;MACD,IAAI,CAAC3K,MAAM,CAACyK,OAAO,EAAE;QACjB,OAAOzK,MAAM,CAACyK,OAAO;MACzB;MACA,IAAI,CAACN,MAAM,GAAG,IAAI3Q,YAAY,CAACwG,MAAM,CAAC;IAC1C;IACA,MAAM8J,cAAc,GAAG;MACnB,GAAG,IAAI,CAACnK,YAAY;MACpB,GAAGI;IACP,CAAC;IACD,OAAO+J,cAAc;EACzB;EACAc,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;EACA;EACAC,iBAAiBA,CAAC,GAAGC,UAAU,EAAE;IAC7B,OAAOA,UAAU,CAAC/B,MAAM,CAAC,CAACgC,GAAG,EAAEzD,SAAS,KAAK;MACzC,IAAIA,SAAS,IAAIA,SAAS,CAACa,UAAU,EAAE;QACnC4C,GAAG,CAAC5C,UAAU,CAACV,gBAAgB,IAC3BH,SAAS,CAACa,UAAU,CAACV,gBAAgB,IAAI,CAAC;QAC9CsD,GAAG,CAAC5C,UAAU,CAACX,YAAY,IAAIF,SAAS,CAACa,UAAU,CAACX,YAAY,IAAI,CAAC;QACrEuD,GAAG,CAAC5C,UAAU,CAACT,WAAW,IAAIJ,SAAS,CAACa,UAAU,CAACT,WAAW,IAAI,CAAC;MACvE;MACA,OAAOqD,GAAG;IACd,CAAC,EAAE;MACC5C,UAAU,EAAE;QACRV,gBAAgB,EAAE,CAAC;QACnBD,YAAY,EAAE,CAAC;QACfE,WAAW,EAAE;MACjB;IACJ,CAAC,CAAC;EACN;EACAsD,oBAAoBA,CAACC,YAAY,EAAEC,MAAM,EAAE;IACvC;IACA,IAAIlK,MAAM;IACV,IAAI1E,IAAI;IACR,IAAI6O,MAAM;IACV,IAAIC,UAAU;IACd,IAAIC,8BAA8B,CAACJ,YAAY,CAAC,EAAE;MAC9CjK,MAAM,GAAGiK,YAAY,CAACjK,MAAM;MAC5B1E,IAAI,GAAG2O,YAAY,CAAC3O,IAAI;MACxB6O,MAAM,GAAGF,YAAY,CAACE,MAAM;MAC5BC,UAAU,GAAGH,YAAY,CAACG,UAAU;IACxC,CAAC,MACI;MACDpK,MAAM,GAAGiK,YAAY;MACrB3O,IAAI,GAAG4O,MAAM,EAAE5O,IAAI;MACnB6O,MAAM,GAAGD,MAAM,EAAEC,MAAM;MACvBC,UAAU,GAAGF,MAAM,EAAEE,UAAU;IACnC;IACA,IAAIE,GAAG;IACP,IAAIC,YAAY;IAChB,IAAIL,MAAM,EAAE/N,MAAM,KAAKC,SAAS,IAAI+N,MAAM,KAAK,UAAU,EAAE;MACvD,MAAM,IAAIrP,KAAK,CAAC,uEAAuE,CAAC;IAC5F;IACA,IAAI,CAAC,IAAI,CAACG,KAAK,CAACE,UAAU,CAAC,OAAO,CAAC,IAC/B,CAAC,IAAI,CAACF,KAAK,CAACE,UAAU,CAAC,QAAQ,CAAC,IAChC,IAAI,CAACF,KAAK,KAAK,OAAO,EAAE;MACxB,IAAIkP,MAAM,KAAK/N,SAAS,EAAE;QACtB+N,MAAM,GAAG,YAAY;MACzB;IACJ,CAAC,MACI,IAAIA,MAAM,KAAK,YAAY,EAAE;MAC9B3P,OAAO,CAACC,IAAI,CAAC,sDAAsD,IAAI,CAACQ,KAAK,kCAAkC,CAAC;IACpH;IACA,IAAIkP,MAAM,KAAK,UAAU,EAAE;MACvBG,GAAG,GAAG,IAAI,CAAC1K,IAAI,CAAC;QACZmB,eAAe,EAAE;UAAEpG,IAAI,EAAE;QAAc;MAC3C,CAAC,CAAC;MACF,IAAIsF,WAAW,CAACD,MAAM,CAAC,EAAE;QACrBuK,YAAY,GAAG9Q,sBAAsB,CAAC+Q,aAAa,CAACxK,MAAM,CAAC;MAC/D,CAAC,MACI;QACDuK,YAAY,GAAG,IAAI/Q,gBAAgB,CAAC,CAAC;MACzC;IACJ,CAAC,MACI,IAAI2Q,MAAM,KAAK,YAAY,EAAE;MAC9BG,GAAG,GAAG,IAAI,CAAC1K,IAAI,CAAC;QACZmB,eAAe,EAAE;UACbpG,IAAI,EAAE,aAAa;UACnBoF,WAAW,EAAE;YACTzE,IAAI,EAAEA,IAAI,IAAI,SAAS;YACvB4E,WAAW,EAAEF,MAAM,CAACE,WAAW;YAC/BF,MAAM;YACN7D,MAAM,EAAE+N,MAAM,EAAE/N;UACpB;QACJ;MACJ,CAAC,CAAC;MACF,IAAI8D,WAAW,CAACD,MAAM,CAAC,EAAE;QACrBuK,YAAY,GAAG9Q,sBAAsB,CAAC+Q,aAAa,CAACxK,MAAM,CAAC;MAC/D,CAAC,MACI;QACDuK,YAAY,GAAG,IAAI/Q,gBAAgB,CAAC,CAAC;MACzC;IACJ,CAAC,MACI;MACD,IAAIiR,YAAY,GAAGnP,IAAI,IAAI,SAAS;MACpC;MACA,IAAI2E,WAAW,CAACD,MAAM,CAAC,EAAE;QACrB,MAAM0K,YAAY,GAAG5Q,eAAe,CAACkG,MAAM,CAAC;QAC5CsK,GAAG,GAAG,IAAI,CAAC1K,IAAI,CAAC;UACZF,KAAK,EAAE,CACH;YACI/E,IAAI,EAAE,UAAU;YAChB0B,QAAQ,EAAE;cACNf,IAAI,EAAEmP,YAAY;cAClBvK,WAAW,EAAEwK,YAAY,CAACxK,WAAW;cACrCyK,UAAU,EAAED;YAChB;UACJ,CAAC,CACJ;UACD5J,WAAW,EAAE;YACTnG,IAAI,EAAE,UAAU;YAChB0B,QAAQ,EAAE;cACNf,IAAI,EAAEmP;YACV;UACJ,CAAC;UACD;UACA,IAAIP,MAAM,EAAE/N,MAAM,KAAKC,SAAS,GAAG;YAAED,MAAM,EAAE+N,MAAM,CAAC/N;UAAO,CAAC,GAAG,CAAC,CAAC;QACrE,CAAC,CAAC;QACFoO,YAAY,GAAG,IAAI7Q,wBAAwB,CAAC;UACxCkR,YAAY,EAAE,IAAI;UAClBC,OAAO,EAAEJ,YAAY;UACrBK,SAAS,EAAE9K;QACf,CAAC,CAAC;MACN,CAAC,MACI;QACD,IAAI+K,wBAAwB;QAC5B,IAAI,OAAO/K,MAAM,CAAC1E,IAAI,KAAK,QAAQ,IAC/B,OAAO0E,MAAM,CAAC2K,UAAU,KAAK,QAAQ,IACrC3K,MAAM,CAAC2K,UAAU,IAAI,IAAI,EAAE;UAC3BI,wBAAwB,GAAG/K,MAAM;UACjCyK,YAAY,GAAGzK,MAAM,CAAC1E,IAAI;QAC9B,CAAC,MACI;UACDmP,YAAY,GAAGzK,MAAM,CAACgL,KAAK,IAAIP,YAAY;UAC3CM,wBAAwB,GAAG;YACvBzP,IAAI,EAAEmP,YAAY;YAClBvK,WAAW,EAAEF,MAAM,CAACE,WAAW,IAAI,EAAE;YACrCyK,UAAU,EAAE3K;UAChB,CAAC;QACL;QACAsK,GAAG,GAAG,IAAI,CAAC1K,IAAI,CAAC;UACZF,KAAK,EAAE,CACH;YACI/E,IAAI,EAAE,UAAU;YAChB0B,QAAQ,EAAE0O;UACd,CAAC,CACJ;UACDjK,WAAW,EAAE;YACTnG,IAAI,EAAE,UAAU;YAChB0B,QAAQ,EAAE;cACNf,IAAI,EAAEmP;YACV;UACJ,CAAC;UACD;UACA,IAAIP,MAAM,EAAE/N,MAAM,KAAKC,SAAS,GAAG;YAAED,MAAM,EAAE+N,MAAM,CAAC/N;UAAO,CAAC,GAAG,CAAC,CAAC;QACrE,CAAC,CAAC;QACFoO,YAAY,GAAG,IAAI7Q,wBAAwB,CAAC;UACxCkR,YAAY,EAAE,IAAI;UAClBC,OAAO,EAAEJ;QACb,CAAC,CAAC;MACN;IACJ;IACA,IAAI,CAACL,UAAU,EAAE;MACb,OAAOE,GAAG,CAACW,IAAI,CAACV,YAAY,CAAC;IACjC;IACA,MAAMW,YAAY,GAAG5R,mBAAmB,CAAC6R,MAAM,CAAC;MAC5C;MACAC,MAAM,EAAEA,CAACC,KAAK,EAAEnB,MAAM,KAAKK,YAAY,CAACe,MAAM,CAACD,KAAK,CAACE,GAAG,EAAErB,MAAM;IACpE,CAAC,CAAC;IACF,MAAMsB,UAAU,GAAGlS,mBAAmB,CAAC6R,MAAM,CAAC;MAC1CC,MAAM,EAAEA,CAAA,KAAM;IAClB,CAAC,CAAC;IACF,MAAMK,kBAAkB,GAAGP,YAAY,CAACQ,aAAa,CAAC;MAClDC,SAAS,EAAE,CAACH,UAAU;IAC1B,CAAC,CAAC;IACF,OAAOjS,gBAAgB,CAACqS,IAAI,CAAC,CACzB;MACIL,GAAG,EAAEjB;IACT,CAAC,EACDmB,kBAAkB,CACrB,CAAC;EACN;AACJ;AACA,SAASxL,WAAWA;AACpB;AACAoL,KAAK,EAAE;EACH;EACA,OAAO,OAAOA,KAAK,EAAE1C,KAAK,KAAK,UAAU;AAC7C;AACA,SAAS0B,8BAA8BA,CAACwB;AACxC;AAAA,EACE;EACE,OAAQA,CAAC,KAAKzP,SAAS;EACnB;EACA,OAAOyP,CAAC,CAAC7L,MAAM,KACX,QAAQ;AACpB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}