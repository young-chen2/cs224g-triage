{"ast":null,"code":"import { insecureHash } from \"../utils/hash.js\";\nimport { mapStoredMessageToChatMessage } from \"../messages/utils.js\";\n/**\n * This cache key should be consistent across all versions of LangChain.\n * It is currently NOT consistent across versions of LangChain.\n *\n * A huge benefit of having a remote cache (like redis) is that you can\n * access the cache from different processes/machines. The allows you to\n * separate concerns and scale horizontally.\n *\n * TODO: Make cache key consistent across versions of LangChain.\n */\nexport const getCacheKey = (...strings) => insecureHash(strings.join(\"_\"));\nexport function deserializeStoredGeneration(storedGeneration) {\n  if (storedGeneration.message !== undefined) {\n    return {\n      text: storedGeneration.text,\n      message: mapStoredMessageToChatMessage(storedGeneration.message)\n    };\n  } else {\n    return {\n      text: storedGeneration.text\n    };\n  }\n}\nexport function serializeGeneration(generation) {\n  const serializedValue = {\n    text: generation.text\n  };\n  if (generation.message !== undefined) {\n    serializedValue.message = generation.message.toDict();\n  }\n  return serializedValue;\n}\n/**\n * Base class for all caches. All caches should extend this class.\n */\nexport class BaseCache {}\nconst GLOBAL_MAP = new Map();\n/**\n * A cache for storing LLM generations that stores data in memory.\n */\nexport class InMemoryCache extends BaseCache {\n  constructor(map) {\n    super();\n    Object.defineProperty(this, \"cache\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.cache = map ?? new Map();\n  }\n  /**\n   * Retrieves data from the cache using a prompt and an LLM key. If the\n   * data is not found, it returns null.\n   * @param prompt The prompt used to find the data.\n   * @param llmKey The LLM key used to find the data.\n   * @returns The data corresponding to the prompt and LLM key, or null if not found.\n   */\n  lookup(prompt, llmKey) {\n    return Promise.resolve(this.cache.get(getCacheKey(prompt, llmKey)) ?? null);\n  }\n  /**\n   * Updates the cache with new data using a prompt and an LLM key.\n   * @param prompt The prompt used to store the data.\n   * @param llmKey The LLM key used to store the data.\n   * @param value The data to be stored.\n   */\n  async update(prompt, llmKey, value) {\n    this.cache.set(getCacheKey(prompt, llmKey), value);\n  }\n  /**\n   * Returns a global instance of InMemoryCache using a predefined global\n   * map as the initial cache.\n   * @returns A global instance of InMemoryCache.\n   */\n  static global() {\n    return new InMemoryCache(GLOBAL_MAP);\n  }\n}","map":{"version":3,"names":["insecureHash","mapStoredMessageToChatMessage","getCacheKey","strings","join","deserializeStoredGeneration","storedGeneration","message","undefined","text","serializeGeneration","generation","serializedValue","toDict","BaseCache","GLOBAL_MAP","Map","InMemoryCache","constructor","map","Object","defineProperty","enumerable","configurable","writable","value","cache","lookup","prompt","llmKey","Promise","resolve","get","update","set","global"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/@langchain/core/dist/caches/base.js"],"sourcesContent":["import { insecureHash } from \"../utils/hash.js\";\nimport { mapStoredMessageToChatMessage } from \"../messages/utils.js\";\n/**\n * This cache key should be consistent across all versions of LangChain.\n * It is currently NOT consistent across versions of LangChain.\n *\n * A huge benefit of having a remote cache (like redis) is that you can\n * access the cache from different processes/machines. The allows you to\n * separate concerns and scale horizontally.\n *\n * TODO: Make cache key consistent across versions of LangChain.\n */\nexport const getCacheKey = (...strings) => insecureHash(strings.join(\"_\"));\nexport function deserializeStoredGeneration(storedGeneration) {\n    if (storedGeneration.message !== undefined) {\n        return {\n            text: storedGeneration.text,\n            message: mapStoredMessageToChatMessage(storedGeneration.message),\n        };\n    }\n    else {\n        return { text: storedGeneration.text };\n    }\n}\nexport function serializeGeneration(generation) {\n    const serializedValue = {\n        text: generation.text,\n    };\n    if (generation.message !== undefined) {\n        serializedValue.message = generation.message.toDict();\n    }\n    return serializedValue;\n}\n/**\n * Base class for all caches. All caches should extend this class.\n */\nexport class BaseCache {\n}\nconst GLOBAL_MAP = new Map();\n/**\n * A cache for storing LLM generations that stores data in memory.\n */\nexport class InMemoryCache extends BaseCache {\n    constructor(map) {\n        super();\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.cache = map ?? new Map();\n    }\n    /**\n     * Retrieves data from the cache using a prompt and an LLM key. If the\n     * data is not found, it returns null.\n     * @param prompt The prompt used to find the data.\n     * @param llmKey The LLM key used to find the data.\n     * @returns The data corresponding to the prompt and LLM key, or null if not found.\n     */\n    lookup(prompt, llmKey) {\n        return Promise.resolve(this.cache.get(getCacheKey(prompt, llmKey)) ?? null);\n    }\n    /**\n     * Updates the cache with new data using a prompt and an LLM key.\n     * @param prompt The prompt used to store the data.\n     * @param llmKey The LLM key used to store the data.\n     * @param value The data to be stored.\n     */\n    async update(prompt, llmKey, value) {\n        this.cache.set(getCacheKey(prompt, llmKey), value);\n    }\n    /**\n     * Returns a global instance of InMemoryCache using a predefined global\n     * map as the initial cache.\n     * @returns A global instance of InMemoryCache.\n     */\n    static global() {\n        return new InMemoryCache(GLOBAL_MAP);\n    }\n}\n"],"mappings":"AAAA,SAASA,YAAY,QAAQ,kBAAkB;AAC/C,SAASC,6BAA6B,QAAQ,sBAAsB;AACpE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,WAAW,GAAGA,CAAC,GAAGC,OAAO,KAAKH,YAAY,CAACG,OAAO,CAACC,IAAI,CAAC,GAAG,CAAC,CAAC;AAC1E,OAAO,SAASC,2BAA2BA,CAACC,gBAAgB,EAAE;EAC1D,IAAIA,gBAAgB,CAACC,OAAO,KAAKC,SAAS,EAAE;IACxC,OAAO;MACHC,IAAI,EAAEH,gBAAgB,CAACG,IAAI;MAC3BF,OAAO,EAAEN,6BAA6B,CAACK,gBAAgB,CAACC,OAAO;IACnE,CAAC;EACL,CAAC,MACI;IACD,OAAO;MAAEE,IAAI,EAAEH,gBAAgB,CAACG;IAAK,CAAC;EAC1C;AACJ;AACA,OAAO,SAASC,mBAAmBA,CAACC,UAAU,EAAE;EAC5C,MAAMC,eAAe,GAAG;IACpBH,IAAI,EAAEE,UAAU,CAACF;EACrB,CAAC;EACD,IAAIE,UAAU,CAACJ,OAAO,KAAKC,SAAS,EAAE;IAClCI,eAAe,CAACL,OAAO,GAAGI,UAAU,CAACJ,OAAO,CAACM,MAAM,CAAC,CAAC;EACzD;EACA,OAAOD,eAAe;AAC1B;AACA;AACA;AACA;AACA,OAAO,MAAME,SAAS,CAAC;AAEvB,MAAMC,UAAU,GAAG,IAAIC,GAAG,CAAC,CAAC;AAC5B;AACA;AACA;AACA,OAAO,MAAMC,aAAa,SAASH,SAAS,CAAC;EACzCI,WAAWA,CAACC,GAAG,EAAE;IACb,KAAK,CAAC,CAAC;IACPC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,OAAO,EAAE;MACjCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACC,KAAK,GAAGP,GAAG,IAAI,IAAIH,GAAG,CAAC,CAAC;EACjC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIW,MAAMA,CAACC,MAAM,EAAEC,MAAM,EAAE;IACnB,OAAOC,OAAO,CAACC,OAAO,CAAC,IAAI,CAACL,KAAK,CAACM,GAAG,CAAC9B,WAAW,CAAC0B,MAAM,EAAEC,MAAM,CAAC,CAAC,IAAI,IAAI,CAAC;EAC/E;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMI,MAAMA,CAACL,MAAM,EAAEC,MAAM,EAAEJ,KAAK,EAAE;IAChC,IAAI,CAACC,KAAK,CAACQ,GAAG,CAAChC,WAAW,CAAC0B,MAAM,EAAEC,MAAM,CAAC,EAAEJ,KAAK,CAAC;EACtD;EACA;AACJ;AACA;AACA;AACA;EACI,OAAOU,MAAMA,CAAA,EAAG;IACZ,OAAO,IAAIlB,aAAa,CAACF,UAAU,CAAC;EACxC;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}