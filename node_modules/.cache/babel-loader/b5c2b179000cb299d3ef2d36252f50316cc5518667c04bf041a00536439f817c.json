{"ast":null,"code":"import { BaseLanguageModel } from \"@langchain/core/language_models/base\";\nimport { BasePromptTemplate } from \"@langchain/core/prompts\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { BaseChain } from \"./base.js\";\nimport { NoOpOutputParser } from \"../output_parsers/noop.js\";\nfunction isBaseLanguageModel(llmLike) {\n  return typeof llmLike._llmType === \"function\";\n}\nfunction _getLanguageModel(llmLike) {\n  if (isBaseLanguageModel(llmLike)) {\n    return llmLike;\n  } else if (\"bound\" in llmLike && Runnable.isRunnable(llmLike.bound)) {\n    return _getLanguageModel(llmLike.bound);\n  } else if (\"runnable\" in llmLike && \"fallbacks\" in llmLike && Runnable.isRunnable(llmLike.runnable)) {\n    return _getLanguageModel(llmLike.runnable);\n  } else if (\"default\" in llmLike && Runnable.isRunnable(llmLike.default)) {\n    return _getLanguageModel(llmLike.default);\n  } else {\n    throw new Error(\"Unable to extract BaseLanguageModel from llmLike object.\");\n  }\n}\n/**\n * @deprecated This class will be removed in 1.0.0. Use the LangChain Expression Language (LCEL) instead.\n * See the example below for how to use LCEL with the LLMChain class:\n *\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const prompt = ChatPromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new ChatOpenAI();\n * const chain = prompt.pipe(llm);\n *\n * const response = await chain.invoke({ adjective: \"funny\" });\n * ```\n */\nexport class LLMChain extends BaseChain {\n  static lc_name() {\n    return \"LLMChain\";\n  }\n  get inputKeys() {\n    return this.prompt.inputVariables;\n  }\n  get outputKeys() {\n    return [this.outputKey];\n  }\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"prompt\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"llm\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"llmKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"outputKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"text\"\n    });\n    Object.defineProperty(this, \"outputParser\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.prompt = fields.prompt;\n    this.llm = fields.llm;\n    this.llmKwargs = fields.llmKwargs;\n    this.outputKey = fields.outputKey ?? this.outputKey;\n    this.outputParser = fields.outputParser ?? new NoOpOutputParser();\n    if (this.prompt.outputParser) {\n      if (fields.outputParser) {\n        throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n      }\n      this.outputParser = this.prompt.outputParser;\n    }\n  }\n  getCallKeys() {\n    const callKeys = \"callKeys\" in this.llm ? this.llm.callKeys : [];\n    return callKeys;\n  }\n  /** @ignore */\n  _selectMemoryInputs(values) {\n    const valuesForMemory = super._selectMemoryInputs(values);\n    const callKeys = this.getCallKeys();\n    for (const key of callKeys) {\n      if (key in values) {\n        delete valuesForMemory[key];\n      }\n    }\n    return valuesForMemory;\n  }\n  /** @ignore */\n  async _getFinalOutput(generations, promptValue, runManager) {\n    let finalCompletion;\n    if (this.outputParser) {\n      finalCompletion = await this.outputParser.parseResultWithPrompt(generations, promptValue, runManager?.getChild());\n    } else {\n      finalCompletion = generations[0].text;\n    }\n    return finalCompletion;\n  }\n  /**\n   * Run the core logic of this chain and add to output if desired.\n   *\n   * Wraps _call and handles memory.\n   */\n  call(values, config) {\n    return super.call(values, config);\n  }\n  /** @ignore */\n  async _call(values, runManager) {\n    const valuesForPrompt = {\n      ...values\n    };\n    const valuesForLLM = {\n      ...this.llmKwargs\n    };\n    const callKeys = this.getCallKeys();\n    for (const key of callKeys) {\n      if (key in values) {\n        if (valuesForLLM) {\n          valuesForLLM[key] = values[key];\n          delete valuesForPrompt[key];\n        }\n      }\n    }\n    const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n    if (\"generatePrompt\" in this.llm) {\n      const {\n        generations\n      } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n      return {\n        [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager)\n      };\n    }\n    const modelWithParser = this.outputParser ? this.llm.pipe(this.outputParser) : this.llm;\n    const response = await modelWithParser.invoke(promptValue, runManager?.getChild());\n    return {\n      [this.outputKey]: response\n    };\n  }\n  /**\n   * Format prompt with values and pass to LLM\n   *\n   * @param values - keys to pass to prompt template\n   * @param callbackManager - CallbackManager to use\n   * @returns Completion from LLM.\n   *\n   * @example\n   * ```ts\n   * llm.predict({ adjective: \"funny\" })\n   * ```\n   */\n  async predict(values, callbackManager) {\n    const output = await this.call(values, callbackManager);\n    return output[this.outputKey];\n  }\n  _chainType() {\n    return \"llm\";\n  }\n  static async deserialize(data) {\n    const {\n      llm,\n      prompt\n    } = data;\n    if (!llm) {\n      throw new Error(\"LLMChain must have llm\");\n    }\n    if (!prompt) {\n      throw new Error(\"LLMChain must have prompt\");\n    }\n    return new LLMChain({\n      llm: await BaseLanguageModel.deserialize(llm),\n      prompt: await BasePromptTemplate.deserialize(prompt)\n    });\n  }\n  /** @deprecated */\n  serialize() {\n    const serialize = \"serialize\" in this.llm ? this.llm.serialize() : undefined;\n    return {\n      _type: `${this._chainType()}_chain`,\n      llm: serialize,\n      prompt: this.prompt.serialize()\n    };\n  }\n  _getNumTokens(text) {\n    return _getLanguageModel(this.llm).getNumTokens(text);\n  }\n}","map":{"version":3,"names":["BaseLanguageModel","BasePromptTemplate","Runnable","BaseChain","NoOpOutputParser","isBaseLanguageModel","llmLike","_llmType","_getLanguageModel","isRunnable","bound","runnable","default","Error","LLMChain","lc_name","inputKeys","prompt","inputVariables","outputKeys","outputKey","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","llm","llmKwargs","outputParser","getCallKeys","callKeys","_selectMemoryInputs","values","valuesForMemory","key","_getFinalOutput","generations","promptValue","runManager","finalCompletion","parseResultWithPrompt","getChild","text","call","config","_call","valuesForPrompt","valuesForLLM","formatPromptValue","generatePrompt","modelWithParser","pipe","response","invoke","predict","callbackManager","output","_chainType","deserialize","data","serialize","undefined","_type","_getNumTokens","getNumTokens"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/langchain/dist/chains/llm_chain.js"],"sourcesContent":["import { BaseLanguageModel, } from \"@langchain/core/language_models/base\";\nimport { BasePromptTemplate } from \"@langchain/core/prompts\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { BaseChain } from \"./base.js\";\nimport { NoOpOutputParser } from \"../output_parsers/noop.js\";\nfunction isBaseLanguageModel(llmLike) {\n    return typeof llmLike._llmType === \"function\";\n}\nfunction _getLanguageModel(llmLike) {\n    if (isBaseLanguageModel(llmLike)) {\n        return llmLike;\n    }\n    else if (\"bound\" in llmLike && Runnable.isRunnable(llmLike.bound)) {\n        return _getLanguageModel(llmLike.bound);\n    }\n    else if (\"runnable\" in llmLike &&\n        \"fallbacks\" in llmLike &&\n        Runnable.isRunnable(llmLike.runnable)) {\n        return _getLanguageModel(llmLike.runnable);\n    }\n    else if (\"default\" in llmLike && Runnable.isRunnable(llmLike.default)) {\n        return _getLanguageModel(llmLike.default);\n    }\n    else {\n        throw new Error(\"Unable to extract BaseLanguageModel from llmLike object.\");\n    }\n}\n/**\n * @deprecated This class will be removed in 1.0.0. Use the LangChain Expression Language (LCEL) instead.\n * See the example below for how to use LCEL with the LLMChain class:\n *\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const prompt = ChatPromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new ChatOpenAI();\n * const chain = prompt.pipe(llm);\n *\n * const response = await chain.invoke({ adjective: \"funny\" });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    static lc_name() {\n        return \"LLMChain\";\n    }\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llmKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.llmKwargs = fields.llmKwargs;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser =\n            fields.outputParser ?? new NoOpOutputParser();\n        if (this.prompt.outputParser) {\n            if (fields.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    getCallKeys() {\n        const callKeys = \"callKeys\" in this.llm ? this.llm.callKeys : [];\n        return callKeys;\n    }\n    /** @ignore */\n    _selectMemoryInputs(values) {\n        const valuesForMemory = super._selectMemoryInputs(values);\n        const callKeys = this.getCallKeys();\n        for (const key of callKeys) {\n            if (key in values) {\n                delete valuesForMemory[key];\n            }\n        }\n        return valuesForMemory;\n    }\n    /** @ignore */\n    async _getFinalOutput(generations, promptValue, runManager) {\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseResultWithPrompt(generations, promptValue, runManager?.getChild());\n        }\n        else {\n            finalCompletion = generations[0].text;\n        }\n        return finalCompletion;\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    call(values, config) {\n        return super.call(values, config);\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        const valuesForPrompt = { ...values };\n        const valuesForLLM = {\n            ...this.llmKwargs,\n        };\n        const callKeys = this.getCallKeys();\n        for (const key of callKeys) {\n            if (key in values) {\n                if (valuesForLLM) {\n                    valuesForLLM[key] =\n                        values[key];\n                    delete valuesForPrompt[key];\n                }\n            }\n        }\n        const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n        if (\"generatePrompt\" in this.llm) {\n            const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n            return {\n                [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager),\n            };\n        }\n        const modelWithParser = this.outputParser\n            ? this.llm.pipe(this.outputParser)\n            : this.llm;\n        const response = await modelWithParser.invoke(promptValue, runManager?.getChild());\n        return {\n            [this.outputKey]: response,\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @param callbackManager - CallbackManager to use\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values, callbackManager) {\n        const output = await this.call(values, callbackManager);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    /** @deprecated */\n    serialize() {\n        const serialize = \"serialize\" in this.llm ? this.llm.serialize() : undefined;\n        return {\n            _type: `${this._chainType()}_chain`,\n            llm: serialize,\n            prompt: this.prompt.serialize(),\n        };\n    }\n    _getNumTokens(text) {\n        return _getLanguageModel(this.llm).getNumTokens(text);\n    }\n}\n"],"mappings":"AAAA,SAASA,iBAAiB,QAAS,sCAAsC;AACzE,SAASC,kBAAkB,QAAQ,yBAAyB;AAC5D,SAASC,QAAQ,QAAQ,2BAA2B;AACpD,SAASC,SAAS,QAAQ,WAAW;AACrC,SAASC,gBAAgB,QAAQ,2BAA2B;AAC5D,SAASC,mBAAmBA,CAACC,OAAO,EAAE;EAClC,OAAO,OAAOA,OAAO,CAACC,QAAQ,KAAK,UAAU;AACjD;AACA,SAASC,iBAAiBA,CAACF,OAAO,EAAE;EAChC,IAAID,mBAAmB,CAACC,OAAO,CAAC,EAAE;IAC9B,OAAOA,OAAO;EAClB,CAAC,MACI,IAAI,OAAO,IAAIA,OAAO,IAAIJ,QAAQ,CAACO,UAAU,CAACH,OAAO,CAACI,KAAK,CAAC,EAAE;IAC/D,OAAOF,iBAAiB,CAACF,OAAO,CAACI,KAAK,CAAC;EAC3C,CAAC,MACI,IAAI,UAAU,IAAIJ,OAAO,IAC1B,WAAW,IAAIA,OAAO,IACtBJ,QAAQ,CAACO,UAAU,CAACH,OAAO,CAACK,QAAQ,CAAC,EAAE;IACvC,OAAOH,iBAAiB,CAACF,OAAO,CAACK,QAAQ,CAAC;EAC9C,CAAC,MACI,IAAI,SAAS,IAAIL,OAAO,IAAIJ,QAAQ,CAACO,UAAU,CAACH,OAAO,CAACM,OAAO,CAAC,EAAE;IACnE,OAAOJ,iBAAiB,CAACF,OAAO,CAACM,OAAO,CAAC;EAC7C,CAAC,MACI;IACD,MAAM,IAAIC,KAAK,CAAC,0DAA0D,CAAC;EAC/E;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,QAAQ,SAASX,SAAS,CAAC;EACpC,OAAOY,OAAOA,CAAA,EAAG;IACb,OAAO,UAAU;EACrB;EACA,IAAIC,SAASA,CAAA,EAAG;IACZ,OAAO,IAAI,CAACC,MAAM,CAACC,cAAc;EACrC;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO,CAAC,IAAI,CAACC,SAAS,CAAC;EAC3B;EACAC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,KAAK,EAAE;MAC/BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACX,MAAM,GAAGK,MAAM,CAACL,MAAM;IAC3B,IAAI,CAACY,GAAG,GAAGP,MAAM,CAACO,GAAG;IACrB,IAAI,CAACC,SAAS,GAAGR,MAAM,CAACQ,SAAS;IACjC,IAAI,CAACV,SAAS,GAAGE,MAAM,CAACF,SAAS,IAAI,IAAI,CAACA,SAAS;IACnD,IAAI,CAACW,YAAY,GACbT,MAAM,CAACS,YAAY,IAAI,IAAI3B,gBAAgB,CAAC,CAAC;IACjD,IAAI,IAAI,CAACa,MAAM,CAACc,YAAY,EAAE;MAC1B,IAAIT,MAAM,CAACS,YAAY,EAAE;QACrB,MAAM,IAAIlB,KAAK,CAAC,sDAAsD,CAAC;MAC3E;MACA,IAAI,CAACkB,YAAY,GAAG,IAAI,CAACd,MAAM,CAACc,YAAY;IAChD;EACJ;EACAC,WAAWA,CAAA,EAAG;IACV,MAAMC,QAAQ,GAAG,UAAU,IAAI,IAAI,CAACJ,GAAG,GAAG,IAAI,CAACA,GAAG,CAACI,QAAQ,GAAG,EAAE;IAChE,OAAOA,QAAQ;EACnB;EACA;EACAC,mBAAmBA,CAACC,MAAM,EAAE;IACxB,MAAMC,eAAe,GAAG,KAAK,CAACF,mBAAmB,CAACC,MAAM,CAAC;IACzD,MAAMF,QAAQ,GAAG,IAAI,CAACD,WAAW,CAAC,CAAC;IACnC,KAAK,MAAMK,GAAG,IAAIJ,QAAQ,EAAE;MACxB,IAAII,GAAG,IAAIF,MAAM,EAAE;QACf,OAAOC,eAAe,CAACC,GAAG,CAAC;MAC/B;IACJ;IACA,OAAOD,eAAe;EAC1B;EACA;EACA,MAAME,eAAeA,CAACC,WAAW,EAAEC,WAAW,EAAEC,UAAU,EAAE;IACxD,IAAIC,eAAe;IACnB,IAAI,IAAI,CAACX,YAAY,EAAE;MACnBW,eAAe,GAAG,MAAM,IAAI,CAACX,YAAY,CAACY,qBAAqB,CAACJ,WAAW,EAAEC,WAAW,EAAEC,UAAU,EAAEG,QAAQ,CAAC,CAAC,CAAC;IACrH,CAAC,MACI;MACDF,eAAe,GAAGH,WAAW,CAAC,CAAC,CAAC,CAACM,IAAI;IACzC;IACA,OAAOH,eAAe;EAC1B;EACA;AACJ;AACA;AACA;AACA;EACII,IAAIA,CAACX,MAAM,EAAEY,MAAM,EAAE;IACjB,OAAO,KAAK,CAACD,IAAI,CAACX,MAAM,EAAEY,MAAM,CAAC;EACrC;EACA;EACA,MAAMC,KAAKA,CAACb,MAAM,EAAEM,UAAU,EAAE;IAC5B,MAAMQ,eAAe,GAAG;MAAE,GAAGd;IAAO,CAAC;IACrC,MAAMe,YAAY,GAAG;MACjB,GAAG,IAAI,CAACpB;IACZ,CAAC;IACD,MAAMG,QAAQ,GAAG,IAAI,CAACD,WAAW,CAAC,CAAC;IACnC,KAAK,MAAMK,GAAG,IAAIJ,QAAQ,EAAE;MACxB,IAAII,GAAG,IAAIF,MAAM,EAAE;QACf,IAAIe,YAAY,EAAE;UACdA,YAAY,CAACb,GAAG,CAAC,GACbF,MAAM,CAACE,GAAG,CAAC;UACf,OAAOY,eAAe,CAACZ,GAAG,CAAC;QAC/B;MACJ;IACJ;IACA,MAAMG,WAAW,GAAG,MAAM,IAAI,CAACvB,MAAM,CAACkC,iBAAiB,CAACF,eAAe,CAAC;IACxE,IAAI,gBAAgB,IAAI,IAAI,CAACpB,GAAG,EAAE;MAC9B,MAAM;QAAEU;MAAY,CAAC,GAAG,MAAM,IAAI,CAACV,GAAG,CAACuB,cAAc,CAAC,CAACZ,WAAW,CAAC,EAAEU,YAAY,EAAET,UAAU,EAAEG,QAAQ,CAAC,CAAC,CAAC;MAC1G,OAAO;QACH,CAAC,IAAI,CAACxB,SAAS,GAAG,MAAM,IAAI,CAACkB,eAAe,CAACC,WAAW,CAAC,CAAC,CAAC,EAAEC,WAAW,EAAEC,UAAU;MACxF,CAAC;IACL;IACA,MAAMY,eAAe,GAAG,IAAI,CAACtB,YAAY,GACnC,IAAI,CAACF,GAAG,CAACyB,IAAI,CAAC,IAAI,CAACvB,YAAY,CAAC,GAChC,IAAI,CAACF,GAAG;IACd,MAAM0B,QAAQ,GAAG,MAAMF,eAAe,CAACG,MAAM,CAAChB,WAAW,EAAEC,UAAU,EAAEG,QAAQ,CAAC,CAAC,CAAC;IAClF,OAAO;MACH,CAAC,IAAI,CAACxB,SAAS,GAAGmC;IACtB,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAME,OAAOA,CAACtB,MAAM,EAAEuB,eAAe,EAAE;IACnC,MAAMC,MAAM,GAAG,MAAM,IAAI,CAACb,IAAI,CAACX,MAAM,EAAEuB,eAAe,CAAC;IACvD,OAAOC,MAAM,CAAC,IAAI,CAACvC,SAAS,CAAC;EACjC;EACAwC,UAAUA,CAAA,EAAG;IACT,OAAO,KAAK;EAChB;EACA,aAAaC,WAAWA,CAACC,IAAI,EAAE;IAC3B,MAAM;MAAEjC,GAAG;MAAEZ;IAAO,CAAC,GAAG6C,IAAI;IAC5B,IAAI,CAACjC,GAAG,EAAE;MACN,MAAM,IAAIhB,KAAK,CAAC,wBAAwB,CAAC;IAC7C;IACA,IAAI,CAACI,MAAM,EAAE;MACT,MAAM,IAAIJ,KAAK,CAAC,2BAA2B,CAAC;IAChD;IACA,OAAO,IAAIC,QAAQ,CAAC;MAChBe,GAAG,EAAE,MAAM7B,iBAAiB,CAAC6D,WAAW,CAAChC,GAAG,CAAC;MAC7CZ,MAAM,EAAE,MAAMhB,kBAAkB,CAAC4D,WAAW,CAAC5C,MAAM;IACvD,CAAC,CAAC;EACN;EACA;EACA8C,SAASA,CAAA,EAAG;IACR,MAAMA,SAAS,GAAG,WAAW,IAAI,IAAI,CAAClC,GAAG,GAAG,IAAI,CAACA,GAAG,CAACkC,SAAS,CAAC,CAAC,GAAGC,SAAS;IAC5E,OAAO;MACHC,KAAK,EAAE,GAAG,IAAI,CAACL,UAAU,CAAC,CAAC,QAAQ;MACnC/B,GAAG,EAAEkC,SAAS;MACd9C,MAAM,EAAE,IAAI,CAACA,MAAM,CAAC8C,SAAS,CAAC;IAClC,CAAC;EACL;EACAG,aAAaA,CAACrB,IAAI,EAAE;IAChB,OAAOrC,iBAAiB,CAAC,IAAI,CAACqB,GAAG,CAAC,CAACsC,YAAY,CAACtB,IAAI,CAAC;EACzD;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}