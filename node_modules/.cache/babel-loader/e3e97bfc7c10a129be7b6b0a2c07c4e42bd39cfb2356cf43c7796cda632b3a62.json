{"ast":null,"code":"import { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { AIMessage, HumanMessage, coerceMessageLikeToMessage, isAIMessageChunk, isBaseMessage, isAIMessage } from \"../messages/index.js\";\nimport { RUN_KEY } from \"../outputs.js\";\nimport { BaseLanguageModel } from \"./base.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { RunnableLambda, RunnableSequence } from \"../runnables/base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { RunnablePassthrough } from \"../runnables/passthrough.js\";\nimport { isZodSchema } from \"../utils/types/is_zod_schema.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n  const textEncoder = new TextEncoder();\n  return new TransformStream({\n    transform(chunk, controller) {\n      controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\" ? chunk.content : JSON.stringify(chunk.content)));\n    }\n  });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n  constructor(fields) {\n    super(fields);\n    // Only ever instantiated in main LangChain\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"chat_models\", this._llmType()]\n    });\n    Object.defineProperty(this, \"disableStreaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n  }\n  _separateRunnableConfigFromCallOptionsCompat(options) {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    callOptions.signal = runnableConfig.signal;\n    return [runnableConfig, callOptions];\n  }\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseChatModel._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n    const chatGeneration = result.generations[0][0];\n    // TODO: Remove cast after figuring out inheritance\n    return chatGeneration.message;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_messages, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseChatModel.prototype._streamResponseChunks || this.disableStreaming) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseChatModel._convertInputToPromptValue(input);\n      const messages = prompt.toChatMessages();\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(options);\n      const inheritableMetadata = {\n        ...runnableConfig.metadata,\n        ...this.getLsParams(callOptions)\n      };\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, inheritableMetadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1\n      };\n      const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], runnableConfig.runId, undefined, extra, undefined, undefined, runnableConfig.runName);\n      let generationChunk;\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      let llmOutput;\n      try {\n        for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          chunk.message.response_metadata = {\n            ...chunk.generationInfo,\n            ...chunk.message.response_metadata\n          };\n          yield chunk.message;\n          if (!generationChunk) {\n            generationChunk = chunk;\n          } else {\n            generationChunk = generationChunk.concat(chunk);\n          }\n          if (isAIMessageChunk(chunk.message) && chunk.message.usage_metadata !== undefined) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens\n              }\n            };\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMEnd({\n        // TODO: Remove cast after figuring out inheritance\n        generations: [[generationChunk]],\n        llmOutput\n      })));\n    }\n  }\n  getLsParams(options) {\n    const providerName = this.getName().startsWith(\"Chat\") ? this.getName().replace(\"Chat\", \"\") : this.getName();\n    return {\n      ls_model_type: \"chat\",\n      ls_stop: options.stop,\n      ls_provider: providerName\n    };\n  }\n  /** @ignore */\n  async _generateUncached(messages, parsedOptions, handledOptions, startedRunManagers) {\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    let runManagers;\n    if (startedRunManagers !== undefined && startedRunManagers.length === baseMessages.length) {\n      runManagers = startedRunManagers;\n    } else {\n      const inheritableMetadata = {\n        ...handledOptions.metadata,\n        ...this.getLsParams(parsedOptions)\n      };\n      // create callback manager and start run\n      const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, inheritableMetadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: 1\n      };\n      runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions.runName);\n    }\n    const generations = [];\n    const llmOutputs = [];\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(callbackHandlerPrefersStreaming);\n    if (hasStreamingHandler && !this.disableStreaming && baseMessages.length === 1 && this._streamResponseChunks !== BaseChatModel.prototype._streamResponseChunks) {\n      try {\n        const stream = await this._streamResponseChunks(baseMessages[0], parsedOptions, runManagers?.[0]);\n        let aggregated;\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let llmOutput;\n        for await (const chunk of stream) {\n          if (chunk.message.id == null) {\n            const runId = runManagers?.at(0)?.runId;\n            if (runId != null) chunk.message._updateId(`run-${runId}`);\n          }\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n          if (isAIMessageChunk(chunk.message) && chunk.message.usage_metadata !== undefined) {\n            llmOutput = {\n              tokenUsage: {\n                promptTokens: chunk.message.usage_metadata.input_tokens,\n                completionTokens: chunk.message.usage_metadata.output_tokens,\n                totalTokens: chunk.message.usage_metadata.total_tokens\n              }\n            };\n          }\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        generations.push([aggregated]);\n        await runManagers?.[0].handleLLMEnd({\n          generations,\n          llmOutput\n        });\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      // generate results\n      const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, {\n        ...parsedOptions,\n        promptIndex: i\n      }, runManagers?.[i])));\n      // handle results\n      await Promise.all(results.map(async (pResult, i) => {\n        if (pResult.status === \"fulfilled\") {\n          const result = pResult.value;\n          for (const generation of result.generations) {\n            if (generation.message.id == null) {\n              const runId = runManagers?.at(0)?.runId;\n              if (runId != null) generation.message._updateId(`run-${runId}`);\n            }\n            generation.message.response_metadata = {\n              ...generation.generationInfo,\n              ...generation.message.response_metadata\n            };\n          }\n          if (result.generations.length === 1) {\n            result.generations[0].message.response_metadata = {\n              ...result.llmOutput,\n              ...result.generations[0].message.response_metadata\n            };\n          }\n          generations[i] = result.generations;\n          llmOutputs[i] = result.llmOutput;\n          return runManagers?.[i]?.handleLLMEnd({\n            generations: [result.generations],\n            llmOutput: result.llmOutput\n          });\n        } else {\n          // status === \"rejected\"\n          await runManagers?.[i]?.handleLLMError(pResult.reason);\n          return Promise.reject(pResult.reason);\n        }\n      }));\n    }\n    // create combined output\n    const output = {\n      generations,\n      llmOutput: llmOutputs.length ? this._combineLLMOutput?.(...llmOutputs) : undefined\n    };\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  async _generateCached({\n    messages,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions\n  }) {\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    const inheritableMetadata = {\n      ...handledOptions.metadata,\n      ...this.getLsParams(parsedOptions)\n    };\n    // create callback manager and start run\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, inheritableMetadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: 1\n    };\n    const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions.runName);\n    // generate results\n    const missingPromptIndices = [];\n    const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n      // Join all content into one string for the prompt index\n      const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (result == null) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results.map((result, index) => ({\n      result,\n      runManager: runManagers?.[index]\n    })).filter(({\n      result\n    }) => result.status === \"fulfilled\" && result.value != null || result.status === \"rejected\");\n    // Handle results and call run managers\n    const generations = [];\n    await Promise.all(cachedResults.map(async ({\n      result: promiseResult,\n      runManager\n    }, i) => {\n      if (promiseResult.status === \"fulfilled\") {\n        const result = promiseResult.value;\n        generations[i] = result.map(result => {\n          if (\"message\" in result && isBaseMessage(result.message) && isAIMessage(result.message)) {\n            // eslint-disable-next-line no-param-reassign\n            result.message.usage_metadata = {\n              input_tokens: 0,\n              output_tokens: 0,\n              total_tokens: 0\n            };\n          }\n          // eslint-disable-next-line no-param-reassign\n          result.generationInfo = {\n            ...result.generationInfo,\n            tokenUsage: {}\n          };\n          return result;\n        });\n        if (result.length) {\n          await runManager?.handleLLMNewToken(result[0].text);\n        }\n        return runManager?.handleLLMEnd({\n          generations: [result]\n        }, undefined, undefined, undefined, {\n          cached: true\n        });\n      } else {\n        // status === \"rejected\"\n        await runManager?.handleLLMError(promiseResult.reason, undefined, undefined, undefined, {\n          cached: true\n        });\n        return Promise.reject(promiseResult.reason);\n      }\n    }));\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers\n    };\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generate(messages, options, callbacks) {\n    // parse call options\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const baseMessages = messages.map(messageList => messageList.map(coerceMessageLikeToMessage));\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n    if (!this.cache) {\n      return this._generateUncached(baseMessages, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n    const {\n      generations,\n      missingPromptIndices,\n      startedRunManagers\n    } = await this._generateCached({\n      messages: baseMessages,\n      cache,\n      llmStringKey,\n      parsedOptions: callOptions,\n      handledOptions: runnableConfig\n    });\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(missingPromptIndices.map(i => baseMessages[i]), callOptions, runnableConfig, startedRunManagers !== undefined ? missingPromptIndices.map(i => startedRunManagers?.[i]) : undefined);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        // Join all content into one string for the prompt index\n        const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n        return cache.update(prompt, llmStringKey, generation);\n      }));\n      llmOutput = results.llmOutput ?? {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _modelType() {\n    return \"base_chat_model\";\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this.invocationParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const promptMessages = promptValues.map(promptValue => promptValue.toChatMessages());\n    return this.generate(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async call(messages, options, callbacks) {\n    const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n    const generations = result.generations;\n    return generations[0][0].message;\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Makes a single call to the chat model with a prompt value.\n   * @param promptValue The value of the prompt.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async callPrompt(promptValue, options, callbacks) {\n    const promptMessages = promptValue.toChatMessages();\n    return this.call(promptMessages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on the input messages.\n   * @param messages An array of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a BaseMessage.\n   */\n  async predictMessages(messages, options, callbacks) {\n    return this.call(messages, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * Predicts the next message based on a text input.\n   * @param text The text input.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to a string.\n   */\n  async predict(text, options, callbacks) {\n    const message = new HumanMessage(text);\n    const result = await this.call([message], options, callbacks);\n    if (typeof result.content !== \"string\") {\n      throw new Error(\"Cannot use predict when output is not a string.\");\n    }\n    return result.content;\n  }\n  withStructuredOutput(outputSchema, config) {\n    if (typeof this.bindTools !== \"function\") {\n      throw new Error(`Chat model must implement \".bindTools()\" to use withStructuredOutput.`);\n    }\n    if (config?.strict) {\n      throw new Error(`\"strict\" mode is not supported for this model by default.`);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const schema = outputSchema;\n    const name = config?.name;\n    const description = schema.description ?? \"A function available to call.\";\n    const method = config?.method;\n    const includeRaw = config?.includeRaw;\n    if (method === \"jsonMode\") {\n      throw new Error(`Base withStructuredOutput implementation only supports \"functionCalling\" as a method.`);\n    }\n    let functionName = name ?? \"extract\";\n    let tools;\n    if (isZodSchema(schema)) {\n      tools = [{\n        type: \"function\",\n        function: {\n          name: functionName,\n          description,\n          parameters: zodToJsonSchema(schema)\n        }\n      }];\n    } else {\n      if (\"name\" in schema) {\n        functionName = schema.name;\n      }\n      tools = [{\n        type: \"function\",\n        function: {\n          name: functionName,\n          description,\n          parameters: schema\n        }\n      }];\n    }\n    const llm = this.bindTools(tools);\n    const outputParser = RunnableLambda.from(input => {\n      if (!input.tool_calls || input.tool_calls.length === 0) {\n        throw new Error(\"No tool calls found in the response.\");\n      }\n      const toolCall = input.tool_calls.find(tc => tc.name === functionName);\n      if (!toolCall) {\n        throw new Error(`No tool call found with name ${functionName}.`);\n      }\n      return toolCall.args;\n    });\n    if (!includeRaw) {\n      return llm.pipe(outputParser).withConfig({\n        runName: \"StructuredOutput\"\n      });\n    }\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input, config) => outputParser.invoke(input.raw, config)\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone]\n    });\n    return RunnableSequence.from([{\n      raw: llm\n    }, parsedWithFallback]).withConfig({\n      runName: \"StructuredOutputRunnable\"\n    });\n  }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n  async _generate(messages, options, runManager) {\n    const text = await this._call(messages, options, runManager);\n    const message = new AIMessage(text);\n    if (typeof message.content !== \"string\") {\n      throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n    }\n    return {\n      generations: [{\n        text: message.content,\n        message\n      }]\n    };\n  }\n}","map":{"version":3,"names":["zodToJsonSchema","AIMessage","HumanMessage","coerceMessageLikeToMessage","isAIMessageChunk","isBaseMessage","isAIMessage","RUN_KEY","BaseLanguageModel","CallbackManager","RunnableLambda","RunnableSequence","concat","RunnablePassthrough","isZodSchema","callbackHandlerPrefersStreaming","createChatMessageChunkEncoderStream","textEncoder","TextEncoder","TransformStream","transform","chunk","controller","enqueue","encode","content","JSON","stringify","BaseChatModel","constructor","fields","Object","defineProperty","enumerable","configurable","writable","value","_llmType","_separateRunnableConfigFromCallOptionsCompat","options","runnableConfig","callOptions","_separateRunnableConfigFromCallOptions","signal","invoke","input","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","chatGeneration","generations","message","_streamResponseChunks","_messages","_options","_runManager","Error","_streamIterator","prototype","disableStreaming","prompt","messages","toChatMessages","inheritableMetadata","metadata","getLsParams","callbackManager_","configure","tags","verbose","extra","invocation_params","invocationParams","batch_size","runManagers","handleChatModelStart","toJSON","runId","undefined","runName","generationChunk","llmOutput","id","at","_updateId","response_metadata","generationInfo","usage_metadata","tokenUsage","promptTokens","input_tokens","completionTokens","output_tokens","totalTokens","total_tokens","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","providerName","getName","startsWith","replace","ls_model_type","ls_stop","stop","ls_provider","_generateUncached","parsedOptions","handledOptions","startedRunManagers","baseMessages","messageList","length","llmOutputs","hasStreamingHandler","handlers","find","stream","aggregated","push","e","results","allSettled","i","_generate","promptIndex","pResult","status","generation","reason","reject","output","_combineLLMOutput","runIds","manager","_generateCached","cache","llmStringKey","missingPromptIndices","baseMessage","index","toString","lookup","cachedResults","filter","promiseResult","handleLLMNewToken","text","cached","generate","Array","isArray","_getSerializedCacheKeyParametersForCall","update","_modelType","serialize","_type","_model","promptValues","promptMessages","call","callPrompt","predictMessages","predict","withStructuredOutput","outputSchema","config","bindTools","strict","schema","name","description","method","includeRaw","functionName","tools","type","function","parameters","llm","outputParser","from","tool_calls","toolCall","tc","args","pipe","withConfig","parserAssign","assign","parsed","raw","parserNone","parsedWithFallback","withFallbacks","fallbacks","SimpleChatModel","_call"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/@langchain/core/dist/language_models/chat_models.js"],"sourcesContent":["import { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { AIMessage, HumanMessage, coerceMessageLikeToMessage, isAIMessageChunk, isBaseMessage, isAIMessage, } from \"../messages/index.js\";\nimport { RUN_KEY, } from \"../outputs.js\";\nimport { BaseLanguageModel, } from \"./base.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nimport { RunnableLambda, RunnableSequence, } from \"../runnables/base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { RunnablePassthrough } from \"../runnables/passthrough.js\";\nimport { isZodSchema } from \"../utils/types/is_zod_schema.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n/**\n * Creates a transform stream for encoding chat message chunks.\n * @deprecated Use {@link BytesOutputParser} instead\n * @returns A TransformStream instance that encodes chat message chunks.\n */\nexport function createChatMessageChunkEncoderStream() {\n    const textEncoder = new TextEncoder();\n    return new TransformStream({\n        transform(chunk, controller) {\n            controller.enqueue(textEncoder.encode(typeof chunk.content === \"string\"\n                ? chunk.content\n                : JSON.stringify(chunk.content)));\n        },\n    });\n}\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\nexport class BaseChatModel extends BaseLanguageModel {\n    constructor(fields) {\n        super(fields);\n        // Only ever instantiated in main LangChain\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"chat_models\", this._llmType()]\n        });\n        Object.defineProperty(this, \"disableStreaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n    }\n    _separateRunnableConfigFromCallOptionsCompat(options) {\n        // For backwards compat, keep `signal` in both runnableConfig and callOptions\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        callOptions.signal = runnableConfig.signal;\n        return [runnableConfig, callOptions];\n    }\n    /**\n     * Invokes the chat model with a single input.\n     * @param input The input for the language model.\n     * @param options The call options.\n     * @returns A Promise that resolves to a BaseMessageChunk.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseChatModel._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        const chatGeneration = result.generations[0][0];\n        // TODO: Remove cast after figuring out inheritance\n        return chatGeneration.message;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_messages, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks ===\n            BaseChatModel.prototype._streamResponseChunks ||\n            this.disableStreaming) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseChatModel._convertInputToPromptValue(input);\n            const messages = prompt.toChatMessages();\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(options);\n            const inheritableMetadata = {\n                ...runnableConfig.metadata,\n                ...this.getLsParams(callOptions),\n            };\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, inheritableMetadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n                batch_size: 1,\n            };\n            const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), [messages], runnableConfig.runId, undefined, extra, undefined, undefined, runnableConfig.runName);\n            let generationChunk;\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            let llmOutput;\n            try {\n                for await (const chunk of this._streamResponseChunks(messages, callOptions, runManagers?.[0])) {\n                    if (chunk.message.id == null) {\n                        const runId = runManagers?.at(0)?.runId;\n                        if (runId != null)\n                            chunk.message._updateId(`run-${runId}`);\n                    }\n                    chunk.message.response_metadata = {\n                        ...chunk.generationInfo,\n                        ...chunk.message.response_metadata,\n                    };\n                    yield chunk.message;\n                    if (!generationChunk) {\n                        generationChunk = chunk;\n                    }\n                    else {\n                        generationChunk = generationChunk.concat(chunk);\n                    }\n                    if (isAIMessageChunk(chunk.message) &&\n                        chunk.message.usage_metadata !== undefined) {\n                        llmOutput = {\n                            tokenUsage: {\n                                promptTokens: chunk.message.usage_metadata.input_tokens,\n                                completionTokens: chunk.message.usage_metadata.output_tokens,\n                                totalTokens: chunk.message.usage_metadata.total_tokens,\n                            },\n                        };\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                // TODO: Remove cast after figuring out inheritance\n                generations: [[generationChunk]],\n                llmOutput,\n            })));\n        }\n    }\n    getLsParams(options) {\n        const providerName = this.getName().startsWith(\"Chat\")\n            ? this.getName().replace(\"Chat\", \"\")\n            : this.getName();\n        return {\n            ls_model_type: \"chat\",\n            ls_stop: options.stop,\n            ls_provider: providerName,\n        };\n    }\n    /** @ignore */\n    async _generateUncached(messages, parsedOptions, handledOptions, startedRunManagers) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        let runManagers;\n        if (startedRunManagers !== undefined &&\n            startedRunManagers.length === baseMessages.length) {\n            runManagers = startedRunManagers;\n        }\n        else {\n            const inheritableMetadata = {\n                ...handledOptions.metadata,\n                ...this.getLsParams(parsedOptions),\n            };\n            // create callback manager and start run\n            const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, inheritableMetadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: parsedOptions,\n                invocation_params: this?.invocationParams(parsedOptions),\n                batch_size: 1,\n            };\n            runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions.runName);\n        }\n        const generations = [];\n        const llmOutputs = [];\n        // Even if stream is not explicitly called, check if model is implicitly\n        // called from streamEvents() or streamLog() to get all streamed events.\n        // Bail out if _streamResponseChunks not overridden\n        const hasStreamingHandler = !!runManagers?.[0].handlers.find(callbackHandlerPrefersStreaming);\n        if (hasStreamingHandler &&\n            !this.disableStreaming &&\n            baseMessages.length === 1 &&\n            this._streamResponseChunks !==\n                BaseChatModel.prototype._streamResponseChunks) {\n            try {\n                const stream = await this._streamResponseChunks(baseMessages[0], parsedOptions, runManagers?.[0]);\n                let aggregated;\n                // eslint-disable-next-line @typescript-eslint/no-explicit-any\n                let llmOutput;\n                for await (const chunk of stream) {\n                    if (chunk.message.id == null) {\n                        const runId = runManagers?.at(0)?.runId;\n                        if (runId != null)\n                            chunk.message._updateId(`run-${runId}`);\n                    }\n                    if (aggregated === undefined) {\n                        aggregated = chunk;\n                    }\n                    else {\n                        aggregated = concat(aggregated, chunk);\n                    }\n                    if (isAIMessageChunk(chunk.message) &&\n                        chunk.message.usage_metadata !== undefined) {\n                        llmOutput = {\n                            tokenUsage: {\n                                promptTokens: chunk.message.usage_metadata.input_tokens,\n                                completionTokens: chunk.message.usage_metadata.output_tokens,\n                                totalTokens: chunk.message.usage_metadata.total_tokens,\n                            },\n                        };\n                    }\n                }\n                if (aggregated === undefined) {\n                    throw new Error(\"Received empty response from chat model call.\");\n                }\n                generations.push([aggregated]);\n                await runManagers?.[0].handleLLMEnd({\n                    generations,\n                    llmOutput,\n                });\n            }\n            catch (e) {\n                await runManagers?.[0].handleLLMError(e);\n                throw e;\n            }\n        }\n        else {\n            // generate results\n            const results = await Promise.allSettled(baseMessages.map((messageList, i) => this._generate(messageList, { ...parsedOptions, promptIndex: i }, runManagers?.[i])));\n            // handle results\n            await Promise.all(results.map(async (pResult, i) => {\n                if (pResult.status === \"fulfilled\") {\n                    const result = pResult.value;\n                    for (const generation of result.generations) {\n                        if (generation.message.id == null) {\n                            const runId = runManagers?.at(0)?.runId;\n                            if (runId != null)\n                                generation.message._updateId(`run-${runId}`);\n                        }\n                        generation.message.response_metadata = {\n                            ...generation.generationInfo,\n                            ...generation.message.response_metadata,\n                        };\n                    }\n                    if (result.generations.length === 1) {\n                        result.generations[0].message.response_metadata = {\n                            ...result.llmOutput,\n                            ...result.generations[0].message.response_metadata,\n                        };\n                    }\n                    generations[i] = result.generations;\n                    llmOutputs[i] = result.llmOutput;\n                    return runManagers?.[i]?.handleLLMEnd({\n                        generations: [result.generations],\n                        llmOutput: result.llmOutput,\n                    });\n                }\n                else {\n                    // status === \"rejected\"\n                    await runManagers?.[i]?.handleLLMError(pResult.reason);\n                    return Promise.reject(pResult.reason);\n                }\n            }));\n        }\n        // create combined output\n        const output = {\n            generations,\n            llmOutput: llmOutputs.length\n                ? this._combineLLMOutput?.(...llmOutputs)\n                : undefined,\n        };\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    async _generateCached({ messages, cache, llmStringKey, parsedOptions, handledOptions, }) {\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        const inheritableMetadata = {\n            ...handledOptions.metadata,\n            ...this.getLsParams(parsedOptions),\n        };\n        // create callback manager and start run\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, inheritableMetadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: 1,\n        };\n        const runManagers = await callbackManager_?.handleChatModelStart(this.toJSON(), baseMessages, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions.runName);\n        // generate results\n        const missingPromptIndices = [];\n        const results = await Promise.allSettled(baseMessages.map(async (baseMessage, index) => {\n            // Join all content into one string for the prompt index\n            const prompt = BaseChatModel._convertInputToPromptValue(baseMessage).toString();\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (result == null) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        // Map run managers to the results before filtering out null results\n        // Null results are just absent from the cache.\n        const cachedResults = results\n            .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n            .filter(({ result }) => (result.status === \"fulfilled\" && result.value != null) ||\n            result.status === \"rejected\");\n        // Handle results and call run managers\n        const generations = [];\n        await Promise.all(cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n            if (promiseResult.status === \"fulfilled\") {\n                const result = promiseResult.value;\n                generations[i] = result.map((result) => {\n                    if (\"message\" in result &&\n                        isBaseMessage(result.message) &&\n                        isAIMessage(result.message)) {\n                        // eslint-disable-next-line no-param-reassign\n                        result.message.usage_metadata = {\n                            input_tokens: 0,\n                            output_tokens: 0,\n                            total_tokens: 0,\n                        };\n                    }\n                    // eslint-disable-next-line no-param-reassign\n                    result.generationInfo = {\n                        ...result.generationInfo,\n                        tokenUsage: {},\n                    };\n                    return result;\n                });\n                if (result.length) {\n                    await runManager?.handleLLMNewToken(result[0].text);\n                }\n                return runManager?.handleLLMEnd({\n                    generations: [result],\n                }, undefined, undefined, undefined, {\n                    cached: true,\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManager?.handleLLMError(promiseResult.reason, undefined, undefined, undefined, {\n                    cached: true,\n                });\n                return Promise.reject(promiseResult.reason);\n            }\n        }));\n        const output = {\n            generations,\n            missingPromptIndices,\n            startedRunManagers: runManagers,\n        };\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Generates chat based on the input messages.\n     * @param messages An array of arrays of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generate(messages, options, callbacks) {\n        // parse call options\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const baseMessages = messages.map((messageList) => messageList.map(coerceMessageLikeToMessage));\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(baseMessages, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n        const { generations, missingPromptIndices, startedRunManagers } = await this._generateCached({\n            messages: baseMessages,\n            cache,\n            llmStringKey,\n            parsedOptions: callOptions,\n            handledOptions: runnableConfig,\n        });\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => baseMessages[i]), callOptions, runnableConfig, startedRunManagers !== undefined\n                ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n                : undefined);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                // Join all content into one string for the prompt index\n                const prompt = BaseChatModel._convertInputToPromptValue(baseMessages[promptIndex]).toString();\n                return cache.update(prompt, llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _modelType() {\n        return \"base_chat_model\";\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this.invocationParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    /**\n     * Generates a prompt based on the input prompt values.\n     * @param promptValues An array of BasePromptValue instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to an LLMResult.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const promptMessages = promptValues.map((promptValue) => promptValue.toChatMessages());\n        return this.generate(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async call(messages, options, callbacks) {\n        const result = await this.generate([messages.map(coerceMessageLikeToMessage)], options, callbacks);\n        const generations = result.generations;\n        return generations[0][0].message;\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Makes a single call to the chat model with a prompt value.\n     * @param promptValue The value of the prompt.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async callPrompt(promptValue, options, callbacks) {\n        const promptMessages = promptValue.toChatMessages();\n        return this.call(promptMessages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on the input messages.\n     * @param messages An array of BaseMessage instances.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a BaseMessage.\n     */\n    async predictMessages(messages, options, callbacks) {\n        return this.call(messages, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * Predicts the next message based on a text input.\n     * @param text The text input.\n     * @param options The call options or an array of stop sequences.\n     * @param callbacks The callbacks for the language model.\n     * @returns A Promise that resolves to a string.\n     */\n    async predict(text, options, callbacks) {\n        const message = new HumanMessage(text);\n        const result = await this.call([message], options, callbacks);\n        if (typeof result.content !== \"string\") {\n            throw new Error(\"Cannot use predict when output is not a string.\");\n        }\n        return result.content;\n    }\n    withStructuredOutput(outputSchema, config) {\n        if (typeof this.bindTools !== \"function\") {\n            throw new Error(`Chat model must implement \".bindTools()\" to use withStructuredOutput.`);\n        }\n        if (config?.strict) {\n            throw new Error(`\"strict\" mode is not supported for this model by default.`);\n        }\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        const schema = outputSchema;\n        const name = config?.name;\n        const description = schema.description ?? \"A function available to call.\";\n        const method = config?.method;\n        const includeRaw = config?.includeRaw;\n        if (method === \"jsonMode\") {\n            throw new Error(`Base withStructuredOutput implementation only supports \"functionCalling\" as a method.`);\n        }\n        let functionName = name ?? \"extract\";\n        let tools;\n        if (isZodSchema(schema)) {\n            tools = [\n                {\n                    type: \"function\",\n                    function: {\n                        name: functionName,\n                        description,\n                        parameters: zodToJsonSchema(schema),\n                    },\n                },\n            ];\n        }\n        else {\n            if (\"name\" in schema) {\n                functionName = schema.name;\n            }\n            tools = [\n                {\n                    type: \"function\",\n                    function: {\n                        name: functionName,\n                        description,\n                        parameters: schema,\n                    },\n                },\n            ];\n        }\n        const llm = this.bindTools(tools);\n        const outputParser = RunnableLambda.from((input) => {\n            if (!input.tool_calls || input.tool_calls.length === 0) {\n                throw new Error(\"No tool calls found in the response.\");\n            }\n            const toolCall = input.tool_calls.find((tc) => tc.name === functionName);\n            if (!toolCall) {\n                throw new Error(`No tool call found with name ${functionName}.`);\n            }\n            return toolCall.args;\n        });\n        if (!includeRaw) {\n            return llm.pipe(outputParser).withConfig({\n                runName: \"StructuredOutput\",\n            });\n        }\n        const parserAssign = RunnablePassthrough.assign({\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            parsed: (input, config) => outputParser.invoke(input.raw, config),\n        });\n        const parserNone = RunnablePassthrough.assign({\n            parsed: () => null,\n        });\n        const parsedWithFallback = parserAssign.withFallbacks({\n            fallbacks: [parserNone],\n        });\n        return RunnableSequence.from([\n            {\n                raw: llm,\n            },\n            parsedWithFallback,\n        ]).withConfig({\n            runName: \"StructuredOutputRunnable\",\n        });\n    }\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\nexport class SimpleChatModel extends BaseChatModel {\n    async _generate(messages, options, runManager) {\n        const text = await this._call(messages, options, runManager);\n        const message = new AIMessage(text);\n        if (typeof message.content !== \"string\") {\n            throw new Error(\"Cannot generate with a simple chat model when output is not a string.\");\n        }\n        return {\n            generations: [\n                {\n                    text: message.content,\n                    message,\n                },\n            ],\n        };\n    }\n}\n"],"mappings":"AAAA,SAASA,eAAe,QAAQ,oBAAoB;AACpD,SAASC,SAAS,EAAEC,YAAY,EAAEC,0BAA0B,EAAEC,gBAAgB,EAAEC,aAAa,EAAEC,WAAW,QAAS,sBAAsB;AACzI,SAASC,OAAO,QAAS,eAAe;AACxC,SAASC,iBAAiB,QAAS,WAAW;AAC9C,SAASC,eAAe,QAAS,yBAAyB;AAC1D,SAASC,cAAc,EAAEC,gBAAgB,QAAS,sBAAsB;AACxE,SAASC,MAAM,QAAQ,oBAAoB;AAC3C,SAASC,mBAAmB,QAAQ,6BAA6B;AACjE,SAASC,WAAW,QAAQ,iCAAiC;AAC7D,SAASC,+BAA+B,QAAQ,sBAAsB;AACtE;AACA;AACA;AACA;AACA;AACA,OAAO,SAASC,mCAAmCA,CAAA,EAAG;EAClD,MAAMC,WAAW,GAAG,IAAIC,WAAW,CAAC,CAAC;EACrC,OAAO,IAAIC,eAAe,CAAC;IACvBC,SAASA,CAACC,KAAK,EAAEC,UAAU,EAAE;MACzBA,UAAU,CAACC,OAAO,CAACN,WAAW,CAACO,MAAM,CAAC,OAAOH,KAAK,CAACI,OAAO,KAAK,QAAQ,GACjEJ,KAAK,CAACI,OAAO,GACbC,IAAI,CAACC,SAAS,CAACN,KAAK,CAACI,OAAO,CAAC,CAAC,CAAC;IACzC;EACJ,CAAC,CAAC;AACN;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,aAAa,SAASpB,iBAAiB,CAAC;EACjDqB,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;IACAC,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,aAAa,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IACvD,CAAC,CAAC;IACFN,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;EACN;EACAE,4CAA4CA,CAACC,OAAO,EAAE;IAClD;IACA,MAAM,CAACC,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACC,sCAAsC,CAACH,OAAO,CAAC;IAC3FE,WAAW,CAACE,MAAM,GAAGH,cAAc,CAACG,MAAM;IAC1C,OAAO,CAACH,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA;AACJ;AACA;AACA;AACA;AACA;EACI,MAAMG,MAAMA,CAACC,KAAK,EAAEN,OAAO,EAAE;IACzB,MAAMO,WAAW,GAAGlB,aAAa,CAACmB,0BAA0B,CAACF,KAAK,CAAC;IACnE,MAAMG,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAEP,OAAO,EAAEA,OAAO,EAAEW,SAAS,CAAC;IACpF,MAAMC,cAAc,GAAGH,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C;IACA,OAAOD,cAAc,CAACE,OAAO;EACjC;EACA;EACA,OAAOC,qBAAqBA,CAACC,SAAS,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IAC3D,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACA,OAAOC,eAAeA,CAACd,KAAK,EAAEN,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACe,qBAAqB,KAC1B1B,aAAa,CAACgC,SAAS,CAACN,qBAAqB,IAC7C,IAAI,CAACO,gBAAgB,EAAE;MACvB,MAAM,IAAI,CAACjB,MAAM,CAACC,KAAK,EAAEN,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMuB,MAAM,GAAGlC,aAAa,CAACmB,0BAA0B,CAACF,KAAK,CAAC;MAC9D,MAAMkB,QAAQ,GAAGD,MAAM,CAACE,cAAc,CAAC,CAAC;MACxC,MAAM,CAACxB,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,4CAA4C,CAACC,OAAO,CAAC;MAChG,MAAM0B,mBAAmB,GAAG;QACxB,GAAGzB,cAAc,CAAC0B,QAAQ;QAC1B,GAAG,IAAI,CAACC,WAAW,CAAC1B,WAAW;MACnC,CAAC;MACD,MAAM2B,gBAAgB,GAAG,MAAM3D,eAAe,CAAC4D,SAAS,CAAC7B,cAAc,CAACU,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEV,cAAc,CAAC8B,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEL,mBAAmB,EAAE,IAAI,CAACC,QAAQ,EAAE;QAAEK,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACjM,MAAMC,KAAK,GAAG;QACVjC,OAAO,EAAEE,WAAW;QACpBgC,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACjC,WAAW,CAAC;QACtDkC,UAAU,EAAE;MAChB,CAAC;MACD,MAAMC,WAAW,GAAG,MAAMR,gBAAgB,EAAES,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACf,QAAQ,CAAC,EAAEvB,cAAc,CAACuC,KAAK,EAAEC,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAExC,cAAc,CAACyC,OAAO,CAAC;MACjL,IAAIC,eAAe;MACnB;MACA,IAAIC,SAAS;MACb,IAAI;QACA,WAAW,MAAM9D,KAAK,IAAI,IAAI,CAACiC,qBAAqB,CAACS,QAAQ,EAAEtB,WAAW,EAAEmC,WAAW,GAAG,CAAC,CAAC,CAAC,EAAE;UAC3F,IAAIvD,KAAK,CAACgC,OAAO,CAAC+B,EAAE,IAAI,IAAI,EAAE;YAC1B,MAAML,KAAK,GAAGH,WAAW,EAAES,EAAE,CAAC,CAAC,CAAC,EAAEN,KAAK;YACvC,IAAIA,KAAK,IAAI,IAAI,EACb1D,KAAK,CAACgC,OAAO,CAACiC,SAAS,CAAC,OAAOP,KAAK,EAAE,CAAC;UAC/C;UACA1D,KAAK,CAACgC,OAAO,CAACkC,iBAAiB,GAAG;YAC9B,GAAGlE,KAAK,CAACmE,cAAc;YACvB,GAAGnE,KAAK,CAACgC,OAAO,CAACkC;UACrB,CAAC;UACD,MAAMlE,KAAK,CAACgC,OAAO;UACnB,IAAI,CAAC6B,eAAe,EAAE;YAClBA,eAAe,GAAG7D,KAAK;UAC3B,CAAC,MACI;YACD6D,eAAe,GAAGA,eAAe,CAACtE,MAAM,CAACS,KAAK,CAAC;UACnD;UACA,IAAIjB,gBAAgB,CAACiB,KAAK,CAACgC,OAAO,CAAC,IAC/BhC,KAAK,CAACgC,OAAO,CAACoC,cAAc,KAAKT,SAAS,EAAE;YAC5CG,SAAS,GAAG;cACRO,UAAU,EAAE;gBACRC,YAAY,EAAEtE,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACG,YAAY;gBACvDC,gBAAgB,EAAExE,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACK,aAAa;gBAC5DC,WAAW,EAAE1E,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACO;cAC9C;YACJ,CAAC;UACL;QACJ;MACJ,CAAC,CACD,OAAOC,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACvB,WAAW,IAAI,EAAE,EAAEwB,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACvB,WAAW,IAAI,EAAE,EAAEwB,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEE,YAAY,CAAC;QAC/E;QACAnD,WAAW,EAAE,CAAC,CAAC8B,eAAe,CAAC,CAAC;QAChCC;MACJ,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACAhB,WAAWA,CAAC5B,OAAO,EAAE;IACjB,MAAMiE,YAAY,GAAG,IAAI,CAACC,OAAO,CAAC,CAAC,CAACC,UAAU,CAAC,MAAM,CAAC,GAChD,IAAI,CAACD,OAAO,CAAC,CAAC,CAACE,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,GAClC,IAAI,CAACF,OAAO,CAAC,CAAC;IACpB,OAAO;MACHG,aAAa,EAAE,MAAM;MACrBC,OAAO,EAAEtE,OAAO,CAACuE,IAAI;MACrBC,WAAW,EAAEP;IACjB,CAAC;EACL;EACA;EACA,MAAMQ,iBAAiBA,CAACjD,QAAQ,EAAEkD,aAAa,EAAEC,cAAc,EAAEC,kBAAkB,EAAE;IACjF,MAAMC,YAAY,GAAGrD,QAAQ,CAACqC,GAAG,CAAEiB,WAAW,IAAKA,WAAW,CAACjB,GAAG,CAACjG,0BAA0B,CAAC,CAAC;IAC/F,IAAIyE,WAAW;IACf,IAAIuC,kBAAkB,KAAKnC,SAAS,IAChCmC,kBAAkB,CAACG,MAAM,KAAKF,YAAY,CAACE,MAAM,EAAE;MACnD1C,WAAW,GAAGuC,kBAAkB;IACpC,CAAC,MACI;MACD,MAAMlD,mBAAmB,GAAG;QACxB,GAAGiD,cAAc,CAAChD,QAAQ;QAC1B,GAAG,IAAI,CAACC,WAAW,CAAC8C,aAAa;MACrC,CAAC;MACD;MACA,MAAM7C,gBAAgB,GAAG,MAAM3D,eAAe,CAAC4D,SAAS,CAAC6C,cAAc,CAAChE,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEgE,cAAc,CAAC5C,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEL,mBAAmB,EAAE,IAAI,CAACC,QAAQ,EAAE;QAAEK,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACjM,MAAMC,KAAK,GAAG;QACVjC,OAAO,EAAE0E,aAAa;QACtBxC,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACuC,aAAa,CAAC;QACxDtC,UAAU,EAAE;MAChB,CAAC;MACDC,WAAW,GAAG,MAAMR,gBAAgB,EAAES,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEsC,YAAY,EAAEF,cAAc,CAACnC,KAAK,EAAEC,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAEkC,cAAc,CAACjC,OAAO,CAAC;IACjL;IACA,MAAM7B,WAAW,GAAG,EAAE;IACtB,MAAMmE,UAAU,GAAG,EAAE;IACrB;IACA;IACA;IACA,MAAMC,mBAAmB,GAAG,CAAC,CAAC5C,WAAW,GAAG,CAAC,CAAC,CAAC6C,QAAQ,CAACC,IAAI,CAAC3G,+BAA+B,CAAC;IAC7F,IAAIyG,mBAAmB,IACnB,CAAC,IAAI,CAAC3D,gBAAgB,IACtBuD,YAAY,CAACE,MAAM,KAAK,CAAC,IACzB,IAAI,CAAChE,qBAAqB,KACtB1B,aAAa,CAACgC,SAAS,CAACN,qBAAqB,EAAE;MACnD,IAAI;QACA,MAAMqE,MAAM,GAAG,MAAM,IAAI,CAACrE,qBAAqB,CAAC8D,YAAY,CAAC,CAAC,CAAC,EAAEH,aAAa,EAAErC,WAAW,GAAG,CAAC,CAAC,CAAC;QACjG,IAAIgD,UAAU;QACd;QACA,IAAIzC,SAAS;QACb,WAAW,MAAM9D,KAAK,IAAIsG,MAAM,EAAE;UAC9B,IAAItG,KAAK,CAACgC,OAAO,CAAC+B,EAAE,IAAI,IAAI,EAAE;YAC1B,MAAML,KAAK,GAAGH,WAAW,EAAES,EAAE,CAAC,CAAC,CAAC,EAAEN,KAAK;YACvC,IAAIA,KAAK,IAAI,IAAI,EACb1D,KAAK,CAACgC,OAAO,CAACiC,SAAS,CAAC,OAAOP,KAAK,EAAE,CAAC;UAC/C;UACA,IAAI6C,UAAU,KAAK5C,SAAS,EAAE;YAC1B4C,UAAU,GAAGvG,KAAK;UACtB,CAAC,MACI;YACDuG,UAAU,GAAGhH,MAAM,CAACgH,UAAU,EAAEvG,KAAK,CAAC;UAC1C;UACA,IAAIjB,gBAAgB,CAACiB,KAAK,CAACgC,OAAO,CAAC,IAC/BhC,KAAK,CAACgC,OAAO,CAACoC,cAAc,KAAKT,SAAS,EAAE;YAC5CG,SAAS,GAAG;cACRO,UAAU,EAAE;gBACRC,YAAY,EAAEtE,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACG,YAAY;gBACvDC,gBAAgB,EAAExE,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACK,aAAa;gBAC5DC,WAAW,EAAE1E,KAAK,CAACgC,OAAO,CAACoC,cAAc,CAACO;cAC9C;YACJ,CAAC;UACL;QACJ;QACA,IAAI4B,UAAU,KAAK5C,SAAS,EAAE;UAC1B,MAAM,IAAItB,KAAK,CAAC,+CAA+C,CAAC;QACpE;QACAN,WAAW,CAACyE,IAAI,CAAC,CAACD,UAAU,CAAC,CAAC;QAC9B,MAAMhD,WAAW,GAAG,CAAC,CAAC,CAAC2B,YAAY,CAAC;UAChCnD,WAAW;UACX+B;QACJ,CAAC,CAAC;MACN,CAAC,CACD,OAAO2C,CAAC,EAAE;QACN,MAAMlD,WAAW,GAAG,CAAC,CAAC,CAAC0B,cAAc,CAACwB,CAAC,CAAC;QACxC,MAAMA,CAAC;MACX;IACJ,CAAC,MACI;MACD;MACA,MAAMC,OAAO,GAAG,MAAM7B,OAAO,CAAC8B,UAAU,CAACZ,YAAY,CAAChB,GAAG,CAAC,CAACiB,WAAW,EAAEY,CAAC,KAAK,IAAI,CAACC,SAAS,CAACb,WAAW,EAAE;QAAE,GAAGJ,aAAa;QAAEkB,WAAW,EAAEF;MAAE,CAAC,EAAErD,WAAW,GAAGqD,CAAC,CAAC,CAAC,CAAC,CAAC;MACnK;MACA,MAAM/B,OAAO,CAACC,GAAG,CAAC4B,OAAO,CAAC3B,GAAG,CAAC,OAAOgC,OAAO,EAAEH,CAAC,KAAK;QAChD,IAAIG,OAAO,CAACC,MAAM,KAAK,WAAW,EAAE;UAChC,MAAMrF,MAAM,GAAGoF,OAAO,CAAChG,KAAK;UAC5B,KAAK,MAAMkG,UAAU,IAAItF,MAAM,CAACI,WAAW,EAAE;YACzC,IAAIkF,UAAU,CAACjF,OAAO,CAAC+B,EAAE,IAAI,IAAI,EAAE;cAC/B,MAAML,KAAK,GAAGH,WAAW,EAAES,EAAE,CAAC,CAAC,CAAC,EAAEN,KAAK;cACvC,IAAIA,KAAK,IAAI,IAAI,EACbuD,UAAU,CAACjF,OAAO,CAACiC,SAAS,CAAC,OAAOP,KAAK,EAAE,CAAC;YACpD;YACAuD,UAAU,CAACjF,OAAO,CAACkC,iBAAiB,GAAG;cACnC,GAAG+C,UAAU,CAAC9C,cAAc;cAC5B,GAAG8C,UAAU,CAACjF,OAAO,CAACkC;YAC1B,CAAC;UACL;UACA,IAAIvC,MAAM,CAACI,WAAW,CAACkE,MAAM,KAAK,CAAC,EAAE;YACjCtE,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAACC,OAAO,CAACkC,iBAAiB,GAAG;cAC9C,GAAGvC,MAAM,CAACmC,SAAS;cACnB,GAAGnC,MAAM,CAACI,WAAW,CAAC,CAAC,CAAC,CAACC,OAAO,CAACkC;YACrC,CAAC;UACL;UACAnC,WAAW,CAAC6E,CAAC,CAAC,GAAGjF,MAAM,CAACI,WAAW;UACnCmE,UAAU,CAACU,CAAC,CAAC,GAAGjF,MAAM,CAACmC,SAAS;UAChC,OAAOP,WAAW,GAAGqD,CAAC,CAAC,EAAE1B,YAAY,CAAC;YAClCnD,WAAW,EAAE,CAACJ,MAAM,CAACI,WAAW,CAAC;YACjC+B,SAAS,EAAEnC,MAAM,CAACmC;UACtB,CAAC,CAAC;QACN,CAAC,MACI;UACD;UACA,MAAMP,WAAW,GAAGqD,CAAC,CAAC,EAAE3B,cAAc,CAAC8B,OAAO,CAACG,MAAM,CAAC;UACtD,OAAOrC,OAAO,CAACsC,MAAM,CAACJ,OAAO,CAACG,MAAM,CAAC;QACzC;MACJ,CAAC,CAAC,CAAC;IACP;IACA;IACA,MAAME,MAAM,GAAG;MACXrF,WAAW;MACX+B,SAAS,EAAEoC,UAAU,CAACD,MAAM,GACtB,IAAI,CAACoB,iBAAiB,GAAG,GAAGnB,UAAU,CAAC,GACvCvC;IACV,CAAC;IACDjD,MAAM,CAACC,cAAc,CAACyG,MAAM,EAAElI,OAAO,EAAE;MACnC6B,KAAK,EAAEwC,WAAW,GACZ;QAAE+D,MAAM,EAAE/D,WAAW,EAAEwB,GAAG,CAAEwC,OAAO,IAAKA,OAAO,CAAC7D,KAAK;MAAE,CAAC,GACxDC,SAAS;MACf9C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOuG,MAAM;EACjB;EACA,MAAMI,eAAeA,CAAC;IAAE9E,QAAQ;IAAE+E,KAAK;IAAEC,YAAY;IAAE9B,aAAa;IAAEC;EAAgB,CAAC,EAAE;IACrF,MAAME,YAAY,GAAGrD,QAAQ,CAACqC,GAAG,CAAEiB,WAAW,IAAKA,WAAW,CAACjB,GAAG,CAACjG,0BAA0B,CAAC,CAAC;IAC/F,MAAM8D,mBAAmB,GAAG;MACxB,GAAGiD,cAAc,CAAChD,QAAQ;MAC1B,GAAG,IAAI,CAACC,WAAW,CAAC8C,aAAa;IACrC,CAAC;IACD;IACA,MAAM7C,gBAAgB,GAAG,MAAM3D,eAAe,CAAC4D,SAAS,CAAC6C,cAAc,CAAChE,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEgE,cAAc,CAAC5C,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEL,mBAAmB,EAAE,IAAI,CAACC,QAAQ,EAAE;MAAEK,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACjM,MAAMC,KAAK,GAAG;MACVjC,OAAO,EAAE0E,aAAa;MACtBxC,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACuC,aAAa,CAAC;MACxDtC,UAAU,EAAE;IAChB,CAAC;IACD,MAAMC,WAAW,GAAG,MAAMR,gBAAgB,EAAES,oBAAoB,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEsC,YAAY,EAAEF,cAAc,CAACnC,KAAK,EAAEC,SAAS,EAAER,KAAK,EAAEQ,SAAS,EAAEA,SAAS,EAAEkC,cAAc,CAACjC,OAAO,CAAC;IACnL;IACA,MAAM+D,oBAAoB,GAAG,EAAE;IAC/B,MAAMjB,OAAO,GAAG,MAAM7B,OAAO,CAAC8B,UAAU,CAACZ,YAAY,CAAChB,GAAG,CAAC,OAAO6C,WAAW,EAAEC,KAAK,KAAK;MACpF;MACA,MAAMpF,MAAM,GAAGlC,aAAa,CAACmB,0BAA0B,CAACkG,WAAW,CAAC,CAACE,QAAQ,CAAC,CAAC;MAC/E,MAAMnG,MAAM,GAAG,MAAM8F,KAAK,CAACM,MAAM,CAACtF,MAAM,EAAEiF,YAAY,CAAC;MACvD,IAAI/F,MAAM,IAAI,IAAI,EAAE;QAChBgG,oBAAoB,CAACnB,IAAI,CAACqB,KAAK,CAAC;MACpC;MACA,OAAOlG,MAAM;IACjB,CAAC,CAAC,CAAC;IACH;IACA;IACA,MAAMqG,aAAa,GAAGtB,OAAO,CACxB3B,GAAG,CAAC,CAACpD,MAAM,EAAEkG,KAAK,MAAM;MAAElG,MAAM;MAAEqD,UAAU,EAAEzB,WAAW,GAAGsE,KAAK;IAAE,CAAC,CAAC,CAAC,CACtEI,MAAM,CAAC,CAAC;MAAEtG;IAAO,CAAC,KAAMA,MAAM,CAACqF,MAAM,KAAK,WAAW,IAAIrF,MAAM,CAACZ,KAAK,IAAI,IAAI,IAC9EY,MAAM,CAACqF,MAAM,KAAK,UAAU,CAAC;IACjC;IACA,MAAMjF,WAAW,GAAG,EAAE;IACtB,MAAM8C,OAAO,CAACC,GAAG,CAACkD,aAAa,CAACjD,GAAG,CAAC,OAAO;MAAEpD,MAAM,EAAEuG,aAAa;MAAElD;IAAW,CAAC,EAAE4B,CAAC,KAAK;MACpF,IAAIsB,aAAa,CAAClB,MAAM,KAAK,WAAW,EAAE;QACtC,MAAMrF,MAAM,GAAGuG,aAAa,CAACnH,KAAK;QAClCgB,WAAW,CAAC6E,CAAC,CAAC,GAAGjF,MAAM,CAACoD,GAAG,CAAEpD,MAAM,IAAK;UACpC,IAAI,SAAS,IAAIA,MAAM,IACnB3C,aAAa,CAAC2C,MAAM,CAACK,OAAO,CAAC,IAC7B/C,WAAW,CAAC0C,MAAM,CAACK,OAAO,CAAC,EAAE;YAC7B;YACAL,MAAM,CAACK,OAAO,CAACoC,cAAc,GAAG;cAC5BG,YAAY,EAAE,CAAC;cACfE,aAAa,EAAE,CAAC;cAChBE,YAAY,EAAE;YAClB,CAAC;UACL;UACA;UACAhD,MAAM,CAACwC,cAAc,GAAG;YACpB,GAAGxC,MAAM,CAACwC,cAAc;YACxBE,UAAU,EAAE,CAAC;UACjB,CAAC;UACD,OAAO1C,MAAM;QACjB,CAAC,CAAC;QACF,IAAIA,MAAM,CAACsE,MAAM,EAAE;UACf,MAAMjB,UAAU,EAAEmD,iBAAiB,CAACxG,MAAM,CAAC,CAAC,CAAC,CAACyG,IAAI,CAAC;QACvD;QACA,OAAOpD,UAAU,EAAEE,YAAY,CAAC;UAC5BnD,WAAW,EAAE,CAACJ,MAAM;QACxB,CAAC,EAAEgC,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;UAChC0E,MAAM,EAAE;QACZ,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,MAAMrD,UAAU,EAAEC,cAAc,CAACiD,aAAa,CAAChB,MAAM,EAAEvD,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;UACpF0E,MAAM,EAAE;QACZ,CAAC,CAAC;QACF,OAAOxD,OAAO,CAACsC,MAAM,CAACe,aAAa,CAAChB,MAAM,CAAC;MAC/C;IACJ,CAAC,CAAC,CAAC;IACH,MAAME,MAAM,GAAG;MACXrF,WAAW;MACX4F,oBAAoB;MACpB7B,kBAAkB,EAAEvC;IACxB,CAAC;IACD;IACA;IACA;IACA7C,MAAM,CAACC,cAAc,CAACyG,MAAM,EAAElI,OAAO,EAAE;MACnC6B,KAAK,EAAEwC,WAAW,GACZ;QAAE+D,MAAM,EAAE/D,WAAW,EAAEwB,GAAG,CAAEwC,OAAO,IAAKA,OAAO,CAAC7D,KAAK;MAAE,CAAC,GACxDC,SAAS;MACf9C,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOuG,MAAM;EACjB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAMkB,QAAQA,CAAC5F,QAAQ,EAAExB,OAAO,EAAEW,SAAS,EAAE;IACzC;IACA,IAAI+D,aAAa;IACjB,IAAI2C,KAAK,CAACC,OAAO,CAACtH,OAAO,CAAC,EAAE;MACxB0E,aAAa,GAAG;QAAEH,IAAI,EAAEvE;MAAQ,CAAC;IACrC,CAAC,MACI;MACD0E,aAAa,GAAG1E,OAAO;IAC3B;IACA,MAAM6E,YAAY,GAAGrD,QAAQ,CAACqC,GAAG,CAAEiB,WAAW,IAAKA,WAAW,CAACjB,GAAG,CAACjG,0BAA0B,CAAC,CAAC;IAC/F,MAAM,CAACqC,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACH,4CAA4C,CAAC2E,aAAa,CAAC;IACtGzE,cAAc,CAACU,SAAS,GAAGV,cAAc,CAACU,SAAS,IAAIA,SAAS;IAChE,IAAI,CAAC,IAAI,CAAC4F,KAAK,EAAE;MACb,OAAO,IAAI,CAAC9B,iBAAiB,CAACI,YAAY,EAAE3E,WAAW,EAAED,cAAc,CAAC;IAC5E;IACA,MAAM;MAAEsG;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMC,YAAY,GAAG,IAAI,CAACe,uCAAuC,CAACrH,WAAW,CAAC;IAC9E,MAAM;MAAEW,WAAW;MAAE4F,oBAAoB;MAAE7B;IAAmB,CAAC,GAAG,MAAM,IAAI,CAAC0B,eAAe,CAAC;MACzF9E,QAAQ,EAAEqD,YAAY;MACtB0B,KAAK;MACLC,YAAY;MACZ9B,aAAa,EAAExE,WAAW;MAC1ByE,cAAc,EAAE1E;IACpB,CAAC,CAAC;IACF,IAAI2C,SAAS,GAAG,CAAC,CAAC;IAClB,IAAI6D,oBAAoB,CAAC1B,MAAM,GAAG,CAAC,EAAE;MACjC,MAAMS,OAAO,GAAG,MAAM,IAAI,CAACf,iBAAiB,CAACgC,oBAAoB,CAAC5C,GAAG,CAAE6B,CAAC,IAAKb,YAAY,CAACa,CAAC,CAAC,CAAC,EAAExF,WAAW,EAAED,cAAc,EAAE2E,kBAAkB,KAAKnC,SAAS,GACtJgE,oBAAoB,CAAC5C,GAAG,CAAE6B,CAAC,IAAKd,kBAAkB,GAAGc,CAAC,CAAC,CAAC,GACxDjD,SAAS,CAAC;MAChB,MAAMkB,OAAO,CAACC,GAAG,CAAC4B,OAAO,CAAC3E,WAAW,CAACgD,GAAG,CAAC,OAAOkC,UAAU,EAAEY,KAAK,KAAK;QACnE,MAAMf,WAAW,GAAGa,oBAAoB,CAACE,KAAK,CAAC;QAC/C9F,WAAW,CAAC+E,WAAW,CAAC,GAAGG,UAAU;QACrC;QACA,MAAMxE,MAAM,GAAGlC,aAAa,CAACmB,0BAA0B,CAACqE,YAAY,CAACe,WAAW,CAAC,CAAC,CAACgB,QAAQ,CAAC,CAAC;QAC7F,OAAOL,KAAK,CAACiB,MAAM,CAACjG,MAAM,EAAEiF,YAAY,EAAET,UAAU,CAAC;MACzD,CAAC,CAAC,CAAC;MACHnD,SAAS,GAAG4C,OAAO,CAAC5C,SAAS,IAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAE/B,WAAW;MAAE+B;IAAU,CAAC;EACrC;EACA;AACJ;AACA;EACI;EACAT,gBAAgBA,CAAClB,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAwG,UAAUA,CAAA,EAAG;IACT,OAAO,iBAAiB;EAC5B;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACvF,gBAAgB,CAAC,CAAC;MAC1BwF,KAAK,EAAE,IAAI,CAAC7H,QAAQ,CAAC,CAAC;MACtB8H,MAAM,EAAE,IAAI,CAACH,UAAU,CAAC;IAC5B,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,MAAM/G,cAAcA,CAACmH,YAAY,EAAE7H,OAAO,EAAEW,SAAS,EAAE;IACnD,MAAMmH,cAAc,GAAGD,YAAY,CAAChE,GAAG,CAAEtD,WAAW,IAAKA,WAAW,CAACkB,cAAc,CAAC,CAAC,CAAC;IACtF,OAAO,IAAI,CAAC2F,QAAQ,CAACU,cAAc,EAAE9H,OAAO,EAAEW,SAAS,CAAC;EAC5D;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMoH,IAAIA,CAACvG,QAAQ,EAAExB,OAAO,EAAEW,SAAS,EAAE;IACrC,MAAMF,MAAM,GAAG,MAAM,IAAI,CAAC2G,QAAQ,CAAC,CAAC5F,QAAQ,CAACqC,GAAG,CAACjG,0BAA0B,CAAC,CAAC,EAAEoC,OAAO,EAAEW,SAAS,CAAC;IAClG,MAAME,WAAW,GAAGJ,MAAM,CAACI,WAAW;IACtC,OAAOA,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,OAAO;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMkH,UAAUA,CAACzH,WAAW,EAAEP,OAAO,EAAEW,SAAS,EAAE;IAC9C,MAAMmH,cAAc,GAAGvH,WAAW,CAACkB,cAAc,CAAC,CAAC;IACnD,OAAO,IAAI,CAACsG,IAAI,CAACD,cAAc,EAAE9H,OAAO,EAAEW,SAAS,CAAC;EACxD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMsH,eAAeA,CAACzG,QAAQ,EAAExB,OAAO,EAAEW,SAAS,EAAE;IAChD,OAAO,IAAI,CAACoH,IAAI,CAACvG,QAAQ,EAAExB,OAAO,EAAEW,SAAS,CAAC;EAClD;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMuH,OAAOA,CAAChB,IAAI,EAAElH,OAAO,EAAEW,SAAS,EAAE;IACpC,MAAMG,OAAO,GAAG,IAAInD,YAAY,CAACuJ,IAAI,CAAC;IACtC,MAAMzG,MAAM,GAAG,MAAM,IAAI,CAACsH,IAAI,CAAC,CAACjH,OAAO,CAAC,EAAEd,OAAO,EAAEW,SAAS,CAAC;IAC7D,IAAI,OAAOF,MAAM,CAACvB,OAAO,KAAK,QAAQ,EAAE;MACpC,MAAM,IAAIiC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,OAAOV,MAAM,CAACvB,OAAO;EACzB;EACAiJ,oBAAoBA,CAACC,YAAY,EAAEC,MAAM,EAAE;IACvC,IAAI,OAAO,IAAI,CAACC,SAAS,KAAK,UAAU,EAAE;MACtC,MAAM,IAAInH,KAAK,CAAC,uEAAuE,CAAC;IAC5F;IACA,IAAIkH,MAAM,EAAEE,MAAM,EAAE;MAChB,MAAM,IAAIpH,KAAK,CAAC,2DAA2D,CAAC;IAChF;IACA;IACA,MAAMqH,MAAM,GAAGJ,YAAY;IAC3B,MAAMK,IAAI,GAAGJ,MAAM,EAAEI,IAAI;IACzB,MAAMC,WAAW,GAAGF,MAAM,CAACE,WAAW,IAAI,+BAA+B;IACzE,MAAMC,MAAM,GAAGN,MAAM,EAAEM,MAAM;IAC7B,MAAMC,UAAU,GAAGP,MAAM,EAAEO,UAAU;IACrC,IAAID,MAAM,KAAK,UAAU,EAAE;MACvB,MAAM,IAAIxH,KAAK,CAAC,uFAAuF,CAAC;IAC5G;IACA,IAAI0H,YAAY,GAAGJ,IAAI,IAAI,SAAS;IACpC,IAAIK,KAAK;IACT,IAAIvK,WAAW,CAACiK,MAAM,CAAC,EAAE;MACrBM,KAAK,GAAG,CACJ;QACIC,IAAI,EAAE,UAAU;QAChBC,QAAQ,EAAE;UACNP,IAAI,EAAEI,YAAY;UAClBH,WAAW;UACXO,UAAU,EAAExL,eAAe,CAAC+K,MAAM;QACtC;MACJ,CAAC,CACJ;IACL,CAAC,MACI;MACD,IAAI,MAAM,IAAIA,MAAM,EAAE;QAClBK,YAAY,GAAGL,MAAM,CAACC,IAAI;MAC9B;MACAK,KAAK,GAAG,CACJ;QACIC,IAAI,EAAE,UAAU;QAChBC,QAAQ,EAAE;UACNP,IAAI,EAAEI,YAAY;UAClBH,WAAW;UACXO,UAAU,EAAET;QAChB;MACJ,CAAC,CACJ;IACL;IACA,MAAMU,GAAG,GAAG,IAAI,CAACZ,SAAS,CAACQ,KAAK,CAAC;IACjC,MAAMK,YAAY,GAAGhL,cAAc,CAACiL,IAAI,CAAE9I,KAAK,IAAK;MAChD,IAAI,CAACA,KAAK,CAAC+I,UAAU,IAAI/I,KAAK,CAAC+I,UAAU,CAACtE,MAAM,KAAK,CAAC,EAAE;QACpD,MAAM,IAAI5D,KAAK,CAAC,sCAAsC,CAAC;MAC3D;MACA,MAAMmI,QAAQ,GAAGhJ,KAAK,CAAC+I,UAAU,CAAClE,IAAI,CAAEoE,EAAE,IAAKA,EAAE,CAACd,IAAI,KAAKI,YAAY,CAAC;MACxE,IAAI,CAACS,QAAQ,EAAE;QACX,MAAM,IAAInI,KAAK,CAAC,gCAAgC0H,YAAY,GAAG,CAAC;MACpE;MACA,OAAOS,QAAQ,CAACE,IAAI;IACxB,CAAC,CAAC;IACF,IAAI,CAACZ,UAAU,EAAE;MACb,OAAOM,GAAG,CAACO,IAAI,CAACN,YAAY,CAAC,CAACO,UAAU,CAAC;QACrChH,OAAO,EAAE;MACb,CAAC,CAAC;IACN;IACA,MAAMiH,YAAY,GAAGrL,mBAAmB,CAACsL,MAAM,CAAC;MAC5C;MACAC,MAAM,EAAEA,CAACvJ,KAAK,EAAE+H,MAAM,KAAKc,YAAY,CAAC9I,MAAM,CAACC,KAAK,CAACwJ,GAAG,EAAEzB,MAAM;IACpE,CAAC,CAAC;IACF,MAAM0B,UAAU,GAAGzL,mBAAmB,CAACsL,MAAM,CAAC;MAC1CC,MAAM,EAAEA,CAAA,KAAM;IAClB,CAAC,CAAC;IACF,MAAMG,kBAAkB,GAAGL,YAAY,CAACM,aAAa,CAAC;MAClDC,SAAS,EAAE,CAACH,UAAU;IAC1B,CAAC,CAAC;IACF,OAAO3L,gBAAgB,CAACgL,IAAI,CAAC,CACzB;MACIU,GAAG,EAAEZ;IACT,CAAC,EACDc,kBAAkB,CACrB,CAAC,CAACN,UAAU,CAAC;MACVhH,OAAO,EAAE;IACb,CAAC,CAAC;EACN;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMyH,eAAe,SAAS9K,aAAa,CAAC;EAC/C,MAAMsG,SAASA,CAACnE,QAAQ,EAAExB,OAAO,EAAE8D,UAAU,EAAE;IAC3C,MAAMoD,IAAI,GAAG,MAAM,IAAI,CAACkD,KAAK,CAAC5I,QAAQ,EAAExB,OAAO,EAAE8D,UAAU,CAAC;IAC5D,MAAMhD,OAAO,GAAG,IAAIpD,SAAS,CAACwJ,IAAI,CAAC;IACnC,IAAI,OAAOpG,OAAO,CAAC5B,OAAO,KAAK,QAAQ,EAAE;MACrC,MAAM,IAAIiC,KAAK,CAAC,uEAAuE,CAAC;IAC5F;IACA,OAAO;MACHN,WAAW,EAAE,CACT;QACIqG,IAAI,EAAEpG,OAAO,CAAC5B,OAAO;QACrB4B;MACJ,CAAC;IAET,CAAC;EACL;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}