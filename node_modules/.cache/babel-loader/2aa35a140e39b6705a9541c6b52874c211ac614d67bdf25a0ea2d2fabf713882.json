{"ast":null,"code":"/* eslint-disable spaced-comment */\nimport { PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate } from \"@langchain/core/prompts\";\nimport { ConditionalPromptSelector, isChatModel } from \"@langchain/core/example_selectors\";\nexport const DEFAULT_REFINE_PROMPT_TMPL = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\nexport const DEFAULT_REFINE_PROMPT = /*#__PURE__*/new PromptTemplate({\n  inputVariables: [\"question\", \"existing_answer\", \"context\"],\n  template: DEFAULT_REFINE_PROMPT_TMPL\n});\nconst refineTemplate = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\nconst messages = [/*#__PURE__*/HumanMessagePromptTemplate.fromTemplate(\"{question}\"), /*#__PURE__*/AIMessagePromptTemplate.fromTemplate(\"{existing_answer}\"), /*#__PURE__*/HumanMessagePromptTemplate.fromTemplate(refineTemplate)];\nexport const CHAT_REFINE_PROMPT = /*#__PURE__*/ChatPromptTemplate.fromMessages(messages);\nexport const REFINE_PROMPT_SELECTOR = /*#__PURE__*/new ConditionalPromptSelector(DEFAULT_REFINE_PROMPT, [[isChatModel, CHAT_REFINE_PROMPT]]);\nexport const DEFAULT_TEXT_QA_PROMPT_TMPL = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and no prior knowledge, answer the question: {question}`;\nexport const DEFAULT_TEXT_QA_PROMPT = /*#__PURE__*/new PromptTemplate({\n  inputVariables: [\"context\", \"question\"],\n  template: DEFAULT_TEXT_QA_PROMPT_TMPL\n});\nconst chat_qa_prompt_template = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and no prior knowledge, answer any questions`;\nconst chat_messages = [/*#__PURE__*/SystemMessagePromptTemplate.fromTemplate(chat_qa_prompt_template), /*#__PURE__*/HumanMessagePromptTemplate.fromTemplate(\"{question}\")];\nexport const CHAT_QUESTION_PROMPT = /*#__PURE__*/ChatPromptTemplate.fromMessages(chat_messages);\nexport const QUESTION_PROMPT_SELECTOR = /*#__PURE__*/new ConditionalPromptSelector(DEFAULT_TEXT_QA_PROMPT, [[isChatModel, CHAT_QUESTION_PROMPT]]);","map":{"version":3,"names":["PromptTemplate","ChatPromptTemplate","SystemMessagePromptTemplate","HumanMessagePromptTemplate","AIMessagePromptTemplate","ConditionalPromptSelector","isChatModel","DEFAULT_REFINE_PROMPT_TMPL","DEFAULT_REFINE_PROMPT","inputVariables","template","refineTemplate","messages","fromTemplate","CHAT_REFINE_PROMPT","fromMessages","REFINE_PROMPT_SELECTOR","DEFAULT_TEXT_QA_PROMPT_TMPL","DEFAULT_TEXT_QA_PROMPT","chat_qa_prompt_template","chat_messages","CHAT_QUESTION_PROMPT","QUESTION_PROMPT_SELECTOR"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/langchain/dist/chains/question_answering/refine_prompts.js"],"sourcesContent":["/* eslint-disable spaced-comment */\nimport { PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, } from \"@langchain/core/prompts\";\nimport { ConditionalPromptSelector, isChatModel, } from \"@langchain/core/example_selectors\";\nexport const DEFAULT_REFINE_PROMPT_TMPL = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\nexport const DEFAULT_REFINE_PROMPT = /*#__PURE__*/ new PromptTemplate({\n    inputVariables: [\"question\", \"existing_answer\", \"context\"],\n    template: DEFAULT_REFINE_PROMPT_TMPL,\n});\nconst refineTemplate = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\nconst messages = [\n    /*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n    /*#__PURE__*/ AIMessagePromptTemplate.fromTemplate(\"{existing_answer}\"),\n    /*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(refineTemplate),\n];\nexport const CHAT_REFINE_PROMPT = \n/*#__PURE__*/ ChatPromptTemplate.fromMessages(messages);\nexport const REFINE_PROMPT_SELECTOR = \n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_REFINE_PROMPT, [\n    [isChatModel, CHAT_REFINE_PROMPT],\n]);\nexport const DEFAULT_TEXT_QA_PROMPT_TMPL = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and no prior knowledge, answer the question: {question}`;\nexport const DEFAULT_TEXT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\n    inputVariables: [\"context\", \"question\"],\n    template: DEFAULT_TEXT_QA_PROMPT_TMPL,\n});\nconst chat_qa_prompt_template = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and no prior knowledge, answer any questions`;\nconst chat_messages = [\n    /*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(chat_qa_prompt_template),\n    /*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nexport const CHAT_QUESTION_PROMPT = \n/*#__PURE__*/ ChatPromptTemplate.fromMessages(chat_messages);\nexport const QUESTION_PROMPT_SELECTOR = \n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_TEXT_QA_PROMPT, [\n    [isChatModel, CHAT_QUESTION_PROMPT],\n]);\n"],"mappings":"AAAA;AACA,SAASA,cAAc,EAAEC,kBAAkB,EAAEC,2BAA2B,EAAEC,0BAA0B,EAAEC,uBAAuB,QAAS,yBAAyB;AAC/J,SAASC,yBAAyB,EAAEC,WAAW,QAAS,mCAAmC;AAC3F,OAAO,MAAMC,0BAA0B,GAAG;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yDAAyD;AACzD,OAAO,MAAMC,qBAAqB,GAAG,aAAc,IAAIR,cAAc,CAAC;EAClES,cAAc,EAAE,CAAC,UAAU,EAAE,iBAAiB,EAAE,SAAS,CAAC;EAC1DC,QAAQ,EAAEH;AACd,CAAC,CAAC;AACF,MAAMI,cAAc,GAAG;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yDAAyD;AACzD,MAAMC,QAAQ,GAAG,CACb,aAAcT,0BAA0B,CAACU,YAAY,CAAC,YAAY,CAAC,EACnE,aAAcT,uBAAuB,CAACS,YAAY,CAAC,mBAAmB,CAAC,EACvE,aAAcV,0BAA0B,CAACU,YAAY,CAACF,cAAc,CAAC,CACxE;AACD,OAAO,MAAMG,kBAAkB,GAC/B,aAAcb,kBAAkB,CAACc,YAAY,CAACH,QAAQ,CAAC;AACvD,OAAO,MAAMI,sBAAsB,GACnC,aAAc,IAAIX,yBAAyB,CAACG,qBAAqB,EAAE,CAC/D,CAACF,WAAW,EAAEQ,kBAAkB,CAAC,CACpC,CAAC;AACF,OAAO,MAAMG,2BAA2B,GAAG;AAC3C;AACA;AACA;AACA,sFAAsF;AACtF,OAAO,MAAMC,sBAAsB,GAAG,aAAc,IAAIlB,cAAc,CAAC;EACnES,cAAc,EAAE,CAAC,SAAS,EAAE,UAAU,CAAC;EACvCC,QAAQ,EAAEO;AACd,CAAC,CAAC;AACF,MAAME,uBAAuB,GAAG;AAChC;AACA;AACA;AACA,2EAA2E;AAC3E,MAAMC,aAAa,GAAG,CAClB,aAAclB,2BAA2B,CAACW,YAAY,CAACM,uBAAuB,CAAC,EAC/E,aAAchB,0BAA0B,CAACU,YAAY,CAAC,YAAY,CAAC,CACtE;AACD,OAAO,MAAMQ,oBAAoB,GACjC,aAAcpB,kBAAkB,CAACc,YAAY,CAACK,aAAa,CAAC;AAC5D,OAAO,MAAME,wBAAwB,GACrC,aAAc,IAAIjB,yBAAyB,CAACa,sBAAsB,EAAE,CAChE,CAACZ,WAAW,EAAEe,oBAAoB,CAAC,CACtC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}