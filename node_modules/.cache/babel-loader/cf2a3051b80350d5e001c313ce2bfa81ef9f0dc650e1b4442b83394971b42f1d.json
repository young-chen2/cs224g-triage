{"ast":null,"code":"import { AIMessage, getBufferString } from \"../messages/index.js\";\nimport { RUN_KEY, GenerationChunk } from \"../outputs.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { BaseLanguageModel } from \"./base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n  constructor({\n    concurrency,\n    ...rest\n  }) {\n    super(concurrency ? {\n      maxConcurrency: concurrency,\n      ...rest\n    } : rest);\n    // Only ever instantiated in main LangChain\n    Object.defineProperty(this, \"lc_namespace\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: [\"langchain\", \"llms\", this._llmType()]\n    });\n  }\n  /**\n   * This method takes an input and options, and returns a string. It\n   * converts the input to a prompt value and generates a result based on\n   * the prompt.\n   * @param input Input for the LLM.\n   * @param options Options for the LLM call.\n   * @returns A string result based on the prompt.\n   */\n  async invoke(input, options) {\n    const promptValue = BaseLLM._convertInputToPromptValue(input);\n    const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n    return result.generations[0][0].text;\n  }\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(_input, _options, _runManager) {\n    throw new Error(\"Not implemented.\");\n  }\n  _separateRunnableConfigFromCallOptionsCompat(options) {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n    callOptions.signal = runnableConfig.signal;\n    return [runnableConfig, callOptions];\n  }\n  async *_streamIterator(input, options) {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseLLM._convertInputToPromptValue(input);\n      const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(options);\n      const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1\n      };\n      const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), [prompt.toString()], runnableConfig.runId, undefined, extra, undefined, undefined, runnableConfig.runName);\n      let generation = new GenerationChunk({\n        text: \"\"\n      });\n      try {\n        for await (const chunk of this._streamResponseChunks(prompt.toString(), callOptions, runManagers?.[0])) {\n          if (!generation) {\n            generation = chunk;\n          } else {\n            generation = generation.concat(chunk);\n          }\n          if (typeof chunk.text === \"string\") {\n            yield chunk.text;\n          }\n        }\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMEnd({\n        generations: [[generation]]\n      })));\n    }\n  }\n  /**\n   * This method takes prompt values, options, and callbacks, and generates\n   * a result based on the prompts.\n   * @param promptValues Prompt values for the LLM.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns An LLMResult based on the prompts.\n   */\n  async generatePrompt(promptValues, options, callbacks) {\n    const prompts = promptValues.map(promptValue => promptValue.toString());\n    return this.generate(prompts, options, callbacks);\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options) {\n    return {};\n  }\n  _flattenLLMResult(llmResult) {\n    const llmResults = [];\n    for (let i = 0; i < llmResult.generations.length; i += 1) {\n      const genList = llmResult.generations[i];\n      if (i === 0) {\n        llmResults.push({\n          generations: [genList],\n          llmOutput: llmResult.llmOutput\n        });\n      } else {\n        const llmOutput = llmResult.llmOutput ? {\n          ...llmResult.llmOutput,\n          tokenUsage: {}\n        } : undefined;\n        llmResults.push({\n          generations: [genList],\n          llmOutput\n        });\n      }\n    }\n    return llmResults;\n  }\n  /** @ignore */\n  async _generateUncached(prompts, parsedOptions, handledOptions, startedRunManagers) {\n    let runManagers;\n    if (startedRunManagers !== undefined && startedRunManagers.length === prompts.length) {\n      runManagers = startedRunManagers;\n    } else {\n      const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n        verbose: this.verbose\n      });\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: prompts.length\n      };\n      runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions?.runName);\n    }\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(callbackHandlerPrefersStreaming);\n    let output;\n    if (hasStreamingHandler && prompts.length === 1 && this._streamResponseChunks !== BaseLLM.prototype._streamResponseChunks) {\n      try {\n        const stream = await this._streamResponseChunks(prompts[0], parsedOptions, runManagers?.[0]);\n        let aggregated;\n        for await (const chunk of stream) {\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        output = {\n          generations: [[aggregated]],\n          llmOutput: {}\n        };\n        await runManagers?.[0].handleLLMEnd(output);\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      try {\n        output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n      } catch (err) {\n        await Promise.all((runManagers ?? []).map(runManager => runManager?.handleLLMError(err)));\n        throw err;\n      }\n      const flattenedOutputs = this._flattenLLMResult(output);\n      await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n    }\n    const runIds = runManagers?.map(manager => manager.runId) || undefined;\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runIds ? {\n        runIds\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  async _generateCached({\n    prompts,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions,\n    runId\n  }) {\n    const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, {\n      verbose: this.verbose\n    });\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: prompts.length\n    };\n    const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, runId, undefined, extra, undefined, undefined, handledOptions?.runName);\n    // generate results\n    const missingPromptIndices = [];\n    const results = await Promise.allSettled(prompts.map(async (prompt, index) => {\n      const result = await cache.lookup(prompt, llmStringKey);\n      if (result == null) {\n        missingPromptIndices.push(index);\n      }\n      return result;\n    }));\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results.map((result, index) => ({\n      result,\n      runManager: runManagers?.[index]\n    })).filter(({\n      result\n    }) => result.status === \"fulfilled\" && result.value != null || result.status === \"rejected\");\n    // Handle results and call run managers\n    const generations = [];\n    await Promise.all(cachedResults.map(async ({\n      result: promiseResult,\n      runManager\n    }, i) => {\n      if (promiseResult.status === \"fulfilled\") {\n        const result = promiseResult.value;\n        generations[i] = result.map(result => {\n          // eslint-disable-next-line no-param-reassign\n          result.generationInfo = {\n            ...result.generationInfo,\n            tokenUsage: {}\n          };\n          return result;\n        });\n        if (result.length) {\n          await runManager?.handleLLMNewToken(result[0].text);\n        }\n        return runManager?.handleLLMEnd({\n          generations: [result]\n        }, undefined, undefined, undefined, {\n          cached: true\n        });\n      } else {\n        // status === \"rejected\"\n        await runManager?.handleLLMError(promiseResult.reason, undefined, undefined, undefined, {\n          cached: true\n        });\n        return Promise.reject(promiseResult.reason);\n      }\n    }));\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers\n    };\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers ? {\n        runIds: runManagers?.map(manager => manager.runId)\n      } : undefined,\n      configurable: true\n    });\n    return output;\n  }\n  /**\n   * Run the LLM on the given prompts and input, handling caching.\n   */\n  async generate(prompts, options, callbacks) {\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n    let parsedOptions;\n    if (Array.isArray(options)) {\n      parsedOptions = {\n        stop: options\n      };\n    } else {\n      parsedOptions = options;\n    }\n    const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n    if (!this.cache) {\n      return this._generateUncached(prompts, callOptions, runnableConfig);\n    }\n    const {\n      cache\n    } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n    const {\n      generations,\n      missingPromptIndices,\n      startedRunManagers\n    } = await this._generateCached({\n      prompts,\n      cache,\n      llmStringKey,\n      parsedOptions: callOptions,\n      handledOptions: runnableConfig,\n      runId: runnableConfig.runId\n    });\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(missingPromptIndices.map(i => prompts[i]), callOptions, runnableConfig, startedRunManagers !== undefined ? missingPromptIndices.map(i => startedRunManagers?.[i]) : undefined);\n      await Promise.all(results.generations.map(async (generation, index) => {\n        const promptIndex = missingPromptIndices[index];\n        generations[promptIndex] = generation;\n        return cache.update(prompts[promptIndex], llmStringKey, generation);\n      }));\n      llmOutput = results.llmOutput ?? {};\n    }\n    return {\n      generations,\n      llmOutput\n    };\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n   */\n  async call(prompt, options, callbacks) {\n    const {\n      generations\n    } = await this.generate([prompt], options, callbacks);\n    return generations[0][0].text;\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * This method is similar to `call`, but it's used for making predictions\n   * based on the input text.\n   * @param text Input text for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A prediction based on the input text.\n   */\n  async predict(text, options, callbacks) {\n    return this.call(text, options, callbacks);\n  }\n  /**\n   * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n   *\n   * This method takes a list of messages, options, and callbacks, and\n   * returns a predicted message.\n   * @param messages A list of messages for the prediction.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns A predicted message based on the list of messages.\n   */\n  async predictMessages(messages, options, callbacks) {\n    const text = getBufferString(messages);\n    const prediction = await this.call(text, options, callbacks);\n    return new AIMessage(prediction);\n  }\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams() {\n    return {};\n  }\n  /**\n   * @deprecated\n   * Return a json-like object representing this LLM.\n   */\n  serialize() {\n    return {\n      ...this._identifyingParams(),\n      _type: this._llmType(),\n      _model: this._modelType()\n    };\n  }\n  _modelType() {\n    return \"base_llm\";\n  }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n  async _generate(prompts, options, runManager) {\n    const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, {\n      ...options,\n      promptIndex\n    }, runManager).then(text => [{\n      text\n    }])));\n    return {\n      generations\n    };\n  }\n}","map":{"version":3,"names":["AIMessage","getBufferString","RUN_KEY","GenerationChunk","CallbackManager","BaseLanguageModel","concat","callbackHandlerPrefersStreaming","BaseLLM","constructor","concurrency","rest","maxConcurrency","Object","defineProperty","enumerable","configurable","writable","value","_llmType","invoke","input","options","promptValue","_convertInputToPromptValue","result","generatePrompt","callbacks","generations","text","_streamResponseChunks","_input","_options","_runManager","Error","_separateRunnableConfigFromCallOptionsCompat","runnableConfig","callOptions","_separateRunnableConfigFromCallOptions","signal","_streamIterator","prototype","prompt","callbackManager_","configure","tags","metadata","verbose","extra","invocation_params","invocationParams","batch_size","runManagers","handleLLMStart","toJSON","toString","runId","undefined","runName","generation","chunk","err","Promise","all","map","runManager","handleLLMError","handleLLMEnd","promptValues","prompts","generate","_flattenLLMResult","llmResult","llmResults","i","length","genList","push","llmOutput","tokenUsage","_generateUncached","parsedOptions","handledOptions","startedRunManagers","hasStreamingHandler","handlers","find","output","stream","aggregated","e","_generate","flattenedOutputs","runIds","manager","_generateCached","cache","llmStringKey","missingPromptIndices","results","allSettled","index","lookup","cachedResults","filter","status","promiseResult","generationInfo","handleLLMNewToken","cached","reason","reject","Array","isArray","stop","_getSerializedCacheKeyParametersForCall","promptIndex","update","call","predict","predictMessages","messages","prediction","_identifyingParams","serialize","_type","_model","_modelType","LLM","_call","then"],"sources":["/Users/youngchen/Downloads/cs224g-triage/node_modules/@langchain/core/dist/language_models/llms.js"],"sourcesContent":["import { AIMessage, getBufferString, } from \"../messages/index.js\";\nimport { RUN_KEY, GenerationChunk, } from \"../outputs.js\";\nimport { CallbackManager, } from \"../callbacks/manager.js\";\nimport { BaseLanguageModel, } from \"./base.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport class BaseLLM extends BaseLanguageModel {\n    constructor({ concurrency, ...rest }) {\n        super(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\n        // Only ever instantiated in main LangChain\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"llms\", this._llmType()]\n        });\n    }\n    /**\n     * This method takes an input and options, and returns a string. It\n     * converts the input to a prompt value and generates a result based on\n     * the prompt.\n     * @param input Input for the LLM.\n     * @param options Options for the LLM call.\n     * @returns A string result based on the prompt.\n     */\n    async invoke(input, options) {\n        const promptValue = BaseLLM._convertInputToPromptValue(input);\n        const result = await this.generatePrompt([promptValue], options, options?.callbacks);\n        return result.generations[0][0].text;\n    }\n    // eslint-disable-next-line require-yield\n    async *_streamResponseChunks(_input, _options, _runManager) {\n        throw new Error(\"Not implemented.\");\n    }\n    _separateRunnableConfigFromCallOptionsCompat(options) {\n        // For backwards compat, keep `signal` in both runnableConfig and callOptions\n        const [runnableConfig, callOptions] = super._separateRunnableConfigFromCallOptions(options);\n        callOptions.signal = runnableConfig.signal;\n        return [runnableConfig, callOptions];\n    }\n    async *_streamIterator(input, options) {\n        // Subclass check required to avoid double callbacks with default implementation\n        if (this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks) {\n            yield this.invoke(input, options);\n        }\n        else {\n            const prompt = BaseLLM._convertInputToPromptValue(input);\n            const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(options);\n            const callbackManager_ = await CallbackManager.configure(runnableConfig.callbacks, this.callbacks, runnableConfig.tags, this.tags, runnableConfig.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: callOptions,\n                invocation_params: this?.invocationParams(callOptions),\n                batch_size: 1,\n            };\n            const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), [prompt.toString()], runnableConfig.runId, undefined, extra, undefined, undefined, runnableConfig.runName);\n            let generation = new GenerationChunk({\n                text: \"\",\n            });\n            try {\n                for await (const chunk of this._streamResponseChunks(prompt.toString(), callOptions, runManagers?.[0])) {\n                    if (!generation) {\n                        generation = chunk;\n                    }\n                    else {\n                        generation = generation.concat(chunk);\n                    }\n                    if (typeof chunk.text === \"string\") {\n                        yield chunk.text;\n                    }\n                }\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMEnd({\n                generations: [[generation]],\n            })));\n        }\n    }\n    /**\n     * This method takes prompt values, options, and callbacks, and generates\n     * a result based on the prompts.\n     * @param promptValues Prompt values for the LLM.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns An LLMResult based on the prompts.\n     */\n    async generatePrompt(promptValues, options, callbacks) {\n        const prompts = promptValues.map((promptValue) => promptValue.toString());\n        return this.generate(prompts, options, callbacks);\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _flattenLLMResult(llmResult) {\n        const llmResults = [];\n        for (let i = 0; i < llmResult.generations.length; i += 1) {\n            const genList = llmResult.generations[i];\n            if (i === 0) {\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput: llmResult.llmOutput,\n                });\n            }\n            else {\n                const llmOutput = llmResult.llmOutput\n                    ? { ...llmResult.llmOutput, tokenUsage: {} }\n                    : undefined;\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput,\n                });\n            }\n        }\n        return llmResults;\n    }\n    /** @ignore */\n    async _generateUncached(prompts, parsedOptions, handledOptions, startedRunManagers) {\n        let runManagers;\n        if (startedRunManagers !== undefined &&\n            startedRunManagers.length === prompts.length) {\n            runManagers = startedRunManagers;\n        }\n        else {\n            const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n            const extra = {\n                options: parsedOptions,\n                invocation_params: this?.invocationParams(parsedOptions),\n                batch_size: prompts.length,\n            };\n            runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, handledOptions.runId, undefined, extra, undefined, undefined, handledOptions?.runName);\n        }\n        // Even if stream is not explicitly called, check if model is implicitly\n        // called from streamEvents() or streamLog() to get all streamed events.\n        // Bail out if _streamResponseChunks not overridden\n        const hasStreamingHandler = !!runManagers?.[0].handlers.find(callbackHandlerPrefersStreaming);\n        let output;\n        if (hasStreamingHandler &&\n            prompts.length === 1 &&\n            this._streamResponseChunks !== BaseLLM.prototype._streamResponseChunks) {\n            try {\n                const stream = await this._streamResponseChunks(prompts[0], parsedOptions, runManagers?.[0]);\n                let aggregated;\n                for await (const chunk of stream) {\n                    if (aggregated === undefined) {\n                        aggregated = chunk;\n                    }\n                    else {\n                        aggregated = concat(aggregated, chunk);\n                    }\n                }\n                if (aggregated === undefined) {\n                    throw new Error(\"Received empty response from chat model call.\");\n                }\n                output = { generations: [[aggregated]], llmOutput: {} };\n                await runManagers?.[0].handleLLMEnd(output);\n            }\n            catch (e) {\n                await runManagers?.[0].handleLLMError(e);\n                throw e;\n            }\n        }\n        else {\n            try {\n                output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n            }\n            catch (err) {\n                await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n                throw err;\n            }\n            const flattenedOutputs = this._flattenLLMResult(output);\n            await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n        }\n        const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runIds ? { runIds } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    async _generateCached({ prompts, cache, llmStringKey, parsedOptions, handledOptions, runId, }) {\n        const callbackManager_ = await CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n            batch_size: prompts.length,\n        };\n        const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, runId, undefined, extra, undefined, undefined, handledOptions?.runName);\n        // generate results\n        const missingPromptIndices = [];\n        const results = await Promise.allSettled(prompts.map(async (prompt, index) => {\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (result == null) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        // Map run managers to the results before filtering out null results\n        // Null results are just absent from the cache.\n        const cachedResults = results\n            .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n            .filter(({ result }) => (result.status === \"fulfilled\" && result.value != null) ||\n            result.status === \"rejected\");\n        // Handle results and call run managers\n        const generations = [];\n        await Promise.all(cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n            if (promiseResult.status === \"fulfilled\") {\n                const result = promiseResult.value;\n                generations[i] = result.map((result) => {\n                    // eslint-disable-next-line no-param-reassign\n                    result.generationInfo = {\n                        ...result.generationInfo,\n                        tokenUsage: {},\n                    };\n                    return result;\n                });\n                if (result.length) {\n                    await runManager?.handleLLMNewToken(result[0].text);\n                }\n                return runManager?.handleLLMEnd({\n                    generations: [result],\n                }, undefined, undefined, undefined, {\n                    cached: true,\n                });\n            }\n            else {\n                // status === \"rejected\"\n                await runManager?.handleLLMError(promiseResult.reason, undefined, undefined, undefined, {\n                    cached: true,\n                });\n                return Promise.reject(promiseResult.reason);\n            }\n        }));\n        const output = {\n            generations,\n            missingPromptIndices,\n            startedRunManagers: runManagers,\n        };\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, RUN_KEY, {\n            value: runManagers\n                ? { runIds: runManagers?.map((manager) => manager.runId) }\n                : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Run the LLM on the given prompts and input, handling caching.\n     */\n    async generate(prompts, options, callbacks) {\n        if (!Array.isArray(prompts)) {\n            throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n        }\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else {\n            parsedOptions = options;\n        }\n        const [runnableConfig, callOptions] = this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n        runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n        if (!this.cache) {\n            return this._generateUncached(prompts, callOptions, runnableConfig);\n        }\n        const { cache } = this;\n        const llmStringKey = this._getSerializedCacheKeyParametersForCall(callOptions);\n        const { generations, missingPromptIndices, startedRunManagers } = await this._generateCached({\n            prompts,\n            cache,\n            llmStringKey,\n            parsedOptions: callOptions,\n            handledOptions: runnableConfig,\n            runId: runnableConfig.runId,\n        });\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), callOptions, runnableConfig, startedRunManagers !== undefined\n                ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n                : undefined);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                return cache.update(prompts[promptIndex], llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n     */\n    async call(prompt, options, callbacks) {\n        const { generations } = await this.generate([prompt], options, callbacks);\n        return generations[0][0].text;\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * This method is similar to `call`, but it's used for making predictions\n     * based on the input text.\n     * @param text Input text for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A prediction based on the input text.\n     */\n    async predict(text, options, callbacks) {\n        return this.call(text, options, callbacks);\n    }\n    /**\n     * @deprecated Use .invoke() instead. Will be removed in 0.2.0.\n     *\n     * This method takes a list of messages, options, and callbacks, and\n     * returns a predicted message.\n     * @param messages A list of messages for the prediction.\n     * @param options Options for the LLM call.\n     * @param callbacks Callbacks for the LLM call.\n     * @returns A predicted message based on the list of messages.\n     */\n    async predictMessages(messages, options, callbacks) {\n        const text = getBufferString(messages);\n        const prediction = await this.call(text, options, callbacks);\n        return new AIMessage(prediction);\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * @deprecated\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    _modelType() {\n        return \"base_llm\";\n    }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport class LLM extends BaseLLM {\n    async _generate(prompts, options, runManager) {\n        const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, { ...options, promptIndex }, runManager).then((text) => [{ text }])));\n        return { generations };\n    }\n}\n"],"mappings":"AAAA,SAASA,SAAS,EAAEC,eAAe,QAAS,sBAAsB;AAClE,SAASC,OAAO,EAAEC,eAAe,QAAS,eAAe;AACzD,SAASC,eAAe,QAAS,yBAAyB;AAC1D,SAASC,iBAAiB,QAAS,WAAW;AAC9C,SAASC,MAAM,QAAQ,oBAAoB;AAC3C,SAASC,+BAA+B,QAAQ,sBAAsB;AACtE;AACA;AACA;AACA,OAAO,MAAMC,OAAO,SAASH,iBAAiB,CAAC;EAC3CI,WAAWA,CAAC;IAAEC,WAAW;IAAE,GAAGC;EAAK,CAAC,EAAE;IAClC,KAAK,CAACD,WAAW,GAAG;MAAEE,cAAc,EAAEF,WAAW;MAAE,GAAGC;IAAK,CAAC,GAAGA,IAAI,CAAC;IACpE;IACAE,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,CAAC,WAAW,EAAE,MAAM,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC;IAChD,CAAC,CAAC;EACN;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMC,MAAMA,CAACC,KAAK,EAAEC,OAAO,EAAE;IACzB,MAAMC,WAAW,GAAGf,OAAO,CAACgB,0BAA0B,CAACH,KAAK,CAAC;IAC7D,MAAMI,MAAM,GAAG,MAAM,IAAI,CAACC,cAAc,CAAC,CAACH,WAAW,CAAC,EAAED,OAAO,EAAEA,OAAO,EAAEK,SAAS,CAAC;IACpF,OAAOF,MAAM,CAACG,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACxC;EACA;EACA,OAAOC,qBAAqBA,CAACC,MAAM,EAAEC,QAAQ,EAAEC,WAAW,EAAE;IACxD,MAAM,IAAIC,KAAK,CAAC,kBAAkB,CAAC;EACvC;EACAC,4CAA4CA,CAACb,OAAO,EAAE;IAClD;IACA,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,KAAK,CAACC,sCAAsC,CAAChB,OAAO,CAAC;IAC3Fe,WAAW,CAACE,MAAM,GAAGH,cAAc,CAACG,MAAM;IAC1C,OAAO,CAACH,cAAc,EAAEC,WAAW,CAAC;EACxC;EACA,OAAOG,eAAeA,CAACnB,KAAK,EAAEC,OAAO,EAAE;IACnC;IACA,IAAI,IAAI,CAACQ,qBAAqB,KAAKtB,OAAO,CAACiC,SAAS,CAACX,qBAAqB,EAAE;MACxE,MAAM,IAAI,CAACV,MAAM,CAACC,KAAK,EAAEC,OAAO,CAAC;IACrC,CAAC,MACI;MACD,MAAMoB,MAAM,GAAGlC,OAAO,CAACgB,0BAA0B,CAACH,KAAK,CAAC;MACxD,MAAM,CAACe,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,4CAA4C,CAACb,OAAO,CAAC;MAChG,MAAMqB,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACR,cAAc,CAACT,SAAS,EAAE,IAAI,CAACA,SAAS,EAAES,cAAc,CAACS,IAAI,EAAE,IAAI,CAACA,IAAI,EAAET,cAAc,CAACU,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV1B,OAAO,EAAEe,WAAW;QACpBY,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAACb,WAAW,CAAC;QACtDc,UAAU,EAAE;MAChB,CAAC;MACD,MAAMC,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAE,CAACZ,MAAM,CAACa,QAAQ,CAAC,CAAC,CAAC,EAAEnB,cAAc,CAACoB,KAAK,EAAEC,SAAS,EAAET,KAAK,EAAES,SAAS,EAAEA,SAAS,EAAErB,cAAc,CAACsB,OAAO,CAAC;MACpL,IAAIC,UAAU,GAAG,IAAIxD,eAAe,CAAC;QACjC0B,IAAI,EAAE;MACV,CAAC,CAAC;MACF,IAAI;QACA,WAAW,MAAM+B,KAAK,IAAI,IAAI,CAAC9B,qBAAqB,CAACY,MAAM,CAACa,QAAQ,CAAC,CAAC,EAAElB,WAAW,EAAEe,WAAW,GAAG,CAAC,CAAC,CAAC,EAAE;UACpG,IAAI,CAACO,UAAU,EAAE;YACbA,UAAU,GAAGC,KAAK;UACtB,CAAC,MACI;YACDD,UAAU,GAAGA,UAAU,CAACrD,MAAM,CAACsD,KAAK,CAAC;UACzC;UACA,IAAI,OAAOA,KAAK,CAAC/B,IAAI,KAAK,QAAQ,EAAE;YAChC,MAAM+B,KAAK,CAAC/B,IAAI;UACpB;QACJ;MACJ,CAAC,CACD,OAAOgC,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,IAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,IAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEE,YAAY,CAAC;QAC/EvC,WAAW,EAAE,CAAC,CAAC+B,UAAU,CAAC;MAC9B,CAAC,CAAC,CAAC,CAAC;IACR;EACJ;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMjC,cAAcA,CAAC0C,YAAY,EAAE9C,OAAO,EAAEK,SAAS,EAAE;IACnD,MAAM0C,OAAO,GAAGD,YAAY,CAACJ,GAAG,CAAEzC,WAAW,IAAKA,WAAW,CAACgC,QAAQ,CAAC,CAAC,CAAC;IACzE,OAAO,IAAI,CAACe,QAAQ,CAACD,OAAO,EAAE/C,OAAO,EAAEK,SAAS,CAAC;EACrD;EACA;AACJ;AACA;EACI;EACAuB,gBAAgBA,CAAClB,QAAQ,EAAE;IACvB,OAAO,CAAC,CAAC;EACb;EACAuC,iBAAiBA,CAACC,SAAS,EAAE;IACzB,MAAMC,UAAU,GAAG,EAAE;IACrB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,SAAS,CAAC5C,WAAW,CAAC+C,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACtD,MAAME,OAAO,GAAGJ,SAAS,CAAC5C,WAAW,CAAC8C,CAAC,CAAC;MACxC,IAAIA,CAAC,KAAK,CAAC,EAAE;QACTD,UAAU,CAACI,IAAI,CAAC;UACZjD,WAAW,EAAE,CAACgD,OAAO,CAAC;UACtBE,SAAS,EAAEN,SAAS,CAACM;QACzB,CAAC,CAAC;MACN,CAAC,MACI;QACD,MAAMA,SAAS,GAAGN,SAAS,CAACM,SAAS,GAC/B;UAAE,GAAGN,SAAS,CAACM,SAAS;UAAEC,UAAU,EAAE,CAAC;QAAE,CAAC,GAC1CtB,SAAS;QACfgB,UAAU,CAACI,IAAI,CAAC;UACZjD,WAAW,EAAE,CAACgD,OAAO,CAAC;UACtBE;QACJ,CAAC,CAAC;MACN;IACJ;IACA,OAAOL,UAAU;EACrB;EACA;EACA,MAAMO,iBAAiBA,CAACX,OAAO,EAAEY,aAAa,EAAEC,cAAc,EAAEC,kBAAkB,EAAE;IAChF,IAAI/B,WAAW;IACf,IAAI+B,kBAAkB,KAAK1B,SAAS,IAChC0B,kBAAkB,CAACR,MAAM,KAAKN,OAAO,CAACM,MAAM,EAAE;MAC9CvB,WAAW,GAAG+B,kBAAkB;IACpC,CAAC,MACI;MACD,MAAMxC,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACsC,cAAc,CAACvD,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuD,cAAc,CAACrC,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEqC,cAAc,CAACpC,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;QAAEC,OAAO,EAAE,IAAI,CAACA;MAAQ,CAAC,CAAC;MACrM,MAAMC,KAAK,GAAG;QACV1B,OAAO,EAAE2D,aAAa;QACtBhC,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAAC+B,aAAa,CAAC;QACxD9B,UAAU,EAAEkB,OAAO,CAACM;MACxB,CAAC;MACDvB,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,OAAO,EAAEa,cAAc,CAAC1B,KAAK,EAAEC,SAAS,EAAET,KAAK,EAAES,SAAS,EAAEA,SAAS,EAAEyB,cAAc,EAAExB,OAAO,CAAC;IACvK;IACA;IACA;IACA;IACA,MAAM0B,mBAAmB,GAAG,CAAC,CAAChC,WAAW,GAAG,CAAC,CAAC,CAACiC,QAAQ,CAACC,IAAI,CAAC/E,+BAA+B,CAAC;IAC7F,IAAIgF,MAAM;IACV,IAAIH,mBAAmB,IACnBf,OAAO,CAACM,MAAM,KAAK,CAAC,IACpB,IAAI,CAAC7C,qBAAqB,KAAKtB,OAAO,CAACiC,SAAS,CAACX,qBAAqB,EAAE;MACxE,IAAI;QACA,MAAM0D,MAAM,GAAG,MAAM,IAAI,CAAC1D,qBAAqB,CAACuC,OAAO,CAAC,CAAC,CAAC,EAAEY,aAAa,EAAE7B,WAAW,GAAG,CAAC,CAAC,CAAC;QAC5F,IAAIqC,UAAU;QACd,WAAW,MAAM7B,KAAK,IAAI4B,MAAM,EAAE;UAC9B,IAAIC,UAAU,KAAKhC,SAAS,EAAE;YAC1BgC,UAAU,GAAG7B,KAAK;UACtB,CAAC,MACI;YACD6B,UAAU,GAAGnF,MAAM,CAACmF,UAAU,EAAE7B,KAAK,CAAC;UAC1C;QACJ;QACA,IAAI6B,UAAU,KAAKhC,SAAS,EAAE;UAC1B,MAAM,IAAIvB,KAAK,CAAC,+CAA+C,CAAC;QACpE;QACAqD,MAAM,GAAG;UAAE3D,WAAW,EAAE,CAAC,CAAC6D,UAAU,CAAC,CAAC;UAAEX,SAAS,EAAE,CAAC;QAAE,CAAC;QACvD,MAAM1B,WAAW,GAAG,CAAC,CAAC,CAACe,YAAY,CAACoB,MAAM,CAAC;MAC/C,CAAC,CACD,OAAOG,CAAC,EAAE;QACN,MAAMtC,WAAW,GAAG,CAAC,CAAC,CAACc,cAAc,CAACwB,CAAC,CAAC;QACxC,MAAMA,CAAC;MACX;IACJ,CAAC,MACI;MACD,IAAI;QACAH,MAAM,GAAG,MAAM,IAAI,CAACI,SAAS,CAACtB,OAAO,EAAEY,aAAa,EAAE7B,WAAW,GAAG,CAAC,CAAC,CAAC;MAC3E,CAAC,CACD,OAAOS,GAAG,EAAE;QACR,MAAMC,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,IAAI,EAAE,EAAEY,GAAG,CAAEC,UAAU,IAAKA,UAAU,EAAEC,cAAc,CAACL,GAAG,CAAC,CAAC,CAAC;QAC3F,MAAMA,GAAG;MACb;MACA,MAAM+B,gBAAgB,GAAG,IAAI,CAACrB,iBAAiB,CAACgB,MAAM,CAAC;MACvD,MAAMzB,OAAO,CAACC,GAAG,CAAC,CAACX,WAAW,IAAI,EAAE,EAAEY,GAAG,CAAC,CAACC,UAAU,EAAES,CAAC,KAAKT,UAAU,EAAEE,YAAY,CAACyB,gBAAgB,CAAClB,CAAC,CAAC,CAAC,CAAC,CAAC;IAChH;IACA,MAAMmB,MAAM,GAAGzC,WAAW,EAAEY,GAAG,CAAE8B,OAAO,IAAKA,OAAO,CAACtC,KAAK,CAAC,IAAIC,SAAS;IACxE;IACA;IACA;IACA5C,MAAM,CAACC,cAAc,CAACyE,MAAM,EAAErF,OAAO,EAAE;MACnCgB,KAAK,EAAE2E,MAAM,GAAG;QAAEA;MAAO,CAAC,GAAGpC,SAAS;MACtCzC,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOuE,MAAM;EACjB;EACA,MAAMQ,eAAeA,CAAC;IAAE1B,OAAO;IAAE2B,KAAK;IAAEC,YAAY;IAAEhB,aAAa;IAAEC,cAAc;IAAE1B;EAAO,CAAC,EAAE;IAC3F,MAAMb,gBAAgB,GAAG,MAAMvC,eAAe,CAACwC,SAAS,CAACsC,cAAc,CAACvD,SAAS,EAAE,IAAI,CAACA,SAAS,EAAEuD,cAAc,CAACrC,IAAI,EAAE,IAAI,CAACA,IAAI,EAAEqC,cAAc,CAACpC,QAAQ,EAAE,IAAI,CAACA,QAAQ,EAAE;MAAEC,OAAO,EAAE,IAAI,CAACA;IAAQ,CAAC,CAAC;IACrM,MAAMC,KAAK,GAAG;MACV1B,OAAO,EAAE2D,aAAa;MACtBhC,iBAAiB,EAAE,IAAI,EAAEC,gBAAgB,CAAC+B,aAAa,CAAC;MACxD9B,UAAU,EAAEkB,OAAO,CAACM;IACxB,CAAC;IACD,MAAMvB,WAAW,GAAG,MAAMT,gBAAgB,EAAEU,cAAc,CAAC,IAAI,CAACC,MAAM,CAAC,CAAC,EAAEe,OAAO,EAAEb,KAAK,EAAEC,SAAS,EAAET,KAAK,EAAES,SAAS,EAAEA,SAAS,EAAEyB,cAAc,EAAExB,OAAO,CAAC;IAC1J;IACA,MAAMwC,oBAAoB,GAAG,EAAE;IAC/B,MAAMC,OAAO,GAAG,MAAMrC,OAAO,CAACsC,UAAU,CAAC/B,OAAO,CAACL,GAAG,CAAC,OAAOtB,MAAM,EAAE2D,KAAK,KAAK;MAC1E,MAAM5E,MAAM,GAAG,MAAMuE,KAAK,CAACM,MAAM,CAAC5D,MAAM,EAAEuD,YAAY,CAAC;MACvD,IAAIxE,MAAM,IAAI,IAAI,EAAE;QAChByE,oBAAoB,CAACrB,IAAI,CAACwB,KAAK,CAAC;MACpC;MACA,OAAO5E,MAAM;IACjB,CAAC,CAAC,CAAC;IACH;IACA;IACA,MAAM8E,aAAa,GAAGJ,OAAO,CACxBnC,GAAG,CAAC,CAACvC,MAAM,EAAE4E,KAAK,MAAM;MAAE5E,MAAM;MAAEwC,UAAU,EAAEb,WAAW,GAAGiD,KAAK;IAAE,CAAC,CAAC,CAAC,CACtEG,MAAM,CAAC,CAAC;MAAE/E;IAAO,CAAC,KAAMA,MAAM,CAACgF,MAAM,KAAK,WAAW,IAAIhF,MAAM,CAACP,KAAK,IAAI,IAAI,IAC9EO,MAAM,CAACgF,MAAM,KAAK,UAAU,CAAC;IACjC;IACA,MAAM7E,WAAW,GAAG,EAAE;IACtB,MAAMkC,OAAO,CAACC,GAAG,CAACwC,aAAa,CAACvC,GAAG,CAAC,OAAO;MAAEvC,MAAM,EAAEiF,aAAa;MAAEzC;IAAW,CAAC,EAAES,CAAC,KAAK;MACpF,IAAIgC,aAAa,CAACD,MAAM,KAAK,WAAW,EAAE;QACtC,MAAMhF,MAAM,GAAGiF,aAAa,CAACxF,KAAK;QAClCU,WAAW,CAAC8C,CAAC,CAAC,GAAGjD,MAAM,CAACuC,GAAG,CAAEvC,MAAM,IAAK;UACpC;UACAA,MAAM,CAACkF,cAAc,GAAG;YACpB,GAAGlF,MAAM,CAACkF,cAAc;YACxB5B,UAAU,EAAE,CAAC;UACjB,CAAC;UACD,OAAOtD,MAAM;QACjB,CAAC,CAAC;QACF,IAAIA,MAAM,CAACkD,MAAM,EAAE;UACf,MAAMV,UAAU,EAAE2C,iBAAiB,CAACnF,MAAM,CAAC,CAAC,CAAC,CAACI,IAAI,CAAC;QACvD;QACA,OAAOoC,UAAU,EAAEE,YAAY,CAAC;UAC5BvC,WAAW,EAAE,CAACH,MAAM;QACxB,CAAC,EAAEgC,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;UAChCoD,MAAM,EAAE;QACZ,CAAC,CAAC;MACN,CAAC,MACI;QACD;QACA,MAAM5C,UAAU,EAAEC,cAAc,CAACwC,aAAa,CAACI,MAAM,EAAErD,SAAS,EAAEA,SAAS,EAAEA,SAAS,EAAE;UACpFoD,MAAM,EAAE;QACZ,CAAC,CAAC;QACF,OAAO/C,OAAO,CAACiD,MAAM,CAACL,aAAa,CAACI,MAAM,CAAC;MAC/C;IACJ,CAAC,CAAC,CAAC;IACH,MAAMvB,MAAM,GAAG;MACX3D,WAAW;MACXsE,oBAAoB;MACpBf,kBAAkB,EAAE/B;IACxB,CAAC;IACD;IACA;IACA;IACAvC,MAAM,CAACC,cAAc,CAACyE,MAAM,EAAErF,OAAO,EAAE;MACnCgB,KAAK,EAAEkC,WAAW,GACZ;QAAEyC,MAAM,EAAEzC,WAAW,EAAEY,GAAG,CAAE8B,OAAO,IAAKA,OAAO,CAACtC,KAAK;MAAE,CAAC,GACxDC,SAAS;MACfzC,YAAY,EAAE;IAClB,CAAC,CAAC;IACF,OAAOuE,MAAM;EACjB;EACA;AACJ;AACA;EACI,MAAMjB,QAAQA,CAACD,OAAO,EAAE/C,OAAO,EAAEK,SAAS,EAAE;IACxC,IAAI,CAACqF,KAAK,CAACC,OAAO,CAAC5C,OAAO,CAAC,EAAE;MACzB,MAAM,IAAInC,KAAK,CAAC,iDAAiD,CAAC;IACtE;IACA,IAAI+C,aAAa;IACjB,IAAI+B,KAAK,CAACC,OAAO,CAAC3F,OAAO,CAAC,EAAE;MACxB2D,aAAa,GAAG;QAAEiC,IAAI,EAAE5F;MAAQ,CAAC;IACrC,CAAC,MACI;MACD2D,aAAa,GAAG3D,OAAO;IAC3B;IACA,MAAM,CAACc,cAAc,EAAEC,WAAW,CAAC,GAAG,IAAI,CAACF,4CAA4C,CAAC8C,aAAa,CAAC;IACtG7C,cAAc,CAACT,SAAS,GAAGS,cAAc,CAACT,SAAS,IAAIA,SAAS;IAChE,IAAI,CAAC,IAAI,CAACqE,KAAK,EAAE;MACb,OAAO,IAAI,CAAChB,iBAAiB,CAACX,OAAO,EAAEhC,WAAW,EAAED,cAAc,CAAC;IACvE;IACA,MAAM;MAAE4D;IAAM,CAAC,GAAG,IAAI;IACtB,MAAMC,YAAY,GAAG,IAAI,CAACkB,uCAAuC,CAAC9E,WAAW,CAAC;IAC9E,MAAM;MAAET,WAAW;MAAEsE,oBAAoB;MAAEf;IAAmB,CAAC,GAAG,MAAM,IAAI,CAACY,eAAe,CAAC;MACzF1B,OAAO;MACP2B,KAAK;MACLC,YAAY;MACZhB,aAAa,EAAE5C,WAAW;MAC1B6C,cAAc,EAAE9C,cAAc;MAC9BoB,KAAK,EAAEpB,cAAc,CAACoB;IAC1B,CAAC,CAAC;IACF,IAAIsB,SAAS,GAAG,CAAC,CAAC;IAClB,IAAIoB,oBAAoB,CAACvB,MAAM,GAAG,CAAC,EAAE;MACjC,MAAMwB,OAAO,GAAG,MAAM,IAAI,CAACnB,iBAAiB,CAACkB,oBAAoB,CAAClC,GAAG,CAAEU,CAAC,IAAKL,OAAO,CAACK,CAAC,CAAC,CAAC,EAAErC,WAAW,EAAED,cAAc,EAAE+C,kBAAkB,KAAK1B,SAAS,GACjJyC,oBAAoB,CAAClC,GAAG,CAAEU,CAAC,IAAKS,kBAAkB,GAAGT,CAAC,CAAC,CAAC,GACxDjB,SAAS,CAAC;MAChB,MAAMK,OAAO,CAACC,GAAG,CAACoC,OAAO,CAACvE,WAAW,CAACoC,GAAG,CAAC,OAAOL,UAAU,EAAE0C,KAAK,KAAK;QACnE,MAAMe,WAAW,GAAGlB,oBAAoB,CAACG,KAAK,CAAC;QAC/CzE,WAAW,CAACwF,WAAW,CAAC,GAAGzD,UAAU;QACrC,OAAOqC,KAAK,CAACqB,MAAM,CAAChD,OAAO,CAAC+C,WAAW,CAAC,EAAEnB,YAAY,EAAEtC,UAAU,CAAC;MACvE,CAAC,CAAC,CAAC;MACHmB,SAAS,GAAGqB,OAAO,CAACrB,SAAS,IAAI,CAAC,CAAC;IACvC;IACA,OAAO;MAAElD,WAAW;MAAEkD;IAAU,CAAC;EACrC;EACA;AACJ;AACA;AACA;EACI,MAAMwC,IAAIA,CAAC5E,MAAM,EAAEpB,OAAO,EAAEK,SAAS,EAAE;IACnC,MAAM;MAAEC;IAAY,CAAC,GAAG,MAAM,IAAI,CAAC0C,QAAQ,CAAC,CAAC5B,MAAM,CAAC,EAAEpB,OAAO,EAAEK,SAAS,CAAC;IACzE,OAAOC,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,IAAI;EACjC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM0F,OAAOA,CAAC1F,IAAI,EAAEP,OAAO,EAAEK,SAAS,EAAE;IACpC,OAAO,IAAI,CAAC2F,IAAI,CAACzF,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;EAC9C;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM6F,eAAeA,CAACC,QAAQ,EAAEnG,OAAO,EAAEK,SAAS,EAAE;IAChD,MAAME,IAAI,GAAG5B,eAAe,CAACwH,QAAQ,CAAC;IACtC,MAAMC,UAAU,GAAG,MAAM,IAAI,CAACJ,IAAI,CAACzF,IAAI,EAAEP,OAAO,EAAEK,SAAS,CAAC;IAC5D,OAAO,IAAI3B,SAAS,CAAC0H,UAAU,CAAC;EACpC;EACA;AACJ;AACA;EACI;EACAC,kBAAkBA,CAAA,EAAG;IACjB,OAAO,CAAC,CAAC;EACb;EACA;AACJ;AACA;AACA;EACIC,SAASA,CAAA,EAAG;IACR,OAAO;MACH,GAAG,IAAI,CAACD,kBAAkB,CAAC,CAAC;MAC5BE,KAAK,EAAE,IAAI,CAAC1G,QAAQ,CAAC,CAAC;MACtB2G,MAAM,EAAE,IAAI,CAACC,UAAU,CAAC;IAC5B,CAAC;EACL;EACAA,UAAUA,CAAA,EAAG;IACT,OAAO,UAAU;EACrB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,GAAG,SAASxH,OAAO,CAAC;EAC7B,MAAMmF,SAASA,CAACtB,OAAO,EAAE/C,OAAO,EAAE2C,UAAU,EAAE;IAC1C,MAAMrC,WAAW,GAAG,MAAMkC,OAAO,CAACC,GAAG,CAACM,OAAO,CAACL,GAAG,CAAC,CAACtB,MAAM,EAAE0E,WAAW,KAAK,IAAI,CAACa,KAAK,CAACvF,MAAM,EAAE;MAAE,GAAGpB,OAAO;MAAE8F;IAAY,CAAC,EAAEnD,UAAU,CAAC,CAACiE,IAAI,CAAErG,IAAI,IAAK,CAAC;MAAEA;IAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IACnK,OAAO;MAAED;IAAY,CAAC;EAC1B;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}